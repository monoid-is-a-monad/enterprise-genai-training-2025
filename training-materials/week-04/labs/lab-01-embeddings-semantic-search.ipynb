{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ded364",
   "metadata": {},
   "source": [
    "# Lab 1: Embeddings & Semantic Search\n",
    "\n",
    "**Week 4 - RAG Fundamentals**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand text embeddings and vector representations\n",
    "- Generate embeddings using OpenAI's embedding models\n",
    "- Implement similarity search algorithms\n",
    "- Build a vector database from scratch\n",
    "- Work with different distance metrics\n",
    "- Create a semantic search engine\n",
    "- Optimize embedding storage and retrieval\n",
    "- Handle multi-lingual embeddings\n",
    "- Evaluate search quality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 3 labs\n",
    "- Understanding of vectors and linear algebra basics\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f3fca",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f63442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken numpy scikit-learn pandas matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# For similarity computations\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b078da",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Text Embeddings\n",
    "\n",
    "Text embeddings are dense vector representations of text that capture semantic meaning. Similar texts have similar embeddings.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Embedding Models**: Transform text → vectors\n",
    "2. **Dimensionality**: Number of dimensions in the vector (e.g., 1536 for text-embedding-3-small)\n",
    "3. **Semantic Similarity**: Similar meaning → close vectors\n",
    "4. **Distance Metrics**: Measure similarity between vectors\n",
    "\n",
    "Let's start by generating embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embedding for a text using OpenAI's API.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: Embedding model to use\n",
    "    \n",
    "    Returns:\n",
    "        Embedding vector\n",
    "    \"\"\"\n",
    "    # Replace newlines with spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Test with simple examples\n",
    "texts = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"A feline rests on the rug\",\n",
    "    \"The dog plays in the garden\",\n",
    "    \"Python is a programming language\",\n",
    "    \"I love eating pizza\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = get_embedding(text)\n",
    "    embeddings.append(embedding)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Embedding dimension: {len(embedding)}\")\n",
    "    print(f\"First 5 values: {embedding[:5]}\")\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"\\n✓ Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b794686",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Explore Embedding Properties\n",
    "\n",
    "Generate embeddings and explore their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore embedding properties\n",
    "\n",
    "# 1. TODO: Generate embeddings for these sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"The weather is sunny\", \"It's a bright day\"),\n",
    "    (\"I am happy\", \"I am sad\"),\n",
    "    (\"Machine learning is fascinating\", \"AI is interesting\"),\n",
    "    (\"The car is red\", \"The vehicle is crimson\"),\n",
    "    (\"Hello world\", \"Goodbye universe\")\n",
    "]\n",
    "\n",
    "# 2. TODO: For each pair, calculate the cosine similarity\n",
    "# Hint: Use sklearn's cosine_similarity or implement manually\n",
    "\n",
    "# 3. TODO: Which pairs are most similar? Least similar?\n",
    "\n",
    "# 4. TODO: Generate embeddings for the same text with small variations\n",
    "# Example: \"Hello\" vs \"Hello!\" vs \"hello\" vs \"HELLO\"\n",
    "# Do punctuation and case affect embeddings?\n",
    "\n",
    "# Your code here:\n",
    "# for pair in sentence_pairs:\n",
    "#     emb1 = get_embedding(pair[0])\n",
    "#     emb2 = get_embedding(pair[1])\n",
    "#     similarity = ...\n",
    "#     print(f\"{pair[0]} <-> {pair[1]}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9555875",
   "metadata": {},
   "source": [
    "## Part 2: Similarity Metrics\n",
    "\n",
    "Different metrics for measuring similarity between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d618c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCalculator:\n",
    "    \"\"\"\n",
    "    Calculate various similarity metrics between vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Cosine similarity: measures angle between vectors.\n",
    "        Range: [-1, 1], where 1 = identical direction\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Euclidean distance: straight-line distance.\n",
    "        Range: [0, ∞], where 0 = identical\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(vec1 - vec2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Dot product: measures similarity and magnitude.\n",
    "        Range: [-∞, ∞]\n",
    "        \"\"\"\n",
    "        return np.dot(vec1, vec2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Manhattan distance: sum of absolute differences.\n",
    "        Range: [0, ∞], where 0 = identical\n",
    "        \"\"\"\n",
    "        return np.sum(np.abs(vec1 - vec2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_all_metrics(vec1: np.ndarray, vec2: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Compare using all metrics.\"\"\"\n",
    "        return {\n",
    "            \"cosine_similarity\": SimilarityCalculator.cosine_similarity(vec1, vec2),\n",
    "            \"euclidean_distance\": SimilarityCalculator.euclidean_distance(vec1, vec2),\n",
    "            \"dot_product\": SimilarityCalculator.dot_product(vec1, vec2),\n",
    "            \"manhattan_distance\": SimilarityCalculator.manhattan_distance(vec1, vec2)\n",
    "        }\n",
    "\n",
    "# Test with our earlier examples\n",
    "calc = SimilarityCalculator()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMILARITY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparisons = [\n",
    "    (0, 1, \"cat on mat\", \"feline on rug\"),\n",
    "    (0, 2, \"cat on mat\", \"dog in garden\"),\n",
    "    (0, 3, \"cat on mat\", \"Python language\"),\n",
    "    (3, 4, \"Python language\", \"eating pizza\")\n",
    "]\n",
    "\n",
    "for idx1, idx2, text1, text2 in comparisons:\n",
    "    metrics = calc.compare_all_metrics(embeddings[idx1], embeddings[idx2])\n",
    "    \n",
    "    print(f\"\\n{text1} <-> {text2}\")\n",
    "    print(\"-\" * 80)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric:20s}: {value:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a5929",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Compare Distance Metrics\n",
    "\n",
    "Compare different metrics for semantic search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bddbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare distance metrics for search\n",
    "\n",
    "# 1. TODO: Create a small corpus of documents (10-15 sentences on different topics)\n",
    "corpus = [\n",
    "    # TODO: Add diverse sentences\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    # Add more...\n",
    "]\n",
    "\n",
    "# 2. TODO: Generate embeddings for all documents\n",
    "\n",
    "# 3. TODO: Pick a query (e.g., \"animals in nature\")\n",
    "\n",
    "# 4. TODO: Rank documents using each metric\n",
    "# - Cosine similarity (higher = better)\n",
    "# - Euclidean distance (lower = better)\n",
    "# - Dot product (higher = better)\n",
    "\n",
    "# 5. TODO: Do different metrics give different rankings?\n",
    "\n",
    "# Your implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2b77e",
   "metadata": {},
   "source": [
    "## Part 3: Building a Simple Vector Database\n",
    "\n",
    "Implement a basic vector database for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"A document with its embedding.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class SimpleVectorDB:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector database.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        Initialize vector database.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: OpenAI embedding model to use\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents: List[Document] = []\n",
    "        self.embeddings_matrix: Optional[np.ndarray] = None\n",
    "    \n",
    "    def add_document(\n",
    "        self,\n",
    "        text: str,\n",
    "        doc_id: Optional[str] = None,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a document to the database.\n",
    "        \n",
    "        Args:\n",
    "            text: Document text\n",
    "            doc_id: Optional document ID (auto-generated if not provided)\n",
    "            metadata: Optional metadata\n",
    "        \"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = f\"doc_{len(self.documents)}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = np.array(get_embedding(text, self.embedding_model))\n",
    "        \n",
    "        # Create document\n",
    "        doc = Document(\n",
    "            id=doc_id,\n",
    "            text=text,\n",
    "            embedding=embedding,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.documents.append(doc)\n",
    "        \n",
    "        # Rebuild embeddings matrix\n",
    "        self._rebuild_embeddings_matrix()\n",
    "        \n",
    "        return doc_id\n",
    "    \n",
    "    def add_documents(self, texts: List[str], metadatas: Optional[List[Dict]] = None):\n",
    "        \"\"\"Add multiple documents.\"\"\"\n",
    "        if metadatas is None:\n",
    "            metadatas = [{}] * len(texts)\n",
    "        \n",
    "        for text, metadata in zip(texts, metadatas):\n",
    "            self.add_document(text, metadata=metadata)\n",
    "    \n",
    "    def _rebuild_embeddings_matrix(self):\n",
    "        \"\"\"Rebuild the embeddings matrix.\"\"\"\n",
    "        if self.documents:\n",
    "            self.embeddings_matrix = np.vstack([doc.embedding for doc in self.documents])\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        metric: str = \"cosine\"\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Search for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results to return\n",
    "            metric: Similarity metric ('cosine', 'euclidean', 'dot')\n",
    "        \n",
    "        Returns:\n",
    "            List of (document, score) tuples\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(get_embedding(query, self.embedding_model))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        if metric == \"cosine\":\n",
    "            # Cosine similarity (higher is better)\n",
    "            similarities = cosine_similarity(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                self.embeddings_matrix\n",
    "            )[0]\n",
    "            # Sort descending\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "            scores = similarities[top_indices]\n",
    "        \n",
    "        elif metric == \"euclidean\":\n",
    "            # Euclidean distance (lower is better)\n",
    "            distances = euclidean_distances(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                self.embeddings_matrix\n",
    "            )[0]\n",
    "            # Sort ascending\n",
    "            top_indices = np.argsort(distances)[:top_k]\n",
    "            # Convert to similarity score (inverse distance)\n",
    "            scores = 1 / (1 + distances[top_indices])\n",
    "        \n",
    "        elif metric == \"dot\":\n",
    "            # Dot product (higher is better)\n",
    "            scores = np.dot(self.embeddings_matrix, query_embedding)\n",
    "            top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "            scores = scores[top_indices]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        # Return results\n",
    "        results = [\n",
    "            (self.documents[idx], float(score))\n",
    "            for idx, score in zip(top_indices, scores)\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_document(self, doc_id: str) -> Optional[Document]:\n",
    "        \"\"\"Get document by ID.\"\"\"\n",
    "        for doc in self.documents:\n",
    "            if doc.id == doc_id:\n",
    "                return doc\n",
    "        return None\n",
    "    \n",
    "    def delete_document(self, doc_id: str) -> bool:\n",
    "        \"\"\"Delete document by ID.\"\"\"\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            if doc.id == doc_id:\n",
    "                del self.documents[i]\n",
    "                self._rebuild_embeddings_matrix()\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save database to file.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"✓ Database saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath: str) -> 'SimpleVectorDB':\n",
    "        \"\"\"Load database from file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            db = pickle.load(f)\n",
    "        print(f\"✓ Database loaded from {filepath}\")\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of documents.\"\"\"\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation.\"\"\"\n",
    "        return f\"SimpleVectorDB(documents={len(self.documents)}, model={self.embedding_model})\"\n",
    "\n",
    "# Test the vector database\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING VECTOR DATABASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create database\n",
    "db = SimpleVectorDB()\n",
    "\n",
    "# Add sample documents\n",
    "sample_docs = [\n",
    "    \"Python is a high-level programming language known for its simplicity.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Neural networks are inspired by biological neurons.\",\n",
    "    \"JavaScript is commonly used for web development.\",\n",
    "    \"Deep learning uses multiple layers to learn representations.\",\n",
    "    \"The Great Wall of China is one of the world's wonders.\",\n",
    "    \"Natural language processing deals with text and speech.\",\n",
    "    \"React is a popular JavaScript library for building UIs.\",\n",
    "    \"Computer vision enables machines to interpret visual information.\"\n",
    "]\n",
    "\n",
    "print(f\"\\nAdding {len(sample_docs)} documents...\")\n",
    "db.add_documents(sample_docs)\n",
    "print(f\"✓ Database contains {len(db)} documents\")\n",
    "\n",
    "# Search examples\n",
    "queries = [\n",
    "    \"programming languages\",\n",
    "    \"artificial intelligence\",\n",
    "    \"famous landmarks\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = db.search(query, top_k=3)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   Text: {doc.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb17d2",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Build Your Own Vector Database\n",
    "\n",
    "Enhance the vector database with additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c746a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enhance the vector database\n",
    "\n",
    "class EnhancedVectorDB(SimpleVectorDB):\n",
    "    \"\"\"\n",
    "    TODO: Add these features:\n",
    "    \n",
    "    1. Metadata filtering\n",
    "       - Search with filters (e.g., category=\"tech\", date_range)\n",
    "    \n",
    "    2. Hybrid search\n",
    "       - Combine keyword matching with semantic search\n",
    "    \n",
    "    3. Update documents\n",
    "       - Update document text and regenerate embedding\n",
    "    \n",
    "    4. Batch operations\n",
    "       - Add/delete multiple documents efficiently\n",
    "    \n",
    "    5. Statistics\n",
    "       - Track search queries, popular documents, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        super().__init__(embedding_model)\n",
    "        # TODO: Add your enhancements\n",
    "    \n",
    "    def search_with_filter(\n",
    "        self,\n",
    "        query: str,\n",
    "        filters: Dict[str, Any],\n",
    "        top_k: int = 5\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Search with metadata filters.\n",
    "        \n",
    "        Example:\n",
    "            results = db.search_with_filter(\n",
    "                \"AI news\",\n",
    "                filters={\"category\": \"technology\", \"year\": 2024}\n",
    "            )\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        semantic_weight: float = 0.7\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Combine keyword and semantic search.\n",
    "        \n",
    "        semantic_weight: Weight for semantic vs keyword (0-1)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_document(self, doc_id: str, new_text: str):\n",
    "        \"\"\"TODO: Update a document's text and embedding.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your enhancements\n",
    "# enhanced_db = EnhancedVectorDB()\n",
    "# enhanced_db.add_document(\"AI is transforming healthcare\", metadata={\"category\": \"tech\", \"year\": 2024})\n",
    "# results = enhanced_db.search_with_filter(\"medical technology\", filters={\"category\": \"tech\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3a0c3",
   "metadata": {},
   "source": [
    "## Part 4: Semantic Search Engine\n",
    "\n",
    "Build a practical semantic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"\n",
    "    A semantic search engine with advanced features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"Initialize search engine.\"\"\"\n",
    "        self.db = SimpleVectorDB(embedding_model)\n",
    "        self.search_history: List[Dict] = []\n",
    "    \n",
    "    def index_documents(\n",
    "        self,\n",
    "        documents: List[Dict[str, Any]],\n",
    "        text_field: str = \"text\",\n",
    "        id_field: str = \"id\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Index documents for search.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document dictionaries\n",
    "            text_field: Field containing text to index\n",
    "            id_field: Field containing document ID\n",
    "        \"\"\"\n",
    "        print(f\"Indexing {len(documents)} documents...\")\n",
    "        \n",
    "        for doc in documents:\n",
    "            text = doc.get(text_field, \"\")\n",
    "            doc_id = doc.get(id_field)\n",
    "            \n",
    "            # Extract metadata (all fields except text)\n",
    "            metadata = {k: v for k, v in doc.items() if k != text_field}\n",
    "            \n",
    "            self.db.add_document(text, doc_id=doc_id, metadata=metadata)\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.db)} documents\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        return_scores: bool = True,\n",
    "        min_score: Optional[float] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results\n",
    "            return_scores: Include similarity scores\n",
    "            min_score: Minimum similarity score threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of search results\n",
    "        \"\"\"\n",
    "        # Record search\n",
    "        search_record = {\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"top_k\": top_k\n",
    "        }\n",
    "        \n",
    "        # Perform search\n",
    "        results = self.db.search(query, top_k=top_k)\n",
    "        \n",
    "        # Filter by minimum score if specified\n",
    "        if min_score is not None:\n",
    "            results = [(doc, score) for doc, score in results if score >= min_score]\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for doc, score in results:\n",
    "            result = {\n",
    "                \"id\": doc.id,\n",
    "                \"text\": doc.text,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            if return_scores:\n",
    "                result[\"score\"] = score\n",
    "            \n",
    "            formatted_results.append(result)\n",
    "        \n",
    "        search_record[\"num_results\"] = len(formatted_results)\n",
    "        self.search_history.append(search_record)\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def multi_query_search(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        top_k: int = 5,\n",
    "        aggregation: str = \"max\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search using multiple queries and aggregate results.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of search queries\n",
    "            top_k: Number of final results\n",
    "            aggregation: How to combine scores ('max', 'mean', 'sum')\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated search results\n",
    "        \"\"\"\n",
    "        # Collect results from all queries\n",
    "        all_results = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            results = self.db.search(query, top_k=top_k * 2)  # Get more to aggregate\n",
    "            \n",
    "            for doc, score in results:\n",
    "                if doc.id not in all_results:\n",
    "                    all_results[doc.id] = {\n",
    "                        \"doc\": doc,\n",
    "                        \"scores\": []\n",
    "                    }\n",
    "                all_results[doc.id][\"scores\"].append(score)\n",
    "        \n",
    "        # Aggregate scores\n",
    "        aggregated = []\n",
    "        for doc_id, data in all_results.items():\n",
    "            scores = data[\"scores\"]\n",
    "            \n",
    "            if aggregation == \"max\":\n",
    "                final_score = max(scores)\n",
    "            elif aggregation == \"mean\":\n",
    "                final_score = np.mean(scores)\n",
    "            elif aggregation == \"sum\":\n",
    "                final_score = sum(scores)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "            \n",
    "            aggregated.append((data[\"doc\"], final_score))\n",
    "        \n",
    "        # Sort and return top_k\n",
    "        aggregated.sort(key=lambda x: x[1], reverse=True)\n",
    "        aggregated = aggregated[:top_k]\n",
    "        \n",
    "        # Format results\n",
    "        return [\n",
    "            {\n",
    "                \"id\": doc.id,\n",
    "                \"text\": doc.text,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"score\": score\n",
    "            }\n",
    "            for doc, score in aggregated\n",
    "        ]\n",
    "    \n",
    "    def get_similar_documents(\n",
    "        self,\n",
    "        doc_id: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Find documents similar to a given document.\n",
    "        \n",
    "        Args:\n",
    "            doc_id: Document ID\n",
    "            top_k: Number of similar documents\n",
    "        \n",
    "        Returns:\n",
    "            Similar documents\n",
    "        \"\"\"\n",
    "        doc = self.db.get_document(doc_id)\n",
    "        if doc is None:\n",
    "            return []\n",
    "        \n",
    "        # Use document text as query\n",
    "        results = self.search(doc.text, top_k=top_k + 1)\n",
    "        \n",
    "        # Remove the document itself\n",
    "        results = [r for r in results if r[\"id\"] != doc_id][:top_k]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_search_analytics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get analytics about search usage.\"\"\"\n",
    "        if not self.search_history:\n",
    "            return {\"message\": \"No searches yet\"}\n",
    "        \n",
    "        total_searches = len(self.search_history)\n",
    "        avg_results = np.mean([s[\"num_results\"] for s in self.search_history])\n",
    "        \n",
    "        # Most common queries\n",
    "        from collections import Counter\n",
    "        query_counts = Counter(s[\"query\"] for s in self.search_history)\n",
    "        top_queries = query_counts.most_common(5)\n",
    "        \n",
    "        return {\n",
    "            \"total_searches\": total_searches,\n",
    "            \"avg_results_per_search\": avg_results,\n",
    "            \"top_queries\": top_queries,\n",
    "            \"total_documents\": len(self.db)\n",
    "        }\n",
    "\n",
    "# Create a semantic search engine with sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC SEARCH ENGINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "engine = SemanticSearchEngine()\n",
    "\n",
    "# Sample documents\n",
    "tech_articles = [\n",
    "    {\n",
    "        \"id\": \"art1\",\n",
    "        \"text\": \"Artificial intelligence is revolutionizing healthcare with diagnostic tools and personalized treatment plans.\",\n",
    "        \"category\": \"healthcare\",\n",
    "        \"author\": \"Dr. Smith\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art2\",\n",
    "        \"text\": \"Python has become the dominant language for data science and machine learning applications.\",\n",
    "        \"category\": \"programming\",\n",
    "        \"author\": \"Jane Doe\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art3\",\n",
    "        \"text\": \"Cloud computing enables businesses to scale their infrastructure dynamically and cost-effectively.\",\n",
    "        \"category\": \"cloud\",\n",
    "        \"author\": \"John Tech\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art4\",\n",
    "        \"text\": \"Deep learning models are achieving remarkable results in natural language understanding tasks.\",\n",
    "        \"category\": \"AI\",\n",
    "        \"author\": \"Dr. Smith\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art5\",\n",
    "        \"text\": \"Cybersecurity threats are evolving rapidly, requiring advanced detection and prevention systems.\",\n",
    "        \"category\": \"security\",\n",
    "        \"author\": \"Alice Guard\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art6\",\n",
    "        \"text\": \"Quantum computing promises to solve problems that are intractable for classical computers.\",\n",
    "        \"category\": \"quantum\",\n",
    "        \"author\": \"Bob Quantum\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art7\",\n",
    "        \"text\": \"Medical imaging powered by AI helps radiologists detect diseases earlier and more accurately.\",\n",
    "        \"category\": \"healthcare\",\n",
    "        \"author\": \"Dr. Smith\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"art8\",\n",
    "        \"text\": \"JavaScript frameworks like React and Vue make building interactive web applications easier.\",\n",
    "        \"category\": \"programming\",\n",
    "        \"author\": \"Jane Doe\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "engine.index_documents(tech_articles)\n",
    "\n",
    "# Single query search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Single Query Search\")\n",
    "print(\"=\"*80)\n",
    "query = \"AI in medicine\"\n",
    "results = engine.search(query, top_k=3)\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. [{result['id']}] Score: {result['score']:.4f}\")\n",
    "    print(f\"   {result['text']}\")\n",
    "    print(f\"   Category: {result['metadata']['category']}, Author: {result['metadata']['author']}\")\n",
    "\n",
    "# Multi-query search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Multi-Query Search\")\n",
    "print(\"=\"*80)\n",
    "queries = [\"artificial intelligence\", \"machine learning\", \"neural networks\"]\n",
    "results = engine.multi_query_search(queries, top_k=3)\n",
    "print(f\"\\nQueries: {queries}\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. [{result['id']}] Aggregated Score: {result['score']:.4f}\")\n",
    "    print(f\"   {result['text'][:80]}...\")\n",
    "\n",
    "# Find similar documents\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Similar Documents\")\n",
    "print(\"=\"*80)\n",
    "doc_id = \"art1\"\n",
    "similar = engine.get_similar_documents(doc_id, top_k=2)\n",
    "print(f\"\\nDocuments similar to '{doc_id}':\")\n",
    "for i, result in enumerate(similar, 1):\n",
    "    print(f\"\\n{i}. [{result['id']}] Score: {result['score']:.4f}\")\n",
    "    print(f\"   {result['text'][:80]}...\")\n",
    "\n",
    "# Analytics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Search Analytics\")\n",
    "print(\"=\"*80)\n",
    "analytics = engine.get_search_analytics()\n",
    "for key, value in analytics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9669b",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Build a Domain-Specific Search Engine\n",
    "\n",
    "Create a search engine for a specific domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c294dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a domain-specific search engine\n",
    "\n",
    "# 1. TODO: Choose a domain (e.g., medical papers, legal documents, code snippets)\n",
    "\n",
    "# 2. TODO: Create a dataset (15-20 documents)\n",
    "# You can generate synthetic data or use real examples\n",
    "\n",
    "domain_documents = [\n",
    "    # TODO: Add domain-specific documents\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"...\",\n",
    "        \"metadata\": {...}\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. TODO: Implement domain-specific features\n",
    "class DomainSearchEngine(SemanticSearchEngine):\n",
    "    \"\"\"\n",
    "    TODO: Add domain-specific features:\n",
    "    - Custom preprocessing for domain\n",
    "    - Domain-specific metadata extraction\n",
    "    - Specialized ranking logic\n",
    "    - Query expansion using domain knowledge\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# 4. TODO: Test with domain-specific queries\n",
    "# domain_engine = DomainSearchEngine()\n",
    "# domain_engine.index_documents(domain_documents)\n",
    "# results = domain_engine.search(\"your domain query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730aa99",
   "metadata": {},
   "source": [
    "## Part 5: Visualization and Analysis\n",
    "\n",
    "Visualize embeddings and analyze search quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a250b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVisualizer:\n",
    "    \"\"\"\n",
    "    Visualize embeddings and similarity relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_embeddings_2d(\n",
    "        embeddings: np.ndarray,\n",
    "        labels: List[str],\n",
    "        title: str = \"Embedding Visualization\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot embeddings in 2D using PCA.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Embedding matrix (n_samples, n_dimensions)\n",
    "            labels: Labels for each embedding\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        # Reduce to 2D using PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(\n",
    "            embeddings_2d[:, 0],\n",
    "            embeddings_2d[:, 1],\n",
    "            c=range(len(labels)),\n",
    "            cmap='viridis',\n",
    "            s=100,\n",
    "            alpha=0.6\n",
    "        )\n",
    "        \n",
    "        # Add labels\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.annotate(\n",
    "                label[:30] + \"...\" if len(label) > 30 else label,\n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=8,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "        plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "        plt.colorbar(scatter, label=\"Document Index\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_similarity_matrix(\n",
    "        embeddings: np.ndarray,\n",
    "        labels: List[str],\n",
    "        title: str = \"Similarity Matrix\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot similarity matrix heatmap.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Embedding matrix\n",
    "            labels: Labels for each embedding\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        # Compute similarity matrix\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            similarities,\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"YlOrRd\",\n",
    "            square=True,\n",
    "            cbar_kws={\"label\": \"Cosine Similarity\"}\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_query_results(\n",
    "        query: str,\n",
    "        results: List[Tuple[str, float]],\n",
    "        title: str = \"Search Results\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Visualize search results as bar chart.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            results: List of (text, score) tuples\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        texts = [text[:40] + \"...\" if len(text) > 40 else text for text, _ in results]\n",
    "        scores = [score for _, score in results]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(range(len(texts)), scores, color='steelblue', alpha=0.7)\n",
    "        plt.yticks(range(len(texts)), texts)\n",
    "        plt.xlabel(\"Similarity Score\")\n",
    "        plt.title(f\"{title}\\nQuery: '{query}'\")\n",
    "        plt.xlim(0, max(scores) * 1.1)\n",
    "        \n",
    "        # Add score labels on bars\n",
    "        for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "            plt.text(score, i, f' {score:.3f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize our tech articles\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get embeddings and labels from search engine\n",
    "embeddings_viz = np.vstack([doc.embedding for doc in engine.db.documents])\n",
    "labels_viz = [doc.metadata.get('category', 'unknown') + \": \" + doc.text[:40] \n",
    "              for doc in engine.db.documents]\n",
    "\n",
    "# Plot embeddings in 2D\n",
    "visualizer = EmbeddingVisualizer()\n",
    "visualizer.plot_embeddings_2d(\n",
    "    embeddings_viz,\n",
    "    labels_viz,\n",
    "    title=\"Tech Articles Embeddings (PCA)\"\n",
    ")\n",
    "\n",
    "# Plot similarity matrix\n",
    "visualizer.plot_similarity_matrix(\n",
    "    embeddings_viz[:6],  # First 6 documents for readability\n",
    "    [f\"art{i+1}\" for i in range(6)],\n",
    "    title=\"Document Similarity Matrix\"\n",
    ")\n",
    "\n",
    "# Plot search results\n",
    "query = \"AI and machine learning\"\n",
    "search_results = engine.search(query, top_k=5)\n",
    "results_viz = [(r['text'], r['score']) for r in search_results]\n",
    "visualizer.plot_query_results(query, results_viz, title=\"Search Results Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e93cf",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Analyze Embedding Clusters\n",
    "\n",
    "Analyze how embeddings cluster by topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clustering analysis\n",
    "\n",
    "# 1. TODO: Create a larger dataset with clear categories (50+ documents, 4-5 categories)\n",
    "\n",
    "# 2. TODO: Use clustering algorithms (K-means, DBSCAN) to group similar documents\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# 3. TODO: Visualize clusters in 2D\n",
    "\n",
    "# 4. TODO: Evaluate cluster quality\n",
    "# - Are documents in the same cluster actually similar?\n",
    "# - Do clusters align with your intended categories?\n",
    "\n",
    "# 5. TODO: Try different embedding models and compare\n",
    "# - text-embedding-3-small\n",
    "# - text-embedding-3-large\n",
    "# Do they produce different clusters?\n",
    "\n",
    "# Your implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a39ae",
   "metadata": {},
   "source": [
    "## Part 6: Optimizing Embeddings\n",
    "\n",
    "Techniques for optimizing embedding usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingOptimizer:\n",
    "    \"\"\"\n",
    "    Optimize embedding storage and retrieval.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_dimensionality(\n",
    "        embeddings: np.ndarray,\n",
    "        target_dim: int = 256,\n",
    "        method: str = \"pca\"\n",
    "    ) -> Tuple[np.ndarray, Any]:\n",
    "        \"\"\"\n",
    "        Reduce embedding dimensionality.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Original embeddings\n",
    "            target_dim: Target dimensionality\n",
    "            method: Reduction method ('pca')\n",
    "        \n",
    "        Returns:\n",
    "            (reduced_embeddings, transformer)\n",
    "        \"\"\"\n",
    "        if method == \"pca\":\n",
    "            pca = PCA(n_components=target_dim)\n",
    "            reduced = pca.fit_transform(embeddings)\n",
    "            print(f\"✓ Reduced from {embeddings.shape[1]} to {target_dim} dimensions\")\n",
    "            print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "            return reduced, pca\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_embeddings(\n",
    "        embeddings: np.ndarray,\n",
    "        bits: int = 8\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Quantize embeddings to reduce storage.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Original embeddings\n",
    "            bits: Bits per value (8 or 16)\n",
    "        \n",
    "        Returns:\n",
    "            Quantized embeddings\n",
    "        \"\"\"\n",
    "        if bits == 8:\n",
    "            # Quantize to int8\n",
    "            min_val = embeddings.min()\n",
    "            max_val = embeddings.max()\n",
    "            scale = (max_val - min_val) / 255\n",
    "            quantized = ((embeddings - min_val) / scale).astype(np.uint8)\n",
    "            \n",
    "            # Calculate compression ratio\n",
    "            original_size = embeddings.nbytes\n",
    "            quantized_size = quantized.nbytes\n",
    "            ratio = original_size / quantized_size\n",
    "            \n",
    "            print(f\"✓ Quantized to {bits} bits\")\n",
    "            print(f\"  Original size: {original_size:,} bytes\")\n",
    "            print(f\"  Quantized size: {quantized_size:,} bytes\")\n",
    "            print(f\"  Compression ratio: {ratio:.1f}x\")\n",
    "            \n",
    "            return quantized\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported bits: {bits}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_embeddings(\n",
    "        texts: List[str],\n",
    "        cache_file: str = \"embedding_cache.pkl\"\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Cache embeddings to avoid regenerating.\n",
    "        \n",
    "        Args:\n",
    "            texts: Texts to embed\n",
    "            cache_file: Cache file path\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of text -> embedding\n",
    "        \"\"\"\n",
    "        # Load existing cache\n",
    "        cache = {}\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                cache = pickle.load(f)\n",
    "            print(f\"✓ Loaded cache with {len(cache)} embeddings\")\n",
    "        \n",
    "        # Generate embeddings for new texts\n",
    "        new_texts = [t for t in texts if t not in cache]\n",
    "        if new_texts:\n",
    "            print(f\"Generating {len(new_texts)} new embeddings...\")\n",
    "            for text in new_texts:\n",
    "                cache[text] = get_embedding(text)\n",
    "        \n",
    "        # Save cache\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(cache, f)\n",
    "        print(f\"✓ Cache saved with {len(cache)} embeddings\")\n",
    "        \n",
    "        return cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_search_speed(\n",
    "        db: SimpleVectorDB,\n",
    "        query: str,\n",
    "        num_runs: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Benchmark search speed.\n",
    "        \n",
    "        Args:\n",
    "            db: Vector database\n",
    "            query: Search query\n",
    "            num_runs: Number of search runs\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            db.search(query, top_k=10)\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        \n",
    "        print(f\"\\n✓ Search benchmark ({num_runs} runs):\")\n",
    "        print(f\"  Average time: {avg_time*1000:.2f} ms\")\n",
    "        print(f\"  Std deviation: {std_time*1000:.2f} ms\")\n",
    "        print(f\"  Min time: {min(times)*1000:.2f} ms\")\n",
    "        print(f\"  Max time: {max(times)*1000:.2f} ms\")\n",
    "\n",
    "# Test optimization techniques\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EMBEDDING OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimizer = EmbeddingOptimizer()\n",
    "\n",
    "# Test dimensionality reduction\n",
    "embeddings_full = np.vstack([doc.embedding for doc in engine.db.documents])\n",
    "embeddings_reduced, pca_transformer = optimizer.reduce_dimensionality(embeddings_full, target_dim=128)\n",
    "\n",
    "# Test quantization\n",
    "embeddings_quantized = optimizer.quantize_embeddings(embeddings_full)\n",
    "\n",
    "# Benchmark search speed\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Search Speed Benchmark\")\n",
    "print(\"=\"*80)\n",
    "optimizer.benchmark_search_speed(engine.db, \"artificial intelligence\", num_runs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893b45a",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Optimize for Production\n",
    "\n",
    "Optimize a vector database for production use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Production optimization\n",
    "\n",
    "class ProductionVectorDB(SimpleVectorDB):\n",
    "    \"\"\"\n",
    "    TODO: Implement production optimizations:\n",
    "    \n",
    "    1. Lazy loading\n",
    "       - Don't load all embeddings into memory at once\n",
    "       - Load on-demand for large databases\n",
    "    \n",
    "    2. Approximate nearest neighbor search\n",
    "       - Use libraries like FAISS or Annoy for faster search\n",
    "       - Trade accuracy for speed\n",
    "    \n",
    "    3. Sharding\n",
    "       - Split database into shards for parallel search\n",
    "       - Useful for very large databases\n",
    "    \n",
    "    4. Caching\n",
    "       - Cache frequent queries\n",
    "       - Cache recent search results\n",
    "    \n",
    "    5. Monitoring\n",
    "       - Track search latency\n",
    "       - Monitor memory usage\n",
    "       - Alert on slow queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # TODO: Add production features\n",
    "    \n",
    "    # TODO: Implement optimized methods\n",
    "\n",
    "# Test production features\n",
    "# prod_db = ProductionVectorDB()\n",
    "# Test with large dataset and measure improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf8631",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Multi-Modal Search\n",
    "\n",
    "Build a search engine that handles text and other data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalSearchEngine:\n",
    "    \"\"\"\n",
    "    Search engine supporting multiple data types.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Text + metadata search\n",
    "    2. Date range filtering\n",
    "    3. Numeric range filtering (e.g., price, rating)\n",
    "    4. Geospatial search (location-based)\n",
    "    5. Combined scoring (semantic + metadata + filters)\n",
    "    6. Faceted search (category counts, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize multi-modal search.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def search_with_filters(\n",
    "        self,\n",
    "        query: str,\n",
    "        filters: Dict[str, Any],\n",
    "        top_k: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        TODO: Search with multiple filter types.\n",
    "        \n",
    "        Example:\n",
    "            results = engine.search_with_filters(\n",
    "                \"best restaurants\",\n",
    "                filters={\n",
    "                    \"price_range\": (10, 50),\n",
    "                    \"rating_min\": 4.0,\n",
    "                    \"distance_km\": 5,\n",
    "                    \"categories\": [\"italian\", \"pizza\"]\n",
    "                }\n",
    "            )\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# multi_engine = MultiModalSearchEngine()\n",
    "# multi_engine.index_documents(restaurant_data)\n",
    "# results = multi_engine.search_with_filters(\"authentic pizza\", filters={...})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad333f8",
   "metadata": {},
   "source": [
    "### Challenge 2: Real-Time Search System\n",
    "\n",
    "Build a search system that updates in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeSearchSystem:\n",
    "    \"\"\"\n",
    "    Real-time search with incremental updates.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Stream new documents as they arrive\n",
    "    2. Update index without rebuilding\n",
    "    3. Handle concurrent reads and writes\n",
    "    4. Implement write-ahead log for durability\n",
    "    5. Support rollback on errors\n",
    "    6. Real-time search analytics dashboard\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize real-time search.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def stream_document(self, document: Dict):\n",
    "        \"\"\"TODO: Add document to index in real-time.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def subscribe_to_updates(self, callback: callable):\n",
    "        \"\"\"TODO: Subscribe to index updates.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# rt_system = RealTimeSearchSystem()\n",
    "# rt_system.subscribe_to_updates(lambda doc: print(f\"New doc: {doc}\"))\n",
    "# rt_system.stream_document({\"text\": \"Breaking news...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6ca2b",
   "metadata": {},
   "source": [
    "### Challenge 3: Distributed Search System\n",
    "\n",
    "Build a distributed search system for scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedSearchSystem:\n",
    "    \"\"\"\n",
    "    Distributed search across multiple nodes.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Shard data across multiple nodes\n",
    "    2. Parallel search across shards\n",
    "    3. Result aggregation from multiple nodes\n",
    "    4. Load balancing\n",
    "    5. Fault tolerance (handle node failures)\n",
    "    6. Replication for redundancy\n",
    "    7. Consistent hashing for shard assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_shards: int = 4):\n",
    "        \"\"\"Initialize distributed search.\"\"\"\n",
    "        self.num_shards = num_shards\n",
    "        self.shards = []\n",
    "    \n",
    "    def add_shard(self, shard_db: SimpleVectorDB):\n",
    "        \"\"\"TODO: Add a shard to the system.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def search_distributed(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        TODO: Search across all shards in parallel.\n",
    "        \n",
    "        Steps:\n",
    "        1. Send query to all shards\n",
    "        2. Collect results from each shard\n",
    "        3. Merge and re-rank\n",
    "        4. Return top_k global results\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# dist_system = DistributedSearchSystem(num_shards=4)\n",
    "# for i in range(4):\n",
    "#     shard = SimpleVectorDB()\n",
    "#     # Add documents to shard...\n",
    "#     dist_system.add_shard(shard)\n",
    "# results = dist_system.search_distributed(\"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ade7e8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ Text embeddings and vector representations\n",
    "2. ✅ Similarity metrics (cosine, euclidean, dot product)\n",
    "3. ✅ Building a vector database from scratch\n",
    "4. ✅ Implementing semantic search\n",
    "5. ✅ Multi-query and aggregation strategies\n",
    "6. ✅ Visualizing embeddings and similarities\n",
    "7. ✅ Optimizing embeddings for production\n",
    "8. ✅ Advanced search features\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Embeddings:**\n",
    "- Dense vector representations of text\n",
    "- Capture semantic meaning\n",
    "- Similar texts have similar embeddings\n",
    "- Different models, different dimensions\n",
    "\n",
    "**Vector Databases:**\n",
    "- Store and search embeddings efficiently\n",
    "- Support similarity-based retrieval\n",
    "- Enable semantic search\n",
    "- Foundation for RAG systems\n",
    "\n",
    "**Similarity Metrics:**\n",
    "- **Cosine similarity**: Angle between vectors (most common)\n",
    "- **Euclidean distance**: Straight-line distance\n",
    "- **Dot product**: Considers magnitude and direction\n",
    "- Choice depends on use case\n",
    "\n",
    "**Optimization Techniques:**\n",
    "- Dimensionality reduction (PCA)\n",
    "- Quantization for storage\n",
    "- Caching for performance\n",
    "- Approximate search for scale\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Choose the right metric**: Cosine similarity for most text use cases\n",
    "2. **Normalize embeddings**: For dot product similarity\n",
    "3. **Cache embeddings**: Don't regenerate unnecessarily\n",
    "4. **Monitor quality**: Evaluate search relevance regularly\n",
    "5. **Consider scale**: Use approximate methods for large datasets\n",
    "6. **Metadata matters**: Combine semantic + metadata filtering\n",
    "7. **Test with real queries**: User queries differ from synthetic ones\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Experiment with different embedding models\n",
    "- Build domain-specific search engines\n",
    "- Move on to Lab 2: Basic RAG Implementation\n",
    "- Explore vector database libraries (FAISS, Pinecone, Weaviate)\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
