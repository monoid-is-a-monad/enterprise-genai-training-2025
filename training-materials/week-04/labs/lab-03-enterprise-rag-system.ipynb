{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6d6baf",
   "metadata": {},
   "source": [
    "# Lab 3: Enterprise RAG System\n",
    "\n",
    "**Week 4 - RAG Fundamentals**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Build production-ready RAG systems\n",
    "- Implement advanced RAG patterns\n",
    "- Handle concurrent requests\n",
    "- Add caching and optimization\n",
    "- Implement monitoring and logging\n",
    "- Build fault-tolerant systems\n",
    "- Add security and access control\n",
    "- Deploy RAG at scale\n",
    "- Implement RAG pipelines with streaming\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 4 Labs 1-2\n",
    "- Understanding of RAG architecture\n",
    "- Experience with embeddings and vector search\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bedfa",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f164651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken numpy scikit-learn pandas redis --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "import threading\n",
    "from typing import List, Dict, Optional, Tuple, Any, Callable, Iterator\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a21072",
   "metadata": {},
   "source": [
    "## Part 1: Production-Ready RAG Components\n",
    "\n",
    "Building enterprise-grade RAG requires robust, scalable components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9374234",
   "metadata": {},
   "source": [
    "### 1.1 Embeddings Cache\n",
    "\n",
    "Cache embeddings to avoid redundant API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsCache:\n",
    "    \"\"\"\n",
    "    Cache for embeddings to reduce API calls and costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \".embeddings_cache\",\n",
    "        ttl_hours: int = 24 * 7  # 1 week\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize embeddings cache.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir: Directory to store cache\n",
    "            ttl_hours: Time-to-live in hours\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.ttl = timedelta(hours=ttl_hours)\n",
    "        \n",
    "        # In-memory cache\n",
    "        self.memory_cache: Dict[str, Tuple[List[float], datetime]] = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "        logger.info(f\"Initialized embeddings cache at {cache_dir}\")\n",
    "    \n",
    "    def _get_cache_key(self, text: str, model: str) -> str:\n",
    "        \"\"\"Generate cache key from text and model.\"\"\"\n",
    "        combined = f\"{model}:{text}\"\n",
    "        return hashlib.sha256(combined.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str, model: str) -> Optional[List[float]]:\n",
    "        \"\"\"\n",
    "        Get embedding from cache.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to embed\n",
    "            model: Embedding model name\n",
    "        \n",
    "        Returns:\n",
    "            Cached embedding or None\n",
    "        \"\"\"\n",
    "        cache_key = self._get_cache_key(text, model)\n",
    "        \n",
    "        # Check memory cache first\n",
    "        if cache_key in self.memory_cache:\n",
    "            embedding, timestamp = self.memory_cache[cache_key]\n",
    "            if datetime.now() - timestamp < self.ttl:\n",
    "                self.hits += 1\n",
    "                return embedding\n",
    "            else:\n",
    "                # Expired\n",
    "                del self.memory_cache[cache_key]\n",
    "        \n",
    "        # Check disk cache\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                timestamp = data['timestamp']\n",
    "                if datetime.now() - timestamp < self.ttl:\n",
    "                    embedding = data['embedding']\n",
    "                    # Load into memory cache\n",
    "                    self.memory_cache[cache_key] = (embedding, timestamp)\n",
    "                    self.hits += 1\n",
    "                    return embedding\n",
    "                else:\n",
    "                    # Expired\n",
    "                    cache_file.unlink()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error reading cache: {e}\")\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, text: str, model: str, embedding: List[float]):\n",
    "        \"\"\"\n",
    "        Store embedding in cache.\n",
    "        \n",
    "        Args:\n",
    "            text: Text that was embedded\n",
    "            model: Embedding model name\n",
    "            embedding: Embedding vector\n",
    "        \"\"\"\n",
    "        cache_key = self._get_cache_key(text, model)\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Store in memory\n",
    "        self.memory_cache[cache_key] = (embedding, timestamp)\n",
    "        \n",
    "        # Store on disk\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'embedding': embedding,\n",
    "                    'timestamp': timestamp,\n",
    "                    'text_length': len(text),\n",
    "                    'model': model\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error writing cache: {e}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"total_requests\": total,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"memory_cache_size\": len(self.memory_cache),\n",
    "            \"disk_cache_size\": len(list(self.cache_dir.glob(\"*.pkl\")))\n",
    "        }\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache.\"\"\"\n",
    "        self.memory_cache.clear()\n",
    "        for cache_file in self.cache_dir.glob(\"*.pkl\"):\n",
    "            cache_file.unlink()\n",
    "        logger.info(\"Cache cleared\")\n",
    "\n",
    "# Test embeddings cache\n",
    "print(\"=\"*80)\n",
    "print(\"EMBEDDINGS CACHE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cache = EmbeddingsCache()\n",
    "\n",
    "def get_embedding_with_cache(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"Get embedding with caching.\"\"\"\n",
    "    # Check cache first\n",
    "    embedding = cache.get(text, model)\n",
    "    if embedding is not None:\n",
    "        return embedding\n",
    "    \n",
    "    # Get from API\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    \n",
    "    # Store in cache\n",
    "    cache.set(text, model, embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Test cache\n",
    "test_text = \"This is a test sentence for caching.\"\n",
    "\n",
    "print(\"\\nFirst call (cache miss):\")\n",
    "start = time.time()\n",
    "emb1 = get_embedding_with_cache(test_text)\n",
    "time1 = time.time() - start\n",
    "print(f\"  Time: {time1:.4f}s\")\n",
    "\n",
    "print(\"\\nSecond call (cache hit):\")\n",
    "start = time.time()\n",
    "emb2 = get_embedding_with_cache(test_text)\n",
    "time2 = time.time() - start\n",
    "print(f\"  Time: {time2:.4f}s\")\n",
    "print(f\"  Speedup: {time1/time2:.1f}x\")\n",
    "\n",
    "print(\"\\nCache statistics:\")\n",
    "stats = cache.get_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f513f",
   "metadata": {},
   "source": [
    "### 1.2 Rate Limiter\n",
    "\n",
    "Manage API rate limits and concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Rate limiter for API calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_requests_per_minute: int = 3500,\n",
    "        max_tokens_per_minute: int = 90000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize rate limiter.\n",
    "        \n",
    "        Args:\n",
    "            max_requests_per_minute: Maximum requests per minute\n",
    "            max_tokens_per_minute: Maximum tokens per minute\n",
    "        \"\"\"\n",
    "        self.max_requests_per_minute = max_requests_per_minute\n",
    "        self.max_tokens_per_minute = max_tokens_per_minute\n",
    "        \n",
    "        # Tracking\n",
    "        self.request_times: deque = deque()\n",
    "        self.token_usage: deque = deque()  # (timestamp, tokens)\n",
    "        \n",
    "        # Thread safety\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        logger.info(f\"Initialized rate limiter: {max_requests_per_minute} req/min, {max_tokens_per_minute} tokens/min\")\n",
    "    \n",
    "    def _clean_old_entries(self):\n",
    "        \"\"\"Remove entries older than 1 minute.\"\"\"\n",
    "        now = time.time()\n",
    "        cutoff = now - 60\n",
    "        \n",
    "        # Clean requests\n",
    "        while self.request_times and self.request_times[0] < cutoff:\n",
    "            self.request_times.popleft()\n",
    "        \n",
    "        # Clean tokens\n",
    "        while self.token_usage and self.token_usage[0][0] < cutoff:\n",
    "            self.token_usage.popleft()\n",
    "    \n",
    "    def acquire(self, estimated_tokens: int = 1000) -> bool:\n",
    "        \"\"\"\n",
    "        Try to acquire permission for an API call.\n",
    "        \n",
    "        Args:\n",
    "            estimated_tokens: Estimated token usage\n",
    "        \n",
    "        Returns:\n",
    "            True if acquired, False if rate limit exceeded\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            self._clean_old_entries()\n",
    "            \n",
    "            # Check request limit\n",
    "            if len(self.request_times) >= self.max_requests_per_minute:\n",
    "                return False\n",
    "            \n",
    "            # Check token limit\n",
    "            current_tokens = sum(tokens for _, tokens in self.token_usage)\n",
    "            if current_tokens + estimated_tokens > self.max_tokens_per_minute:\n",
    "                return False\n",
    "            \n",
    "            # Acquire\n",
    "            now = time.time()\n",
    "            self.request_times.append(now)\n",
    "            self.token_usage.append((now, estimated_tokens))\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    def wait_if_needed(self, estimated_tokens: int = 1000, max_wait: float = 60.0):\n",
    "        \"\"\"\n",
    "        Wait until rate limit allows the request.\n",
    "        \n",
    "        Args:\n",
    "            estimated_tokens: Estimated token usage\n",
    "            max_wait: Maximum wait time in seconds\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        while not self.acquire(estimated_tokens):\n",
    "            if time.time() - start > max_wait:\n",
    "                raise TimeoutError(\"Rate limit wait timeout\")\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get rate limiter statistics.\"\"\"\n",
    "        with self.lock:\n",
    "            self._clean_old_entries()\n",
    "            current_requests = len(self.request_times)\n",
    "            current_tokens = sum(tokens for _, tokens in self.token_usage)\n",
    "            \n",
    "            return {\n",
    "                \"current_requests_per_minute\": current_requests,\n",
    "                \"current_tokens_per_minute\": current_tokens,\n",
    "                \"request_capacity_used\": current_requests / self.max_requests_per_minute,\n",
    "                \"token_capacity_used\": current_tokens / self.max_tokens_per_minute\n",
    "            }\n",
    "\n",
    "# Test rate limiter\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RATE LIMITER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rate_limiter = RateLimiter(max_requests_per_minute=10, max_tokens_per_minute=5000)\n",
    "\n",
    "print(\"\\nSimulating API calls:\")\n",
    "for i in range(12):\n",
    "    if rate_limiter.acquire(estimated_tokens=500):\n",
    "        print(f\"  Request {i+1}: Accepted\")\n",
    "    else:\n",
    "        print(f\"  Request {i+1}: Rate limited\")\n",
    "    \n",
    "    if i == 5:\n",
    "        print(\"\\n  Checking stats:\")\n",
    "        stats = rate_limiter.get_stats()\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"    {key}: {value:.2%}\")\n",
    "            else:\n",
    "                print(f\"    {key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8514ec4",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Build Connection Pool\n",
    "\n",
    "Implement a connection pool for API clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa768afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement connection pool\n",
    "\n",
    "class APIConnectionPool:\n",
    "    \"\"\"\n",
    "    TODO: Connection pool for API clients.\n",
    "    \n",
    "    Should implement:\n",
    "    1. Pool of OpenAI clients\n",
    "    2. Connection reuse\n",
    "    3. Health checking\n",
    "    4. Automatic retry with backoff\n",
    "    5. Circuit breaker pattern\n",
    "    6. Load balancing across clients\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pool_size: int = 5):\n",
    "        \"\"\"Initialize connection pool.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_client(self) -> OpenAI:\n",
    "        \"\"\"TODO: Get an available client from pool.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def release_client(self, client: OpenAI):\n",
    "        \"\"\"TODO: Return client to pool.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def health_check(self) -> Dict[str, bool]:\n",
    "        \"\"\"TODO: Check health of all clients.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your connection pool\n",
    "# pool = APIConnectionPool(pool_size=3)\n",
    "# client = pool.get_client()\n",
    "# # Use client...\n",
    "# pool.release_client(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc93a8",
   "metadata": {},
   "source": [
    "## Part 2: Enterprise RAG System\n",
    "\n",
    "Complete enterprise-grade RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryRequest:\n",
    "    \"\"\"RAG query request.\"\"\"\n",
    "    query: str\n",
    "    top_k: int = 3\n",
    "    min_similarity: float = 0.0\n",
    "    filters: Optional[Dict[str, Any]] = None\n",
    "    user_id: Optional[str] = None\n",
    "    session_id: Optional[str] = None\n",
    "    \n",
    "@dataclass\n",
    "class QueryResponse:\n",
    "    \"\"\"RAG query response.\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    confidence: float\n",
    "    latency_ms: float\n",
    "    num_chunks_retrieved: int\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class EnterpriseRAG:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system with enterprise features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-3.5-turbo\",\n",
    "        cache_dir: str = \".rag_cache\",\n",
    "        max_concurrent_requests: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize enterprise RAG.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Model for embeddings\n",
    "            llm_model: Model for generation\n",
    "            cache_dir: Directory for caching\n",
    "            max_concurrent_requests: Max concurrent queries\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        \n",
    "        # Components\n",
    "        self.embeddings_cache = EmbeddingsCache(cache_dir)\n",
    "        self.rate_limiter = RateLimiter()\n",
    "        \n",
    "        # Storage\n",
    "        self.chunks: List[Dict[str, Any]] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        \n",
    "        # Concurrency\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_concurrent_requests)\n",
    "        self.semaphore = threading.Semaphore(max_concurrent_requests)\n",
    "        \n",
    "        # Monitoring\n",
    "        self.query_history: List[Dict[str, Any]] = []\n",
    "        self.error_count = 0\n",
    "        \n",
    "        logger.info(f\"Initialized EnterpriseRAG with model={llm_model}\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding with caching and rate limiting.\"\"\"\n",
    "        # Check cache\n",
    "        embedding = self.embeddings_cache.get(text, self.embedding_model)\n",
    "        if embedding is not None:\n",
    "            return embedding\n",
    "        \n",
    "        # Rate limit\n",
    "        self.rate_limiter.wait_if_needed(estimated_tokens=len(text.split()))\n",
    "        \n",
    "        # Get from API\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        response = client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        embedding = response.data[0].embedding\n",
    "        \n",
    "        # Cache\n",
    "        self.embeddings_cache.set(text, self.embedding_model, embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def ingest_document(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str,\n",
    "        metadata: Optional[Dict] = None,\n",
    "        chunk_size: int = 500\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingest a document.\n",
    "        \n",
    "        Args:\n",
    "            text: Document text\n",
    "            document_id: Document identifier\n",
    "            metadata: Optional metadata\n",
    "            chunk_size: Chunk size in characters\n",
    "        \"\"\"\n",
    "        logger.info(f\"Ingesting document: {document_id}\")\n",
    "        \n",
    "        # Simple chunking\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            current_size += len(word) + 1\n",
    "            \n",
    "            if current_size >= chunk_size:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunks.append(chunk_text)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        logger.info(f\"  Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Generate embeddings (with parallelization)\n",
    "        logger.info(f\"  Generating embeddings...\")\n",
    "        \n",
    "        def embed_chunk(chunk_text, idx):\n",
    "            embedding = self.get_embedding(chunk_text)\n",
    "            return {\n",
    "                'chunk_id': f\"{document_id}_chunk_{idx}\",\n",
    "                'document_id': document_id,\n",
    "                'content': chunk_text,\n",
    "                'embedding': embedding,\n",
    "                'metadata': metadata or {}\n",
    "            }\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [\n",
    "                executor.submit(embed_chunk, chunk, i)\n",
    "                for i, chunk in enumerate(chunks)\n",
    "            ]\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    chunk_data = future.result()\n",
    "                    self.chunks.append(chunk_data)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error embedding chunk: {e}\")\n",
    "                    self.error_count += 1\n",
    "        \n",
    "        # Rebuild embeddings matrix\n",
    "        self._rebuild_embeddings()\n",
    "        \n",
    "        logger.info(f\"✓ Document ingested. Total chunks: {len(self.chunks)}\")\n",
    "    \n",
    "    def _rebuild_embeddings(self):\n",
    "        \"\"\"Rebuild embeddings matrix.\"\"\"\n",
    "        if self.chunks:\n",
    "            embeddings_list = [chunk['embedding'] for chunk in self.chunks]\n",
    "            self.embeddings = np.array(embeddings_list)\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 3,\n",
    "        min_similarity: float = 0.0,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of chunks\n",
    "            min_similarity: Minimum similarity\n",
    "            filters: Metadata filters\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk, score) tuples\n",
    "        \"\"\"\n",
    "        if not self.chunks:\n",
    "            return []\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(self.get_embedding(query))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1),\n",
    "            self.embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Apply filters\n",
    "        valid_indices = []\n",
    "        for idx, chunk in enumerate(self.chunks):\n",
    "            if filters:\n",
    "                match = all(\n",
    "                    chunk['metadata'].get(k) == v\n",
    "                    for k, v in filters.items()\n",
    "                )\n",
    "                if not match:\n",
    "                    continue\n",
    "            valid_indices.append(idx)\n",
    "        \n",
    "        # Get top-k from valid indices\n",
    "        valid_similarities = [(idx, similarities[idx]) for idx in valid_indices]\n",
    "        valid_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in valid_similarities[:top_k]:\n",
    "            if score >= min_similarity:\n",
    "                results.append((self.chunks[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_chunks: List[Dict],\n",
    "        temperature: float = 0.7\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Generate answer with confidence score.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_chunks: Retrieved chunks\n",
    "            temperature: LLM temperature\n",
    "        \n",
    "        Returns:\n",
    "            (answer, confidence_score)\n",
    "        \"\"\"\n",
    "        # Assemble context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {chunk['content']}\"\n",
    "            for i, chunk in enumerate(context_chunks)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Answer the question based on the context provided. If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Rate limit\n",
    "        self.rate_limiter.wait_if_needed(estimated_tokens=len(prompt.split()) + 200)\n",
    "        \n",
    "        # Generate\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Estimate confidence (simple heuristic)\n",
    "        confidence = 0.5\n",
    "        if \"don't have enough information\" not in answer.lower():\n",
    "            confidence = 0.8\n",
    "        if len(context_chunks) >= 3:\n",
    "            confidence = min(confidence + 0.1, 1.0)\n",
    "        \n",
    "        return answer, confidence\n",
    "    \n",
    "    def query(self, request: QueryRequest) -> QueryResponse:\n",
    "        \"\"\"\n",
    "        Process a query request.\n",
    "        \n",
    "        Args:\n",
    "            request: Query request\n",
    "        \n",
    "        Returns:\n",
    "            Query response\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Acquire semaphore for concurrency control\n",
    "            with self.semaphore:\n",
    "                logger.info(f\"Processing query: {request.query}\")\n",
    "                \n",
    "                # Retrieve\n",
    "                retrieved = self.retrieve(\n",
    "                    request.query,\n",
    "                    request.top_k,\n",
    "                    request.min_similarity,\n",
    "                    request.filters\n",
    "                )\n",
    "                \n",
    "                if not retrieved:\n",
    "                    latency = (time.time() - start_time) * 1000\n",
    "                    response = QueryResponse(\n",
    "                        query=request.query,\n",
    "                        answer=\"I don't have any relevant information to answer that question.\",\n",
    "                        sources=[],\n",
    "                        confidence=0.0,\n",
    "                        latency_ms=latency,\n",
    "                        num_chunks_retrieved=0\n",
    "                    )\n",
    "                    self._log_query(request, response)\n",
    "                    return response\n",
    "                \n",
    "                # Generate\n",
    "                chunks = [chunk for chunk, _ in retrieved]\n",
    "                answer, confidence = self.generate_answer(request.query, chunks)\n",
    "                \n",
    "                # Build response\n",
    "                latency = (time.time() - start_time) * 1000\n",
    "                \n",
    "                response = QueryResponse(\n",
    "                    query=request.query,\n",
    "                    answer=answer,\n",
    "                    sources=[\n",
    "                        {\n",
    "                            'chunk_id': chunk['chunk_id'],\n",
    "                            'document_id': chunk['document_id'],\n",
    "                            'content': chunk['content'],\n",
    "                            'score': score,\n",
    "                            'metadata': chunk['metadata']\n",
    "                        }\n",
    "                        for chunk, score in retrieved\n",
    "                    ],\n",
    "                    confidence=confidence,\n",
    "                    latency_ms=latency,\n",
    "                    num_chunks_retrieved=len(retrieved),\n",
    "                    metadata={\n",
    "                        'user_id': request.user_id,\n",
    "                        'session_id': request.session_id\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                self._log_query(request, response)\n",
    "                \n",
    "                return response\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            self.error_count += 1\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            return QueryResponse(\n",
    "                query=request.query,\n",
    "                answer=f\"Error processing query: {str(e)}\",\n",
    "                sources=[],\n",
    "                confidence=0.0,\n",
    "                latency_ms=latency,\n",
    "                num_chunks_retrieved=0,\n",
    "                metadata={'error': str(e)}\n",
    "            )\n",
    "    \n",
    "    def query_batch(\n",
    "        self,\n",
    "        requests: List[QueryRequest]\n",
    "    ) -> List[QueryResponse]:\n",
    "        \"\"\"\n",
    "        Process multiple queries in parallel.\n",
    "        \n",
    "        Args:\n",
    "            requests: List of query requests\n",
    "        \n",
    "        Returns:\n",
    "            List of query responses\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing batch of {len(requests)} queries\")\n",
    "        \n",
    "        futures = [\n",
    "            self.executor.submit(self.query, request)\n",
    "            for request in requests\n",
    "        ]\n",
    "        \n",
    "        responses = []\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                response = future.result()\n",
    "                responses.append(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch query: {e}\")\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def _log_query(self, request: QueryRequest, response: QueryResponse):\n",
    "        \"\"\"Log query for monitoring.\"\"\"\n",
    "        self.query_history.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'query': request.query,\n",
    "            'answer': response.answer,\n",
    "            'confidence': response.confidence,\n",
    "            'latency_ms': response.latency_ms,\n",
    "            'num_sources': response.num_chunks_retrieved,\n",
    "            'user_id': request.user_id,\n",
    "            'session_id': request.session_id\n",
    "        })\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system metrics.\"\"\"\n",
    "        if not self.query_history:\n",
    "            return {\"message\": \"No queries processed yet\"}\n",
    "        \n",
    "        latencies = [q['latency_ms'] for q in self.query_history]\n",
    "        confidences = [q['confidence'] for q in self.query_history]\n",
    "        \n",
    "        cache_stats = self.embeddings_cache.get_stats()\n",
    "        rate_stats = self.rate_limiter.get_stats()\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.query_history),\n",
    "            \"avg_latency_ms\": np.mean(latencies),\n",
    "            \"p50_latency_ms\": np.percentile(latencies, 50),\n",
    "            \"p95_latency_ms\": np.percentile(latencies, 95),\n",
    "            \"p99_latency_ms\": np.percentile(latencies, 99),\n",
    "            \"avg_confidence\": np.mean(confidences),\n",
    "            \"error_count\": self.error_count,\n",
    "            \"cache_hit_rate\": cache_stats['hit_rate'],\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            **rate_stats\n",
    "        }\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Shutdown the system.\"\"\"\n",
    "        self.executor.shutdown(wait=True)\n",
    "        logger.info(\"EnterpriseRAG shutdown complete\")\n",
    "\n",
    "# Test Enterprise RAG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTERPRISE RAG SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create system\n",
    "rag = EnterpriseRAG()\n",
    "\n",
    "# Ingest documents\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"python_guide\",\n",
    "        \"text\": \"\"\"Python is a versatile programming language. It supports multiple programming paradigms including object-oriented and functional programming. Python has a rich ecosystem of libraries for data science, web development, and automation. Popular frameworks include Django and Flask for web development, and NumPy and Pandas for data analysis.\"\"\",\n",
    "        \"metadata\": {\"category\": \"programming\", \"language\": \"python\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ml_intro\",\n",
    "        \"text\": \"\"\"Machine learning is a subset of artificial intelligence. It enables systems to learn from data without explicit programming. Common algorithms include linear regression, decision trees, and neural networks. Deep learning uses multi-layer neural networks to learn complex patterns. Applications include image recognition, natural language processing, and recommendation systems.\"\"\",\n",
    "        \"metadata\": {\"category\": \"ai\", \"topic\": \"machine_learning\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    rag.ingest_document(doc['text'], doc['id'], doc['metadata'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Single query\n",
    "request = QueryRequest(\n",
    "    query=\"What is Python used for?\",\n",
    "    top_k=2,\n",
    "    user_id=\"user123\",\n",
    "    session_id=\"session456\"\n",
    ")\n",
    "\n",
    "response = rag.query(request)\n",
    "print(f\"\\nQuery: {response.query}\")\n",
    "print(f\"Answer: {response.answer}\")\n",
    "print(f\"Confidence: {response.confidence:.2f}\")\n",
    "print(f\"Latency: {response.latency_ms:.2f}ms\")\n",
    "print(f\"Sources: {response.num_chunks_retrieved}\")\n",
    "\n",
    "# Batch queries\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_requests = [\n",
    "    QueryRequest(query=\"Tell me about machine learning\", top_k=2),\n",
    "    QueryRequest(query=\"What frameworks does Python have?\", top_k=2),\n",
    "    QueryRequest(query=\"What is deep learning?\", top_k=2)\n",
    "]\n",
    "\n",
    "batch_responses = rag.query_batch(batch_requests)\n",
    "print(f\"\\nProcessed {len(batch_responses)} queries in parallel\")\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEM METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = rag.get_metrics()\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f908",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Add Monitoring and Alerting\n",
    "\n",
    "Implement comprehensive monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement monitoring system\n",
    "\n",
    "class RAGMonitor:\n",
    "    \"\"\"\n",
    "    TODO: Monitoring and alerting for RAG system.\n",
    "    \n",
    "    Should track:\n",
    "    1. Query volume and patterns\n",
    "    2. Latency percentiles (p50, p95, p99)\n",
    "    3. Error rates and types\n",
    "    4. Cache hit rates\n",
    "    5. Token usage and costs\n",
    "    6. User satisfaction scores\n",
    "    \n",
    "    Should alert on:\n",
    "    - High latency\n",
    "    - High error rate\n",
    "    - Low cache hit rate\n",
    "    - Unusual query patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: EnterpriseRAG):\n",
    "        \"\"\"Initialize monitor.\"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.alerts: List[Dict] = []\n",
    "    \n",
    "    def check_health(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        TODO: Check system health.\n",
    "        \n",
    "        Return health status for:\n",
    "        - Latency\n",
    "        - Error rate\n",
    "        - Cache performance\n",
    "        - Overall status\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_alert(self, alert_type: str, message: str):\n",
    "        \"\"\"TODO: Generate alert.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_dashboard_data(self) -> Dict:\n",
    "        \"\"\"TODO: Get data for monitoring dashboard.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test monitoring\n",
    "# monitor = RAGMonitor(rag)\n",
    "# health = monitor.check_health()\n",
    "# dashboard = monitor.get_dashboard_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd563d",
   "metadata": {},
   "source": [
    "## Part 3: Advanced RAG Patterns\n",
    "\n",
    "Sophisticated RAG techniques for better quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20463717",
   "metadata": {},
   "source": [
    "### 3.1 Query Routing\n",
    "\n",
    "Route queries to appropriate strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRouter:\n",
    "    \"\"\"\n",
    "    Route queries to appropriate RAG strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize query router.\"\"\"\n",
    "        self.query_patterns = {\n",
    "            'factual': re.compile(r'\\b(what|who|when|where|which)\\b', re.IGNORECASE),\n",
    "            'comparative': re.compile(r'\\b(compare|difference|versus|vs|better)\\b', re.IGNORECASE),\n",
    "            'procedural': re.compile(r'\\b(how|steps|process|procedure)\\b', re.IGNORECASE),\n",
    "            'analytical': re.compile(r'\\b(why|explain|analyze|reason)\\b', re.IGNORECASE)\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Initialized query router\")\n",
    "    \n",
    "    def classify_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify query type.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "        \n",
    "        Returns:\n",
    "            Query type\n",
    "        \"\"\"\n",
    "        for query_type, pattern in self.query_patterns.items():\n",
    "            if pattern.search(query):\n",
    "                return query_type\n",
    "        \n",
    "        return 'general'\n",
    "    \n",
    "    def get_strategy(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get retrieval strategy for query.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "        \n",
    "        Returns:\n",
    "            Strategy parameters\n",
    "        \"\"\"\n",
    "        query_type = self.classify_query(query)\n",
    "        \n",
    "        strategies = {\n",
    "            'factual': {\n",
    "                'top_k': 2,\n",
    "                'min_similarity': 0.7,\n",
    "                'temperature': 0.3,\n",
    "                'description': 'Precise, focused retrieval'\n",
    "            },\n",
    "            'comparative': {\n",
    "                'top_k': 5,\n",
    "                'min_similarity': 0.5,\n",
    "                'temperature': 0.5,\n",
    "                'description': 'Broader retrieval for comparison'\n",
    "            },\n",
    "            'procedural': {\n",
    "                'top_k': 4,\n",
    "                'min_similarity': 0.6,\n",
    "                'temperature': 0.4,\n",
    "                'description': 'Sequential, step-by-step context'\n",
    "            },\n",
    "            'analytical': {\n",
    "                'top_k': 5,\n",
    "                'min_similarity': 0.5,\n",
    "                'temperature': 0.7,\n",
    "                'description': 'Diverse context for analysis'\n",
    "            },\n",
    "            'general': {\n",
    "                'top_k': 3,\n",
    "                'min_similarity': 0.6,\n",
    "                'temperature': 0.5,\n",
    "                'description': 'Balanced retrieval'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'query_type': query_type,\n",
    "            **strategies.get(query_type, strategies['general'])\n",
    "        }\n",
    "\n",
    "# Test query router\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY ROUTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "router = QueryRouter()\n",
    "\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Compare Python and Java\",\n",
    "    \"How do I build a web app?\",\n",
    "    \"Why is caching important?\",\n",
    "    \"Tell me about databases\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    strategy = router.get_strategy(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"  Type: {strategy['query_type']}\")\n",
    "    print(f\"  Strategy: {strategy['description']}\")\n",
    "    print(f\"  Parameters: top_k={strategy['top_k']}, temp={strategy['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928fbd2",
   "metadata": {},
   "source": [
    "### 3.2 Streaming Responses\n",
    "\n",
    "Stream responses for better UX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingRAG(EnterpriseRAG):\n",
    "    \"\"\"\n",
    "    RAG system with streaming responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def query_stream(\n",
    "        self,\n",
    "        request: QueryRequest\n",
    "    ) -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process query with streaming response.\n",
    "        \n",
    "        Args:\n",
    "            request: Query request\n",
    "        \n",
    "        Yields:\n",
    "            Response chunks\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Yield status: retrieving\n",
    "            yield {\n",
    "                'type': 'status',\n",
    "                'message': 'Retrieving relevant information...'\n",
    "            }\n",
    "            \n",
    "            # Retrieve\n",
    "            retrieved = self.retrieve(\n",
    "                request.query,\n",
    "                request.top_k,\n",
    "                request.min_similarity,\n",
    "                request.filters\n",
    "            )\n",
    "            \n",
    "            # Yield sources\n",
    "            yield {\n",
    "                'type': 'sources',\n",
    "                'count': len(retrieved),\n",
    "                'sources': [\n",
    "                    {\n",
    "                        'document_id': chunk['document_id'],\n",
    "                        'score': float(score)\n",
    "                    }\n",
    "                    for chunk, score in retrieved\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            if not retrieved:\n",
    "                yield {\n",
    "                    'type': 'answer',\n",
    "                    'content': \"I don't have any relevant information to answer that question.\",\n",
    "                    'done': True\n",
    "                }\n",
    "                return\n",
    "            \n",
    "            # Yield status: generating\n",
    "            yield {\n",
    "                'type': 'status',\n",
    "                'message': 'Generating answer...'\n",
    "            }\n",
    "            \n",
    "            # Prepare context\n",
    "            chunks = [chunk for chunk, _ in retrieved]\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"[Source {i+1}]: {chunk['content']}\"\n",
    "                for i, chunk in enumerate(chunks)\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"\"\"Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {request.query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            # Stream generation\n",
    "            self.rate_limiter.wait_if_needed(estimated_tokens=len(prompt.split()) + 200)\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=self.llm_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=500,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            full_answer = \"\"\n",
    "            for chunk in response:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    full_answer += content\n",
    "                    yield {\n",
    "                        'type': 'answer_chunk',\n",
    "                        'content': content\n",
    "                    }\n",
    "            \n",
    "            # Yield completion\n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            yield {\n",
    "                'type': 'complete',\n",
    "                'latency_ms': latency,\n",
    "                'full_answer': full_answer\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in streaming query: {e}\")\n",
    "            yield {\n",
    "                'type': 'error',\n",
    "                'message': str(e)\n",
    "            }\n",
    "\n",
    "# Test streaming\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STREAMING RESPONSES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "streaming_rag = StreamingRAG()\n",
    "\n",
    "# Ingest documents\n",
    "for doc in documents:\n",
    "    streaming_rag.ingest_document(doc['text'], doc['id'], doc['metadata'])\n",
    "\n",
    "# Stream query\n",
    "request = QueryRequest(query=\"What is Python?\", top_k=2)\n",
    "\n",
    "print(f\"\\nQuery: {request.query}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for event in streaming_rag.query_stream(request):\n",
    "    if event['type'] == 'status':\n",
    "        print(f\"\\n[{event['message']}]\")\n",
    "    elif event['type'] == 'sources':\n",
    "        print(f\"\\n[Found {event['count']} relevant sources]\")\n",
    "    elif event['type'] == 'answer_chunk':\n",
    "        print(event['content'], end='', flush=True)\n",
    "    elif event['type'] == 'complete':\n",
    "        print(f\"\\n\\n[Completed in {event['latency_ms']:.2f}ms]\")\n",
    "    elif event['type'] == 'error':\n",
    "        print(f\"\\n[Error: {event['message']}]\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92827",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Implement Advanced Retrieval\n",
    "\n",
    "Add sophisticated retrieval techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement advanced retrieval\n",
    "\n",
    "class AdvancedRetriever:\n",
    "    \"\"\"\n",
    "    TODO: Advanced retrieval techniques.\n",
    "    \n",
    "    Implement:\n",
    "    1. Maximal Marginal Relevance (MMR)\n",
    "       - Balance relevance and diversity\n",
    "    \n",
    "    2. Reciprocal Rank Fusion (RRF)\n",
    "       - Combine multiple retrieval strategies\n",
    "    \n",
    "    3. Parent-Child Chunking\n",
    "       - Small chunks for retrieval\n",
    "       - Large chunks for context\n",
    "    \n",
    "    4. Hypothetical Document Embeddings (HyDE)\n",
    "       - Generate hypothetical answer\n",
    "       - Use it for retrieval\n",
    "    \n",
    "    5. Multi-Query Retrieval\n",
    "       - Generate multiple query variations\n",
    "       - Combine results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: EnterpriseRAG):\n",
    "        \"\"\"Initialize advanced retriever.\"\"\"\n",
    "        self.rag = rag_system\n",
    "    \n",
    "    def mmr_retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        lambda_param: float = 0.5\n",
    "    ) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Maximal Marginal Relevance retrieval.\n",
    "        \n",
    "        MMR = λ * relevance - (1-λ) * redundancy\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def hyde_retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        TODO: HyDE retrieval.\n",
    "        \n",
    "        1. Generate hypothetical answer to query\n",
    "        2. Embed the hypothetical answer\n",
    "        3. Use it to retrieve similar chunks\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def multi_query_retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_variants: int = 3,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Multi-query retrieval.\n",
    "        \n",
    "        1. Generate query variations\n",
    "        2. Retrieve with each variation\n",
    "        3. Combine and deduplicate results\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test advanced retrieval\n",
    "# retriever = AdvancedRetriever(rag)\n",
    "# results = retriever.mmr_retrieve(\"What is Python?\", lambda_param=0.7)\n",
    "# results = retriever.hyde_retrieve(\"Explain machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1193cad",
   "metadata": {},
   "source": [
    "## Part 4: Security and Access Control\n",
    "\n",
    "Enterprise systems need security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessControl:\n",
    "    \"\"\"\n",
    "    Access control for RAG system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize access control.\"\"\"\n",
    "        # User permissions\n",
    "        self.user_roles: Dict[str, str] = {}\n",
    "        self.role_permissions: Dict[str, List[str]] = {\n",
    "            'admin': ['read', 'write', 'delete', 'manage_users'],\n",
    "            'editor': ['read', 'write'],\n",
    "            'viewer': ['read']\n",
    "        }\n",
    "        \n",
    "        # Document access control\n",
    "        self.document_acl: Dict[str, List[str]] = {}  # doc_id -> allowed_users\n",
    "        \n",
    "        logger.info(\"Initialized access control\")\n",
    "    \n",
    "    def add_user(self, user_id: str, role: str = 'viewer'):\n",
    "        \"\"\"Add user with role.\"\"\"\n",
    "        self.user_roles[user_id] = role\n",
    "        logger.info(f\"Added user {user_id} with role {role}\")\n",
    "    \n",
    "    def set_document_access(self, document_id: str, allowed_users: List[str]):\n",
    "        \"\"\"Set document access control.\"\"\"\n",
    "        self.document_acl[document_id] = allowed_users\n",
    "        logger.info(f\"Set access for {document_id}: {len(allowed_users)} users\")\n",
    "    \n",
    "    def can_access(self, user_id: str, document_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if user can access document.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User identifier\n",
    "            document_id: Document identifier\n",
    "        \n",
    "        Returns:\n",
    "            True if allowed\n",
    "        \"\"\"\n",
    "        # Admins can access everything\n",
    "        if self.user_roles.get(user_id) == 'admin':\n",
    "            return True\n",
    "        \n",
    "        # Check document ACL\n",
    "        if document_id in self.document_acl:\n",
    "            return user_id in self.document_acl[document_id]\n",
    "        \n",
    "        # Default: no access\n",
    "        return False\n",
    "    \n",
    "    def has_permission(self, user_id: str, permission: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if user has permission.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User identifier\n",
    "            permission: Permission to check\n",
    "        \n",
    "        Returns:\n",
    "            True if allowed\n",
    "        \"\"\"\n",
    "        role = self.user_roles.get(user_id, 'viewer')\n",
    "        return permission in self.role_permissions.get(role, [])\n",
    "\n",
    "class SecureRAG(EnterpriseRAG):\n",
    "    \"\"\"\n",
    "    RAG system with access control.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize secure RAG.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.access_control = AccessControl()\n",
    "    \n",
    "    def ingest_document(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str,\n",
    "        metadata: Optional[Dict] = None,\n",
    "        allowed_users: Optional[List[str]] = None,\n",
    "        user_id: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingest document with access control.\n",
    "        \n",
    "        Args:\n",
    "            text: Document text\n",
    "            document_id: Document identifier\n",
    "            metadata: Optional metadata\n",
    "            allowed_users: List of users who can access\n",
    "            user_id: User performing the action\n",
    "        \"\"\"\n",
    "        # Check permission\n",
    "        if user_id and not self.access_control.has_permission(user_id, 'write'):\n",
    "            raise PermissionError(f\"User {user_id} does not have write permission\")\n",
    "        \n",
    "        # Ingest document\n",
    "        super().ingest_document(text, document_id, metadata)\n",
    "        \n",
    "        # Set access control\n",
    "        if allowed_users:\n",
    "            self.access_control.set_document_access(document_id, allowed_users)\n",
    "    \n",
    "    def query(self, request: QueryRequest) -> QueryResponse:\n",
    "        \"\"\"\n",
    "        Query with access control.\n",
    "        \n",
    "        Args:\n",
    "            request: Query request with user_id\n",
    "        \n",
    "        Returns:\n",
    "            Query response (filtered by access)\n",
    "        \"\"\"\n",
    "        # Get base response\n",
    "        response = super().query(request)\n",
    "        \n",
    "        # Filter sources by access\n",
    "        if request.user_id:\n",
    "            filtered_sources = []\n",
    "            for source in response.sources:\n",
    "                doc_id = source['document_id']\n",
    "                if self.access_control.can_access(request.user_id, doc_id):\n",
    "                    filtered_sources.append(source)\n",
    "            \n",
    "            response.sources = filtered_sources\n",
    "            response.num_chunks_retrieved = len(filtered_sources)\n",
    "            \n",
    "            # If no accessible sources, update answer\n",
    "            if not filtered_sources:\n",
    "                response.answer = \"You don't have access to information needed to answer this question.\"\n",
    "                response.confidence = 0.0\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test secure RAG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECURE RAG WITH ACCESS CONTROL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "secure_rag = SecureRAG()\n",
    "\n",
    "# Add users\n",
    "secure_rag.access_control.add_user(\"alice\", \"admin\")\n",
    "secure_rag.access_control.add_user(\"bob\", \"editor\")\n",
    "secure_rag.access_control.add_user(\"charlie\", \"viewer\")\n",
    "\n",
    "# Ingest documents with access control\n",
    "secure_rag.ingest_document(\n",
    "    \"This is confidential company data.\",\n",
    "    \"confidential_doc\",\n",
    "    metadata={\"classification\": \"confidential\"},\n",
    "    allowed_users=[\"alice\"],\n",
    "    user_id=\"alice\"\n",
    ")\n",
    "\n",
    "secure_rag.ingest_document(\n",
    "    \"This is public information available to everyone.\",\n",
    "    \"public_doc\",\n",
    "    metadata={\"classification\": \"public\"},\n",
    "    allowed_users=[\"alice\", \"bob\", \"charlie\"],\n",
    "    user_id=\"alice\"\n",
    ")\n",
    "\n",
    "# Test queries with different users\n",
    "print(\"\\nAlice (admin) query:\")\n",
    "request = QueryRequest(query=\"What is in the documents?\", user_id=\"alice\", top_k=5)\n",
    "response = secure_rag.query(request)\n",
    "print(f\"  Sources accessible: {response.num_chunks_retrieved}\")\n",
    "\n",
    "print(\"\\nCharlie (viewer) query:\")\n",
    "request = QueryRequest(query=\"What is in the documents?\", user_id=\"charlie\", top_k=5)\n",
    "response = secure_rag.query(request)\n",
    "print(f\"  Sources accessible: {response.num_chunks_retrieved}\")\n",
    "print(f\"  Answer: {response.answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537805de",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Add Audit Logging\n",
    "\n",
    "Implement comprehensive audit logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0462ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement audit logging\n",
    "\n",
    "class AuditLogger:\n",
    "    \"\"\"\n",
    "    TODO: Audit logging for RAG system.\n",
    "    \n",
    "    Should log:\n",
    "    1. All queries (who, what, when)\n",
    "    2. Document access attempts\n",
    "    3. Document modifications\n",
    "    4. Permission changes\n",
    "    5. Authentication events\n",
    "    6. Errors and exceptions\n",
    "    \n",
    "    Should support:\n",
    "    - Log rotation\n",
    "    - Log levels\n",
    "    - Structured logging\n",
    "    - Export to external systems\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str = \"rag_audit.log\"):\n",
    "        \"\"\"Initialize audit logger.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def log_query(self, user_id: str, query: str, results: int):\n",
    "        \"\"\"TODO: Log query event.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def log_access_denied(self, user_id: str, resource: str):\n",
    "        \"\"\"TODO: Log access denied event.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def log_document_change(self, user_id: str, document_id: str, action: str):\n",
    "        \"\"\"TODO: Log document change.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_user_activity(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"TODO: Get activity for a user.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test audit logging\n",
    "# auditor = AuditLogger()\n",
    "# auditor.log_query(\"alice\", \"What is Python?\", 3)\n",
    "# activity = auditor.get_user_activity(\"alice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff359d",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Multi-Tenant RAG\n",
    "\n",
    "Build RAG system supporting multiple tenants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTenantRAG:\n",
    "    \"\"\"\n",
    "    Multi-tenant RAG system.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Tenant isolation\n",
    "       - Separate data for each tenant\n",
    "       - No cross-tenant access\n",
    "    \n",
    "    2. Resource quotas\n",
    "       - Limit queries per tenant\n",
    "       - Limit storage per tenant\n",
    "    \n",
    "    3. Custom configurations\n",
    "       - Per-tenant models\n",
    "       - Per-tenant parameters\n",
    "    \n",
    "    4. Cost tracking\n",
    "       - Track usage per tenant\n",
    "       - Billing/chargeback\n",
    "    \n",
    "    5. Tenant management\n",
    "       - Create/delete tenants\n",
    "       - Tenant statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize multi-tenant RAG.\"\"\"\n",
    "        self.tenants: Dict[str, EnterpriseRAG] = {}\n",
    "        self.quotas: Dict[str, Dict] = {}\n",
    "    \n",
    "    def create_tenant(\n",
    "        self,\n",
    "        tenant_id: str,\n",
    "        quota_config: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"TODO: Create new tenant.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tenant_rag(self, tenant_id: str) -> EnterpriseRAG:\n",
    "        \"\"\"TODO: Get RAG system for tenant.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def query_with_tenant(\n",
    "        self,\n",
    "        tenant_id: str,\n",
    "        request: QueryRequest\n",
    "    ) -> QueryResponse:\n",
    "        \"\"\"TODO: Query with tenant isolation.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tenant_usage(self, tenant_id: str) -> Dict:\n",
    "        \"\"\"TODO: Get usage statistics for tenant.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# multi_tenant = MultiTenantRAG()\n",
    "# multi_tenant.create_tenant(\"company_a\", quota_config={\"max_queries\": 1000})\n",
    "# multi_tenant.create_tenant(\"company_b\", quota_config={\"max_queries\": 5000})\n",
    "# \n",
    "# request = QueryRequest(query=\"What is AI?\")\n",
    "# response = multi_tenant.query_with_tenant(\"company_a\", request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480addbf",
   "metadata": {},
   "source": [
    "### Challenge 2: Federated RAG\n",
    "\n",
    "RAG across multiple distributed sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89dccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedRAG:\n",
    "    \"\"\"\n",
    "    Federated RAG across distributed sources.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Multiple data sources\n",
    "       - Different databases\n",
    "       - Different embedding models\n",
    "       - Different formats\n",
    "    \n",
    "    2. Federated search\n",
    "       - Query all sources in parallel\n",
    "       - Merge and rank results\n",
    "    \n",
    "    3. Source prioritization\n",
    "       - Weight by source quality\n",
    "       - Prefer certain sources for certain queries\n",
    "    \n",
    "    4. Caching strategy\n",
    "       - Cache results from remote sources\n",
    "       - Invalidation policy\n",
    "    \n",
    "    5. Fault tolerance\n",
    "       - Handle source failures gracefully\n",
    "       - Partial results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize federated RAG.\"\"\"\n",
    "        self.sources: Dict[str, EnterpriseRAG] = {}\n",
    "        self.source_weights: Dict[str, float] = {}\n",
    "    \n",
    "    def register_source(\n",
    "        self,\n",
    "        source_id: str,\n",
    "        rag_system: EnterpriseRAG,\n",
    "        weight: float = 1.0\n",
    "    ):\n",
    "        \"\"\"TODO: Register a data source.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def federated_query(\n",
    "        self,\n",
    "        request: QueryRequest,\n",
    "        source_ids: Optional[List[str]] = None\n",
    "    ) -> QueryResponse:\n",
    "        \"\"\"\n",
    "        TODO: Query across federated sources.\n",
    "        \n",
    "        Should:\n",
    "        - Query all sources in parallel\n",
    "        - Merge results\n",
    "        - Re-rank combined results\n",
    "        - Handle partial failures\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def health_check_sources(self) -> Dict[str, bool]:\n",
    "        \"\"\"TODO: Check health of all sources.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# federated = FederatedRAG()\n",
    "# federated.register_source(\"internal_docs\", internal_rag, weight=1.0)\n",
    "# federated.register_source(\"public_docs\", public_rag, weight=0.8)\n",
    "# federated.register_source(\"wiki\", wiki_rag, weight=0.6)\n",
    "# \n",
    "# request = QueryRequest(query=\"What is machine learning?\")\n",
    "# response = federated.federated_query(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f625dc",
   "metadata": {},
   "source": [
    "### Challenge 3: Self-Healing RAG\n",
    "\n",
    "RAG system that monitors and improves itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32968828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfHealingRAG:\n",
    "    \"\"\"\n",
    "    Self-healing RAG system.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Automatic problem detection\n",
    "       - High error rate\n",
    "       - Poor answer quality\n",
    "       - Slow performance\n",
    "       - Low cache hit rate\n",
    "    \n",
    "    2. Automatic remediation\n",
    "       - Adjust parameters\n",
    "       - Refresh cache\n",
    "       - Re-index documents\n",
    "       - Switch models\n",
    "    \n",
    "    3. A/B testing\n",
    "       - Test different configurations\n",
    "       - Choose best performer\n",
    "    \n",
    "    4. Learning from feedback\n",
    "       - Track user satisfaction\n",
    "       - Improve based on feedback\n",
    "    \n",
    "    5. Predictive maintenance\n",
    "       - Predict issues before they occur\n",
    "       - Proactive optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: EnterpriseRAG):\n",
    "        \"\"\"Initialize self-healing RAG.\"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.health_history: List[Dict] = []\n",
    "        self.remediation_history: List[Dict] = []\n",
    "    \n",
    "    def monitor_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        TODO: Monitor system health.\n",
    "        \n",
    "        Check:\n",
    "        - Error rate\n",
    "        - Latency\n",
    "        - Cache performance\n",
    "        - Answer quality\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def detect_issues(self) -> List[str]:\n",
    "        \"\"\"TODO: Detect system issues.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def remediate(self, issue: str):\n",
    "        \"\"\"\n",
    "        TODO: Automatically fix issue.\n",
    "        \n",
    "        Actions might include:\n",
    "        - Clear cache\n",
    "        - Adjust top_k\n",
    "        - Change temperature\n",
    "        - Re-index documents\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def run_health_check_loop(self, interval_seconds: int = 60):\n",
    "        \"\"\"TODO: Continuous health monitoring.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# self_healing = SelfHealingRAG(rag)\n",
    "# self_healing.run_health_check_loop(interval_seconds=300)  # Check every 5 min\n",
    "# \n",
    "# # System automatically detects and fixes issues\n",
    "# issues = self_healing.detect_issues()\n",
    "# for issue in issues:\n",
    "#     self_healing.remediate(issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decacb9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ Production-ready RAG components\n",
    "2. ✅ Embeddings caching for cost reduction\n",
    "3. ✅ Rate limiting and concurrency control\n",
    "4. ✅ Enterprise RAG implementation\n",
    "5. ✅ Batch processing and parallelization\n",
    "6. ✅ Advanced RAG patterns\n",
    "7. ✅ Query routing and optimization\n",
    "8. ✅ Streaming responses\n",
    "9. ✅ Security and access control\n",
    "10. ✅ Monitoring and metrics\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Enterprise Requirements:**\n",
    "\n",
    "**Performance:**\n",
    "- Sub-second query latency (p95)\n",
    "- High throughput (100+ queries/sec)\n",
    "- Efficient caching strategies\n",
    "- Parallel processing\n",
    "\n",
    "**Reliability:**\n",
    "- Error handling and recovery\n",
    "- Rate limiting\n",
    "- Circuit breakers\n",
    "- Health monitoring\n",
    "\n",
    "**Security:**\n",
    "- Access control and authentication\n",
    "- Audit logging\n",
    "- Data isolation\n",
    "- Encryption at rest/transit\n",
    "\n",
    "**Scalability:**\n",
    "- Horizontal scaling\n",
    "- Load balancing\n",
    "- Database sharding\n",
    "- Caching layers\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Embeddings caching (reduces API calls by 70-90%)\n",
    "- Batch processing\n",
    "- Resource pooling\n",
    "- Usage tracking\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Caching:**\n",
    "- Cache embeddings aggressively\n",
    "- Use memory + disk caching\n",
    "- Set appropriate TTL\n",
    "- Monitor hit rates (target >80%)\n",
    "\n",
    "**Rate Limiting:**\n",
    "- Stay within API limits\n",
    "- Implement backoff strategies\n",
    "- Queue requests during peaks\n",
    "- Monitor capacity usage\n",
    "\n",
    "**Concurrency:**\n",
    "- Limit concurrent requests\n",
    "- Use connection pools\n",
    "- Implement request queuing\n",
    "- Thread-safe operations\n",
    "\n",
    "**Monitoring:**\n",
    "- Track latency percentiles\n",
    "- Monitor error rates\n",
    "- Cache hit rates\n",
    "- User satisfaction scores\n",
    "\n",
    "**Security:**\n",
    "- Implement RBAC (Role-Based Access Control)\n",
    "- Audit all operations\n",
    "- Sanitize user inputs\n",
    "- Validate permissions\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "**Before Deployment:**\n",
    "\n",
    "- [ ] Load testing completed\n",
    "- [ ] Error handling tested\n",
    "- [ ] Monitoring configured\n",
    "- [ ] Logging set up\n",
    "- [ ] Security review done\n",
    "- [ ] Backup strategy in place\n",
    "- [ ] Rollback plan ready\n",
    "- [ ] Documentation complete\n",
    "\n",
    "**Ongoing Operations:**\n",
    "\n",
    "- [ ] Monitor metrics daily\n",
    "- [ ] Review error logs\n",
    "- [ ] Optimize based on usage\n",
    "- [ ] Update models regularly\n",
    "- [ ] Test disaster recovery\n",
    "- [ ] Track costs\n",
    "- [ ] Collect user feedback\n",
    "- [ ] Iterate and improve\n",
    "\n",
    "### Performance Targets\n",
    "\n",
    "**Latency:**\n",
    "- p50: <500ms\n",
    "- p95: <1000ms\n",
    "- p99: <2000ms\n",
    "\n",
    "**Availability:**\n",
    "- Uptime: >99.9%\n",
    "- Error rate: <0.1%\n",
    "\n",
    "**Quality:**\n",
    "- Answer accuracy: >90%\n",
    "- User satisfaction: >4.5/5\n",
    "\n",
    "**Cost:**\n",
    "- Cache hit rate: >80%\n",
    "- Cost per query: <$0.01\n",
    "\n",
    "### Common Production Issues\n",
    "\n",
    "1. **Cache Misses**: Tune cache size and TTL\n",
    "2. **Rate Limits**: Implement better queuing\n",
    "3. **Slow Queries**: Optimize retrieval, reduce top_k\n",
    "4. **High Costs**: Increase caching, batch operations\n",
    "5. **Poor Quality**: Improve chunking, adjust prompts\n",
    "6. **Concurrency Issues**: Add locks, use thread pools\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Deploy RAG system to production\n",
    "- Implement monitoring dashboards\n",
    "- Set up alerting\n",
    "- Conduct load testing\n",
    "- Gather user feedback\n",
    "- Iterate based on metrics\n",
    "- Explore advanced frameworks:\n",
    "  - **LangChain**: RAG orchestration\n",
    "  - **LlamaIndex**: Advanced indexing\n",
    "  - **Weaviate**: Vector database\n",
    "  - **Pinecone**: Managed vector DB\n",
    "\n",
    "**Congratulations!** You've completed the RAG Fundamentals week and built enterprise-grade RAG systems! 🎉\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
