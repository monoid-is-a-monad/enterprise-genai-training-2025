{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6bd43e",
   "metadata": {},
   "source": [
    "# Lab 2: Basic RAG Implementation\n",
    "\n",
    "**Week 4 - RAG Fundamentals**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand Retrieval-Augmented Generation (RAG) architecture\n",
    "- Implement document chunking strategies\n",
    "- Build a complete RAG pipeline from scratch\n",
    "- Integrate retrieval with LLM generation\n",
    "- Handle different document types\n",
    "- Implement context assembly techniques\n",
    "- Evaluate RAG system quality\n",
    "- Optimize retrieval and generation balance\n",
    "- Build a question-answering system\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 4 Lab 1 (Embeddings & Semantic Search)\n",
    "- Understanding of vector databases\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cd94c",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48018181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken numpy scikit-learn pandas pypdf python-docx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4496fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# For similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48034733",
   "metadata": {},
   "source": [
    "## Part 1: Understanding RAG Architecture\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) combines retrieval and generation:\n",
    "\n",
    "### RAG Pipeline:\n",
    "1. **Document Ingestion**: Load and preprocess documents\n",
    "2. **Chunking**: Split documents into manageable pieces\n",
    "3. **Embedding**: Generate vector embeddings for chunks\n",
    "4. **Indexing**: Store embeddings in vector database\n",
    "5. **Retrieval**: Find relevant chunks for a query\n",
    "6. **Context Assembly**: Combine retrieved chunks\n",
    "7. **Generation**: Generate answer using LLM + context\n",
    "\n",
    "Let's build each component:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72186a",
   "metadata": {},
   "source": [
    "## Part 2: Document Chunking\n",
    "\n",
    "Splitting documents into optimal chunks is crucial for RAG quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4514f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A document chunk.\"\"\"\n",
    "    content: str\n",
    "    chunk_id: str\n",
    "    document_id: str\n",
    "    chunk_index: int\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    start_char: int = 0\n",
    "    end_char: int = 0\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Chunk documents using various strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "        tokenizer_name: str = \"cl100k_base\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize chunker.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Target chunk size in tokens\n",
    "            chunk_overlap: Overlap between chunks in tokens\n",
    "            tokenizer_name: Tokenizer to use\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
    "    \n",
    "    def chunk_by_tokens(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str = \"doc\"\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        Chunk text by token count.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            document_id: Document identifier\n",
    "        \n",
    "        Returns:\n",
    "            List of chunks\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start_idx < len(tokens):\n",
    "            # Get chunk tokens\n",
    "            end_idx = min(start_idx + self.chunk_size, len(tokens))\n",
    "            chunk_tokens = tokens[start_idx:end_idx]\n",
    "            \n",
    "            # Decode back to text\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            \n",
    "            # Create chunk\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                document_id=document_id,\n",
    "                chunk_index=chunk_index,\n",
    "                metadata={\n",
    "                    \"token_count\": len(chunk_tokens),\n",
    "                    \"char_count\": len(chunk_text)\n",
    "                },\n",
    "                start_char=start_idx,\n",
    "                end_char=end_idx\n",
    "            )\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Move to next chunk with overlap\n",
    "            start_idx += self.chunk_size - self.chunk_overlap\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_sentences(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str = \"doc\"\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        Chunk text by sentences, respecting token limits.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            document_id: Document identifier\n",
    "        \n",
    "        Returns:\n",
    "            List of chunks\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
    "            \n",
    "            # If adding this sentence exceeds limit, save current chunk\n",
    "            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                \n",
    "                chunk = Chunk(\n",
    "                    content=chunk_text,\n",
    "                    chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                    document_id=document_id,\n",
    "                    chunk_index=chunk_index,\n",
    "                    metadata={\n",
    "                        \"sentence_count\": len(current_chunk),\n",
    "                        \"token_count\": current_tokens\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                chunk_index += 1\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                if self.chunk_overlap > 0 and len(current_chunk) > 1:\n",
    "                    # Keep last sentence for overlap\n",
    "                    current_chunk = [current_chunk[-1]]\n",
    "                    current_tokens = len(self.tokenizer.encode(current_chunk[0]))\n",
    "                else:\n",
    "                    current_chunk = []\n",
    "                    current_tokens = 0\n",
    "            \n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunk = Chunk(\n",
    "                content=chunk_text,\n",
    "                chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                document_id=document_id,\n",
    "                chunk_index=chunk_index,\n",
    "                metadata={\n",
    "                    \"sentence_count\": len(current_chunk),\n",
    "                    \"token_count\": current_tokens\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_paragraphs(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str = \"doc\"\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        Chunk text by paragraphs.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            document_id: Document identifier\n",
    "        \n",
    "        Returns:\n",
    "            List of chunks\n",
    "        \"\"\"\n",
    "        # Split by double newlines\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            para_tokens = len(self.tokenizer.encode(para))\n",
    "            \n",
    "            # If paragraph alone exceeds limit, chunk it separately\n",
    "            if para_tokens > self.chunk_size:\n",
    "                # Save current chunk if any\n",
    "                if current_chunk:\n",
    "                    chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "                    chunks.append(Chunk(\n",
    "                        content=chunk_text,\n",
    "                        chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                        document_id=document_id,\n",
    "                        chunk_index=chunk_index\n",
    "                    ))\n",
    "                    chunk_index += 1\n",
    "                    current_chunk = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Chunk the large paragraph by sentences\n",
    "                para_chunks = self.chunk_by_sentences(para, f\"{document_id}_para\")\n",
    "                for pc in para_chunks:\n",
    "                    pc.chunk_id = f\"{document_id}_chunk_{chunk_index}\"\n",
    "                    pc.chunk_index = chunk_index\n",
    "                    chunks.append(pc)\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # If adding this paragraph exceeds limit, save current chunk\n",
    "            if current_tokens + para_tokens > self.chunk_size and current_chunk:\n",
    "                chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "                chunks.append(Chunk(\n",
    "                    content=chunk_text,\n",
    "                    chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                    document_id=document_id,\n",
    "                    chunk_index=chunk_index,\n",
    "                    metadata={\"paragraph_count\": len(current_chunk)}\n",
    "                ))\n",
    "                chunk_index += 1\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            current_chunk.append(para)\n",
    "            current_tokens += para_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "            chunks.append(Chunk(\n",
    "                content=chunk_text,\n",
    "                chunk_id=f\"{document_id}_chunk_{chunk_index}\",\n",
    "                document_id=document_id,\n",
    "                chunk_index=chunk_index,\n",
    "                metadata={\"paragraph_count\": len(current_chunk)}\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test chunking strategies\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming the world. Machine learning algorithms can now recognize patterns in data with unprecedented accuracy.\n",
    "\n",
    "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can learn hierarchical representations of data.\n",
    "\n",
    "Natural language processing (NLP) enables computers to understand human language. Recent advances in transformer models have revolutionized NLP tasks.\n",
    "\n",
    "Computer vision allows machines to interpret visual information. Applications range from facial recognition to autonomous vehicles.\n",
    "\n",
    "The future of AI holds both promise and challenges. Ethical considerations must guide AI development and deployment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DOCUMENT CHUNKING STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chunker = DocumentChunker(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Test each strategy\n",
    "strategies = [\n",
    "    (\"Token-based\", lambda: chunker.chunk_by_tokens(sample_text, \"sample\")),\n",
    "    (\"Sentence-based\", lambda: chunker.chunk_by_sentences(sample_text, \"sample\")),\n",
    "    (\"Paragraph-based\", lambda: chunker.chunk_by_paragraphs(sample_text, \"sample\"))\n",
    "]\n",
    "\n",
    "for strategy_name, chunk_func in strategies:\n",
    "    chunks = chunk_func()\n",
    "    print(f\"\\n{strategy_name} Chunking:\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:2]):  # Show first 2 chunks\n",
    "        print(f\"\\n  Chunk {i+1}:\")\n",
    "        print(f\"    Content: {chunk.content[:100]}...\")\n",
    "        print(f\"    Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b13dc",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Implement Smart Chunking\n",
    "\n",
    "Create an intelligent chunking strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861be484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement smart chunking\n",
    "\n",
    "class SmartChunker(DocumentChunker):\n",
    "    \"\"\"\n",
    "    TODO: Implement intelligent chunking that:\n",
    "    \n",
    "    1. Detects document structure (headings, sections)\n",
    "    2. Keeps related content together\n",
    "    3. Avoids splitting mid-sentence or mid-paragraph\n",
    "    4. Handles code blocks specially (don't split code)\n",
    "    5. Preserves markdown/formatting\n",
    "    6. Adds context from headings to chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    def chunk_with_structure(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str = \"doc\"\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        TODO: Chunk while preserving document structure.\n",
    "        \n",
    "        Should handle:\n",
    "        - Markdown headings (# ## ###)\n",
    "        - Code blocks (```)\n",
    "        - Lists\n",
    "        - Tables\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add_context_to_chunks(\n",
    "        self,\n",
    "        chunks: List[Chunk],\n",
    "        context_window: int = 2\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        TODO: Add surrounding context to each chunk.\n",
    "        \n",
    "        For each chunk, add snippets from previous/next chunks\n",
    "        to provide better context.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your smart chunker\n",
    "# smart_chunker = SmartChunker(chunk_size=300)\n",
    "# markdown_text = \"\"\"\n",
    "# # Introduction\n",
    "# This is the introduction.\n",
    "# \n",
    "# ## Technical Details\n",
    "# Here are the technical details...\n",
    "# \"\"\"\n",
    "# chunks = smart_chunker.chunk_with_structure(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b837d",
   "metadata": {},
   "source": [
    "## Part 3: Building the RAG System\n",
    "\n",
    "Complete RAG implementation with retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe08d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"Get embedding for text.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Complete Retrieval-Augmented Generation system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-3.5-turbo\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG system.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Model for embeddings\n",
    "            llm_model: Model for generation\n",
    "            chunk_size: Chunk size in tokens\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        self.chunker = DocumentChunker(chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Storage\n",
    "        self.chunks: List[Chunk] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        \n",
    "        # Statistics\n",
    "        self.query_count = 0\n",
    "        self.total_retrieval_time = 0.0\n",
    "        self.total_generation_time = 0.0\n",
    "    \n",
    "    def ingest_document(\n",
    "        self,\n",
    "        text: str,\n",
    "        document_id: str,\n",
    "        chunking_strategy: str = \"sentences\",\n",
    "        metadata: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingest a document into the system.\n",
    "        \n",
    "        Args:\n",
    "            text: Document text\n",
    "            document_id: Unique document identifier\n",
    "            chunking_strategy: Strategy to use ('tokens', 'sentences', 'paragraphs')\n",
    "            metadata: Optional metadata for the document\n",
    "        \"\"\"\n",
    "        print(f\"Ingesting document: {document_id}\")\n",
    "        \n",
    "        # Chunk document\n",
    "        if chunking_strategy == \"tokens\":\n",
    "            chunks = self.chunker.chunk_by_tokens(text, document_id)\n",
    "        elif chunking_strategy == \"sentences\":\n",
    "            chunks = self.chunker.chunk_by_sentences(text, document_id)\n",
    "        elif chunking_strategy == \"paragraphs\":\n",
    "            chunks = self.chunker.chunk_by_paragraphs(text, document_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {chunking_strategy}\")\n",
    "        \n",
    "        # Add metadata to chunks\n",
    "        if metadata:\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update(metadata)\n",
    "        \n",
    "        print(f\"  Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"  Generating embeddings...\")\n",
    "        for chunk in chunks:\n",
    "            embedding = get_embedding(chunk.content, self.embedding_model)\n",
    "            chunk.metadata['embedding'] = embedding\n",
    "        \n",
    "        # Add to storage\n",
    "        self.chunks.extend(chunks)\n",
    "        self._rebuild_embeddings()\n",
    "        \n",
    "        print(f\"✓ Document ingested. Total chunks: {len(self.chunks)}\")\n",
    "    \n",
    "    def ingest_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Ingest multiple documents.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dicts with 'text', 'id', and optional 'metadata'\n",
    "        \"\"\"\n",
    "        for doc in documents:\n",
    "            self.ingest_document(\n",
    "                text=doc['text'],\n",
    "                document_id=doc['id'],\n",
    "                metadata=doc.get('metadata')\n",
    "            )\n",
    "    \n",
    "    def _rebuild_embeddings(self):\n",
    "        \"\"\"Rebuild embeddings matrix.\"\"\"\n",
    "        if self.chunks:\n",
    "            embeddings_list = [chunk.metadata['embedding'] for chunk in self.chunks]\n",
    "            self.embeddings = np.array(embeddings_list)\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 3,\n",
    "        min_similarity: float = 0.0\n",
    "    ) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of chunks to retrieve\n",
    "            min_similarity: Minimum similarity threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if not self.chunks:\n",
    "            return []\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(get_embedding(query, self.embedding_model))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1),\n",
    "            self.embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Filter by minimum similarity\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            score = similarities[idx]\n",
    "            if score >= min_similarity:\n",
    "                results.append((self.chunks[idx], float(score)))\n",
    "        \n",
    "        # Update stats\n",
    "        self.total_retrieval_time += time.time() - start_time\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_chunks: List[Chunk],\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 500\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using LLM with retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_chunks: Retrieved chunks\n",
    "            temperature: LLM temperature\n",
    "            max_tokens: Maximum tokens in response\n",
    "        \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Assemble context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {chunk.content}\"\n",
    "            for i, chunk in enumerate(context_chunks)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Answer the question based on the context provided. If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Update stats\n",
    "        self.total_generation_time += time.time() - start_time\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "        min_similarity: float = 0.0,\n",
    "        return_sources: bool = True,\n",
    "        verbose: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            top_k: Number of chunks to retrieve\n",
    "            min_similarity: Minimum similarity threshold\n",
    "            return_sources: Include source chunks in response\n",
    "            verbose: Print intermediate steps\n",
    "        \n",
    "        Returns:\n",
    "            Dict with answer and metadata\n",
    "        \"\"\"\n",
    "        self.query_count += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Query: {question}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        retrieved = self.retrieve(question, top_k, min_similarity)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRetrieved {len(retrieved)} chunks:\")\n",
    "            for i, (chunk, score) in enumerate(retrieved, 1):\n",
    "                print(f\"  {i}. Score: {score:.4f}\")\n",
    "                print(f\"     {chunk.content[:100]}...\")\n",
    "        \n",
    "        if not retrieved:\n",
    "            return {\n",
    "                \"answer\": \"I don't have any relevant information to answer that question.\",\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0\n",
    "            }\n",
    "        \n",
    "        # Generate answer\n",
    "        chunks = [chunk for chunk, _ in retrieved]\n",
    "        answer = self.generate_answer(question, chunks)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "        \n",
    "        # Prepare response\n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"num_sources\": len(retrieved)\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            result[\"sources\"] = [\n",
    "                {\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"document_id\": chunk.document_id,\n",
    "                    \"content\": chunk.content,\n",
    "                    \"score\": score,\n",
    "                    \"metadata\": {k: v for k, v in chunk.metadata.items() if k != 'embedding'}\n",
    "                }\n",
    "                for chunk, score in retrieved\n",
    "            ]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system statistics.\"\"\"\n",
    "        return {\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"total_queries\": self.query_count,\n",
    "            \"avg_retrieval_time\": self.total_retrieval_time / max(self.query_count, 1),\n",
    "            \"avg_generation_time\": self.total_generation_time / max(self.query_count, 1),\n",
    "            \"embedding_model\": self.embedding_model,\n",
    "            \"llm_model\": self.llm_model\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save RAG system to file.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"✓ RAG system saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath: str) -> 'RAGSystem':\n",
    "        \"\"\"Load RAG system from file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            rag = pickle.load(f)\n",
    "        print(f\"✓ RAG system loaded from {filepath}\")\n",
    "        return rag\n",
    "\n",
    "# Test RAG system\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING RAG SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create RAG system\n",
    "rag = RAGSystem(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Sample knowledge base\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"ai_basics\",\n",
    "        \"text\": \"\"\"\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence by machines. \n",
    "It encompasses various subfields including machine learning, deep learning, and natural language processing.\n",
    "\n",
    "Machine learning algorithms learn patterns from data without explicit programming. \n",
    "They improve their performance through experience.\n",
    "\n",
    "Deep learning uses neural networks with multiple layers to learn hierarchical representations. \n",
    "It has achieved remarkable results in image recognition, speech recognition, and language translation.\n",
    "\n",
    "Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. \n",
    "Modern NLP uses transformer architectures like BERT and GPT.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"topic\": \"AI\", \"category\": \"fundamentals\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"python_programming\",\n",
    "        \"text\": \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Python features dynamic typing and automatic memory management. \n",
    "It supports multiple programming paradigms including procedural, object-oriented, and functional programming.\n",
    "\n",
    "Python's extensive standard library and third-party packages make it suitable for various applications. \n",
    "Popular libraries include NumPy for numerical computing, Pandas for data analysis, and TensorFlow for machine learning.\n",
    "\n",
    "Python is widely used in data science, web development, automation, and scientific computing. \n",
    "Its syntax emphasizes code readability and allows programmers to express concepts in fewer lines of code.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"topic\": \"Programming\", \"category\": \"languages\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"web_development\",\n",
    "        \"text\": \"\"\"\n",
    "Web development involves building applications that run on the internet. \n",
    "It consists of front-end development (user interface) and back-end development (server-side logic).\n",
    "\n",
    "Front-end technologies include HTML for structure, CSS for styling, and JavaScript for interactivity. \n",
    "Popular frameworks include React, Vue, and Angular.\n",
    "\n",
    "Back-end development handles server logic, databases, and APIs. \n",
    "Common technologies include Node.js, Python (Django/Flask), and Java (Spring).\n",
    "\n",
    "RESTful APIs enable communication between front-end and back-end systems. \n",
    "They use HTTP methods (GET, POST, PUT, DELETE) to perform operations on resources.\n",
    "\n",
    "Modern web development emphasizes responsive design, security, and performance optimization. \n",
    "Progressive Web Apps (PWAs) combine the best of web and mobile applications.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"topic\": \"Web Dev\", \"category\": \"technology\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ingest documents\n",
    "rag.ingest_documents(knowledge_base)\n",
    "\n",
    "# Query the system\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about Python programming\",\n",
    "    \"How do RESTful APIs work?\",\n",
    "    \"What is quantum computing?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING RAG SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = rag.query(query, top_k=2, verbose=True)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEM STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "stats = rag.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6242e74",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Enhance the RAG System\n",
    "\n",
    "Add features to improve RAG quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enhance RAG system\n",
    "\n",
    "class EnhancedRAGSystem(RAGSystem):\n",
    "    \"\"\"\n",
    "    TODO: Add these enhancements:\n",
    "    \n",
    "    1. Re-ranking\n",
    "       - Re-rank retrieved chunks using cross-encoder\n",
    "       - Improve relevance of final context\n",
    "    \n",
    "    2. Query expansion\n",
    "       - Generate multiple variations of user query\n",
    "       - Retrieve with all variations and combine\n",
    "    \n",
    "    3. Chunk deduplication\n",
    "       - Remove duplicate or near-duplicate chunks\n",
    "       - Avoid redundant context\n",
    "    \n",
    "    4. Citation generation\n",
    "       - Track which chunks contributed to answer\n",
    "       - Generate citations/references\n",
    "    \n",
    "    5. Confidence scoring\n",
    "       - Estimate confidence in the answer\n",
    "       - Flag low-confidence responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def rerank_chunks(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[Tuple[Chunk, float]],\n",
    "        top_k: int = 3\n",
    "    ) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Re-rank chunks using more sophisticated scoring.\n",
    "        \n",
    "        Could use:\n",
    "        - Query-chunk relevance (semantic)\n",
    "        - Chunk quality metrics\n",
    "        - Source diversity\n",
    "        - Recency (if timestamps available)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def expand_query(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        TODO: Generate query variations.\n",
    "        \n",
    "        Strategies:\n",
    "        - Rephrase using LLM\n",
    "        - Add synonyms\n",
    "        - Break complex queries into sub-queries\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_with_citations(\n",
    "        self,\n",
    "        query: str,\n",
    "        context_chunks: List[Chunk]\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        TODO: Generate answer with citations.\n",
    "        \n",
    "        Return:\n",
    "        - Answer with [1], [2] markers\n",
    "        - List of source citations\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your enhancements\n",
    "# enhanced_rag = EnhancedRAGSystem()\n",
    "# enhanced_rag.ingest_documents(knowledge_base)\n",
    "# result = enhanced_rag.query(\"What is AI?\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbfcca",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Document RAG\n",
    "\n",
    "Handle multiple document formats and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7941785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"\n",
    "    Load documents from various sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text_file(filepath: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load plain text file.\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return {\n",
    "            \"id\": Path(filepath).stem,\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"source\": filepath,\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_json_file(filepath: str, text_field: str = \"text\") -> Dict[str, Any]:\n",
    "        \"\"\"Load JSON file.\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Handle list of documents\n",
    "        if isinstance(data, list):\n",
    "            return [\n",
    "                {\n",
    "                    \"id\": item.get(\"id\", f\"doc_{i}\"),\n",
    "                    \"text\": item.get(text_field, str(item)),\n",
    "                    \"metadata\": {k: v for k, v in item.items() if k not in [\"id\", text_field]}\n",
    "                }\n",
    "                for i, item in enumerate(data)\n",
    "            ]\n",
    "        \n",
    "        # Single document\n",
    "        return {\n",
    "            \"id\": data.get(\"id\", Path(filepath).stem),\n",
    "            \"text\": data.get(text_field, str(data)),\n",
    "            \"metadata\": {k: v for k, v in data.items() if k not in [\"id\", text_field]}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_markdown(filepath: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load markdown file.\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Extract title from first heading\n",
    "        title_match = re.search(r'^#\\s+(.+)$', text, re.MULTILINE)\n",
    "        title = title_match.group(1) if title_match else Path(filepath).stem\n",
    "        \n",
    "        return {\n",
    "            \"id\": Path(filepath).stem,\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"source\": filepath,\n",
    "                \"type\": \"markdown\",\n",
    "                \"title\": title\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_directory(\n",
    "        dirpath: str,\n",
    "        extensions: List[str] = ['.txt', '.md', '.json']\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load all documents from a directory.\n",
    "        \n",
    "        Args:\n",
    "            dirpath: Directory path\n",
    "            extensions: File extensions to load\n",
    "        \n",
    "        Returns:\n",
    "            List of document dicts\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        dir_path = Path(dirpath)\n",
    "        \n",
    "        for ext in extensions:\n",
    "            for filepath in dir_path.glob(f\"**/*{ext}\"):\n",
    "                try:\n",
    "                    if ext == '.json':\n",
    "                        doc = DocumentLoader.load_json_file(str(filepath))\n",
    "                    elif ext == '.md':\n",
    "                        doc = DocumentLoader.load_markdown(str(filepath))\n",
    "                    else:\n",
    "                        doc = DocumentLoader.load_text_file(str(filepath))\n",
    "                    \n",
    "                    # Handle list of documents\n",
    "                    if isinstance(doc, list):\n",
    "                        documents.extend(doc)\n",
    "                    else:\n",
    "                        documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filepath}: {e}\")\n",
    "        \n",
    "        return documents\n",
    "\n",
    "class MultiDocumentRAG(RAGSystem):\n",
    "    \"\"\"\n",
    "    RAG system with multi-document support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize multi-document RAG.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.document_metadata: Dict[str, Dict] = {}\n",
    "    \n",
    "    def ingest_from_file(self, filepath: str):\n",
    "        \"\"\"Ingest document from file.\"\"\"\n",
    "        ext = Path(filepath).suffix.lower()\n",
    "        \n",
    "        if ext == '.json':\n",
    "            doc = DocumentLoader.load_json_file(filepath)\n",
    "        elif ext == '.md':\n",
    "            doc = DocumentLoader.load_markdown(filepath)\n",
    "        else:\n",
    "            doc = DocumentLoader.load_text_file(filepath)\n",
    "        \n",
    "        # Handle list of documents\n",
    "        if isinstance(doc, list):\n",
    "            for d in doc:\n",
    "                self.ingest_document(d['text'], d['id'], metadata=d.get('metadata'))\n",
    "                self.document_metadata[d['id']] = d.get('metadata', {})\n",
    "        else:\n",
    "            self.ingest_document(doc['text'], doc['id'], metadata=doc.get('metadata'))\n",
    "            self.document_metadata[doc['id']] = doc.get('metadata', {})\n",
    "    \n",
    "    def ingest_from_directory(self, dirpath: str):\n",
    "        \"\"\"Ingest all documents from directory.\"\"\"\n",
    "        documents = DocumentLoader.load_directory(dirpath)\n",
    "        \n",
    "        for doc in documents:\n",
    "            self.ingest_document(doc['text'], doc['id'], metadata=doc.get('metadata'))\n",
    "            self.document_metadata[doc['id']] = doc.get('metadata', {})\n",
    "    \n",
    "    def query_with_filters(\n",
    "        self,\n",
    "        question: str,\n",
    "        filters: Optional[Dict[str, Any]] = None,\n",
    "        top_k: int = 3\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query with metadata filters.\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            filters: Metadata filters (e.g., {\"topic\": \"AI\"})\n",
    "            top_k: Number of results\n",
    "        \n",
    "        Returns:\n",
    "            Query results\n",
    "        \"\"\"\n",
    "        # Retrieve chunks\n",
    "        retrieved = self.retrieve(question, top_k=top_k * 2)  # Get more for filtering\n",
    "        \n",
    "        # Apply filters\n",
    "        if filters:\n",
    "            filtered = []\n",
    "            for chunk, score in retrieved:\n",
    "                match = True\n",
    "                for key, value in filters.items():\n",
    "                    if chunk.metadata.get(key) != value:\n",
    "                        match = False\n",
    "                        break\n",
    "                if match:\n",
    "                    filtered.append((chunk, score))\n",
    "            retrieved = filtered[:top_k]\n",
    "        else:\n",
    "            retrieved = retrieved[:top_k]\n",
    "        \n",
    "        # Generate answer\n",
    "        if not retrieved:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found matching the filters.\",\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0\n",
    "            }\n",
    "        \n",
    "        chunks = [chunk for chunk, _ in retrieved]\n",
    "        answer = self.generate_answer(question, chunks)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"num_sources\": len(retrieved),\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"document_id\": chunk.document_id,\n",
    "                    \"content\": chunk.content,\n",
    "                    \"score\": score,\n",
    "                    \"metadata\": {k: v for k, v in chunk.metadata.items() if k != 'embedding'}\n",
    "                }\n",
    "                for chunk, score in retrieved\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def list_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all ingested documents.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"document_id\": doc_id,\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            for doc_id, metadata in self.document_metadata.items()\n",
    "        ]\n",
    "\n",
    "# Test multi-document RAG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-DOCUMENT RAG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "multi_rag = MultiDocumentRAG()\n",
    "\n",
    "# Ingest our previous knowledge base\n",
    "multi_rag.ingest_documents(knowledge_base)\n",
    "\n",
    "# List documents\n",
    "print(\"\\nIngested documents:\")\n",
    "for doc in multi_rag.list_documents():\n",
    "    print(f\"  - {doc['document_id']}: {doc['metadata']}\")\n",
    "\n",
    "# Query with filters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Querying with filters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = multi_rag.query_with_filters(\n",
    "    \"Tell me about programming\",\n",
    "    filters={\"topic\": \"Programming\"},\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: Tell me about programming\")\n",
    "print(f\"Filter: topic='Programming'\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(f\"\\nSources used: {result['num_sources']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b91cbd",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Build a Document Management System\n",
    "\n",
    "Create a system to manage documents in RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build document management system\n",
    "\n",
    "class DocumentManager:\n",
    "    \"\"\"\n",
    "    TODO: Implement document management features:\n",
    "    \n",
    "    1. Document versioning\n",
    "       - Track document versions\n",
    "       - Update documents without losing history\n",
    "    \n",
    "    2. Document deletion\n",
    "       - Remove documents and their chunks\n",
    "       - Update embeddings index\n",
    "    \n",
    "    3. Document search\n",
    "       - Find documents by metadata\n",
    "       - List documents by filters\n",
    "    \n",
    "    4. Document statistics\n",
    "       - Track document usage (how often retrieved)\n",
    "       - Identify most/least useful documents\n",
    "    \n",
    "    5. Document refresh\n",
    "       - Detect when source documents change\n",
    "       - Auto-refresh embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        \"\"\"Initialize document manager.\"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.document_versions: Dict[str, List[str]] = {}\n",
    "    \n",
    "    def update_document(self, document_id: str, new_text: str):\n",
    "        \"\"\"TODO: Update document and maintain version history.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def delete_document(self, document_id: str):\n",
    "        \"\"\"TODO: Remove document and its chunks.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_document_stats(self, document_id: str) -> Dict:\n",
    "        \"\"\"TODO: Get statistics for a document.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test document management\n",
    "# manager = DocumentManager(multi_rag)\n",
    "# manager.update_document(\"ai_basics\", \"Updated content...\")\n",
    "# stats = manager.get_document_stats(\"ai_basics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20a398",
   "metadata": {},
   "source": [
    "## Part 5: RAG Evaluation\n",
    "\n",
    "Evaluate RAG system quality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate RAG system performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        \"\"\"Initialize evaluator.\"\"\"\n",
    "        self.rag = rag_system\n",
    "    \n",
    "    def evaluate_retrieval(\n",
    "        self,\n",
    "        test_cases: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate retrieval quality.\n",
    "        \n",
    "        Args:\n",
    "            test_cases: List of dicts with 'query' and 'relevant_doc_ids'\n",
    "        \n",
    "        Returns:\n",
    "            Metrics dict\n",
    "        \"\"\"\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        total_f1 = 0.0\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['query']\n",
    "            relevant_ids = set(test_case['relevant_doc_ids'])\n",
    "            \n",
    "            # Retrieve chunks\n",
    "            retrieved = self.rag.retrieve(query, top_k=5)\n",
    "            retrieved_doc_ids = set(chunk.document_id for chunk, _ in retrieved)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if not retrieved_doc_ids:\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                f1 = 0.0\n",
    "            else:\n",
    "                true_positives = len(relevant_ids & retrieved_doc_ids)\n",
    "                precision = true_positives / len(retrieved_doc_ids)\n",
    "                recall = true_positives / len(relevant_ids) if relevant_ids else 0.0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            \n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "        \n",
    "        n = len(test_cases)\n",
    "        return {\n",
    "            \"precision\": total_precision / n,\n",
    "            \"recall\": total_recall / n,\n",
    "            \"f1\": total_f1 / n\n",
    "        }\n",
    "    \n",
    "    def evaluate_generation(\n",
    "        self,\n",
    "        test_cases: List[Dict[str, Any]],\n",
    "        use_llm: bool = True\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate generation quality.\n",
    "        \n",
    "        Args:\n",
    "            test_cases: List of dicts with 'query' and 'expected_answer'\n",
    "            use_llm: Use LLM to evaluate answer quality\n",
    "        \n",
    "        Returns:\n",
    "            Metrics dict\n",
    "        \"\"\"\n",
    "        if not use_llm:\n",
    "            # Simple string matching (not very useful)\n",
    "            return {\"note\": \"LLM evaluation disabled\"}\n",
    "        \n",
    "        total_score = 0.0\n",
    "        scores = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            query = test_case['query']\n",
    "            expected = test_case['expected_answer']\n",
    "            \n",
    "            # Get actual answer\n",
    "            result = self.rag.query(query, verbose=False)\n",
    "            actual = result['answer']\n",
    "            \n",
    "            # Use LLM to evaluate\n",
    "            eval_prompt = f\"\"\"Evaluate how well the actual answer matches the expected answer.\n",
    "Rate from 0-10 where:\n",
    "- 10: Perfect match in meaning\n",
    "- 7-9: Mostly correct with minor differences\n",
    "- 4-6: Partially correct\n",
    "- 1-3: Mostly incorrect\n",
    "- 0: Completely incorrect\n",
    "\n",
    "Expected answer: {expected}\n",
    "Actual answer: {actual}\n",
    "\n",
    "Rating (just the number):\"\"\"\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=10\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                score = float(response.choices[0].message.content.strip())\n",
    "                scores.append(score)\n",
    "                total_score += score\n",
    "            except:\n",
    "                scores.append(0)\n",
    "        \n",
    "        return {\n",
    "            \"avg_score\": total_score / len(test_cases),\n",
    "            \"min_score\": min(scores),\n",
    "            \"max_score\": max(scores),\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_latency(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        num_runs: int = 10\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate system latency.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of test queries\n",
    "            num_runs: Number of runs per query\n",
    "        \n",
    "        Returns:\n",
    "            Latency metrics\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        retrieval_times = []\n",
    "        generation_times = []\n",
    "        total_times = []\n",
    "        \n",
    "        for query in queries:\n",
    "            for _ in range(num_runs):\n",
    "                start = time.time()\n",
    "                \n",
    "                # Retrieval\n",
    "                retrieval_start = time.time()\n",
    "                retrieved = self.rag.retrieve(query, top_k=3)\n",
    "                retrieval_time = time.time() - retrieval_start\n",
    "                retrieval_times.append(retrieval_time)\n",
    "                \n",
    "                # Generation\n",
    "                generation_start = time.time()\n",
    "                chunks = [chunk for chunk, _ in retrieved]\n",
    "                if chunks:\n",
    "                    self.rag.generate_answer(query, chunks)\n",
    "                generation_time = time.time() - generation_start\n",
    "                generation_times.append(generation_time)\n",
    "                \n",
    "                total_time = time.time() - start\n",
    "                total_times.append(total_time)\n",
    "        \n",
    "        return {\n",
    "            \"avg_retrieval_ms\": np.mean(retrieval_times) * 1000,\n",
    "            \"avg_generation_ms\": np.mean(generation_times) * 1000,\n",
    "            \"avg_total_ms\": np.mean(total_times) * 1000,\n",
    "            \"p50_total_ms\": np.percentile(total_times, 50) * 1000,\n",
    "            \"p95_total_ms\": np.percentile(total_times, 95) * 1000,\n",
    "            \"p99_total_ms\": np.percentile(total_times, 99) * 1000\n",
    "        }\n",
    "\n",
    "# Test RAG evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluator = RAGEvaluator(rag)\n",
    "\n",
    "# Retrieval evaluation\n",
    "retrieval_test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"relevant_doc_ids\": [\"ai_basics\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I use Python for data analysis?\",\n",
    "        \"relevant_doc_ids\": [\"python_programming\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are RESTful APIs?\",\n",
    "        \"relevant_doc_ids\": [\"web_development\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nRetrieval Evaluation:\")\n",
    "retrieval_metrics = evaluator.evaluate_retrieval(retrieval_test_cases)\n",
    "for metric, value in retrieval_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Latency evaluation\n",
    "print(\"\\nLatency Evaluation:\")\n",
    "test_queries = [\"What is AI?\", \"Tell me about Python\"]\n",
    "latency_metrics = evaluator.evaluate_latency(test_queries, num_runs=5)\n",
    "for metric, value in latency_metrics.items():\n",
    "    print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f1ee5",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Build Comprehensive Evaluation Suite\n",
    "\n",
    "Create a complete evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build evaluation suite\n",
    "\n",
    "class ComprehensiveEvaluator(RAGEvaluator):\n",
    "    \"\"\"\n",
    "    TODO: Add comprehensive evaluation metrics:\n",
    "    \n",
    "    1. Retrieval metrics\n",
    "       - Precision@K, Recall@K, F1@K\n",
    "       - Mean Average Precision (MAP)\n",
    "       - Normalized Discounted Cumulative Gain (NDCG)\n",
    "    \n",
    "    2. Generation metrics\n",
    "       - Faithfulness (answer grounded in context?)\n",
    "       - Relevance (answers the question?)\n",
    "       - Coherence\n",
    "       - BLEU/ROUGE scores\n",
    "    \n",
    "    3. End-to-end metrics\n",
    "       - User satisfaction (simulated)\n",
    "       - Answer completeness\n",
    "       - Citation quality\n",
    "    \n",
    "    4. Efficiency metrics\n",
    "       - Tokens used per query\n",
    "       - Cost per query\n",
    "       - Cache hit rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def evaluate_faithfulness(\n",
    "        self,\n",
    "        answer: str,\n",
    "        context: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        TODO: Evaluate if answer is grounded in context.\n",
    "        \n",
    "        Use LLM to check if answer contains information\n",
    "        not present in the context.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calculate_ndcg(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved: List[Tuple[Chunk, float]],\n",
    "        relevance_scores: List[float]\n",
    "    ) -> float:\n",
    "        \"\"\"TODO: Calculate NDCG score.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def evaluate_cost_efficiency(\n",
    "        self,\n",
    "        queries: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        TODO: Calculate cost efficiency.\n",
    "        \n",
    "        Track:\n",
    "        - Tokens used\n",
    "        - API calls made\n",
    "        - Estimated cost\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test comprehensive evaluation\n",
    "# comp_eval = ComprehensiveEvaluator(rag)\n",
    "# metrics = comp_eval.evaluate_faithfulness(answer, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0e13f",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Conversational RAG\n",
    "\n",
    "Build RAG system with conversation memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalRAG(RAGSystem):\n",
    "    \"\"\"\n",
    "    RAG system with conversation history.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Maintain conversation history\n",
    "    2. Use history for context in queries\n",
    "    3. Handle follow-up questions (\"What about its applications?\")\n",
    "    4. Conversation summarization for long conversations\n",
    "    5. Multi-turn query understanding\n",
    "    6. Conversation branching/reset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize conversational RAG.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conversations: Dict[str, List[Dict]] = {}\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        conversation_id: str = \"default\",\n",
    "        top_k: int = 3\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Chat with conversation memory.\n",
    "        \n",
    "        Should:\n",
    "        - Understand context from previous messages\n",
    "        - Resolve pronouns/references\n",
    "        - Handle follow-ups naturally\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reset_conversation(self, conversation_id: str = \"default\"):\n",
    "        \"\"\"TODO: Reset conversation history.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# conv_rag = ConversationalRAG()\n",
    "# conv_rag.ingest_documents(knowledge_base)\n",
    "# conv_rag.chat(\"What is AI?\")\n",
    "# conv_rag.chat(\"Tell me more about its applications\")  # Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7b30c",
   "metadata": {},
   "source": [
    "### Challenge 2: Hybrid Search RAG\n",
    "\n",
    "Combine semantic and keyword search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearchRAG(RAGSystem):\n",
    "    \"\"\"\n",
    "    RAG with hybrid semantic + keyword search.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. BM25 for keyword search\n",
    "    2. Combine BM25 and semantic scores\n",
    "    3. Reciprocal Rank Fusion (RRF)\n",
    "    4. Query analysis to choose strategy\n",
    "    5. Per-query weight adjustment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize hybrid search RAG.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bm25_index = None\n",
    "    \n",
    "    def build_bm25_index(self):\n",
    "        \"\"\"TODO: Build BM25 index for keyword search.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def hybrid_retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 3,\n",
    "        semantic_weight: float = 0.7\n",
    "    ) -> List[Tuple[Chunk, float]]:\n",
    "        \"\"\"\n",
    "        TODO: Retrieve using hybrid search.\n",
    "        \n",
    "        Combine:\n",
    "        - Semantic similarity scores\n",
    "        - BM25 keyword scores\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# hybrid_rag = HybridSearchRAG()\n",
    "# hybrid_rag.ingest_documents(knowledge_base)\n",
    "# hybrid_rag.build_bm25_index()\n",
    "# results = hybrid_rag.hybrid_retrieve(\"machine learning algorithms\", semantic_weight=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21a4d2",
   "metadata": {},
   "source": [
    "### Challenge 3: Adaptive RAG\n",
    "\n",
    "RAG system that adapts based on performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd23e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRAG(RAGSystem):\n",
    "    \"\"\"\n",
    "    Self-improving RAG system.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Track query success/failure\n",
    "    2. Learn optimal top_k per query type\n",
    "    3. Adjust chunking strategy based on performance\n",
    "    4. A/B test different retrieval strategies\n",
    "    5. User feedback integration\n",
    "    6. Automatic reindexing when performance degrades\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize adaptive RAG.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.performance_history = []\n",
    "        self.optimal_params = {}\n",
    "    \n",
    "    def query_with_feedback(\n",
    "        self,\n",
    "        question: str,\n",
    "        user_feedback: Optional[Dict] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        TODO: Query and learn from feedback.\n",
    "        \n",
    "        Feedback could include:\n",
    "        - Was answer helpful? (yes/no)\n",
    "        - What was wrong? (irrelevant/incomplete/incorrect)\n",
    "        - Better answer (for training)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"TODO: Analyze history and adjust parameters.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def suggest_improvements(self) -> List[str]:\n",
    "        \"\"\"TODO: Suggest system improvements based on performance.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# adaptive_rag = AdaptiveRAG()\n",
    "# adaptive_rag.ingest_documents(knowledge_base)\n",
    "# result = adaptive_rag.query_with_feedback(\"What is AI?\", feedback={\"helpful\": True})\n",
    "# adaptive_rag.optimize_parameters()\n",
    "# suggestions = adaptive_rag.suggest_improvements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76b69b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ RAG architecture and pipeline\n",
    "2. ✅ Document chunking strategies\n",
    "3. ✅ Building complete RAG systems\n",
    "4. ✅ Integrating retrieval with generation\n",
    "5. ✅ Multi-document support\n",
    "6. ✅ RAG evaluation metrics\n",
    "7. ✅ Performance optimization\n",
    "8. ✅ Quality assessment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**RAG Benefits:**\n",
    "- Grounds LLM responses in factual data\n",
    "- Enables knowledge updates without retraining\n",
    "- Reduces hallucinations\n",
    "- Provides source attribution\n",
    "- Cost-effective for domain-specific applications\n",
    "\n",
    "**Critical Components:**\n",
    "1. **Chunking**: Balance between context and specificity\n",
    "2. **Retrieval**: Find truly relevant information\n",
    "3. **Context Assembly**: Provide coherent context to LLM\n",
    "4. **Generation**: Produce accurate, grounded answers\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "**Chunking:**\n",
    "- 300-500 tokens per chunk (adjust for use case)\n",
    "- Use overlap to preserve context\n",
    "- Respect document structure (paragraphs, sections)\n",
    "- Don't split mid-sentence\n",
    "\n",
    "**Retrieval:**\n",
    "- Start with top_k=3-5, adjust based on testing\n",
    "- Set minimum similarity threshold\n",
    "- Consider multiple retrieval strategies\n",
    "- Deduplicate similar chunks\n",
    "\n",
    "**Generation:**\n",
    "- Clear instructions in prompt\n",
    "- Explicit grounding requirement\n",
    "- Handle \"no information\" cases\n",
    "- Include source attribution\n",
    "\n",
    "**Evaluation:**\n",
    "- Test with real queries\n",
    "- Measure retrieval quality (precision/recall)\n",
    "- Assess answer faithfulness\n",
    "- Monitor latency and costs\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Chunks too large**: Irrelevant information\n",
    "2. **Chunks too small**: Missing context\n",
    "3. **No overlap**: Lost information at boundaries\n",
    "4. **Wrong top_k**: Too few (miss info) or too many (noise)\n",
    "5. **Poor prompts**: LLM ignores context or hallucinates\n",
    "6. **No evaluation**: Unknown quality issues\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Experiment with different chunking strategies\n",
    "- Test with your own documents\n",
    "- Move on to Lab 3: Enterprise RAG System\n",
    "- Explore production RAG frameworks (LangChain, LlamaIndex)\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
