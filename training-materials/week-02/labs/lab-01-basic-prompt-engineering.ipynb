{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664a7955",
   "metadata": {},
   "source": [
    "# Lab 1: Basic Prompt Engineering\n",
    "\n",
    "**Week 2 - Prompt Engineering & LLM Basics**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Master the fundamentals of prompt engineering\n",
    "- Understand prompt components (instructions, context, examples, constraints)\n",
    "- Practice different prompt styles and formats\n",
    "- Learn to optimize prompts iteratively\n",
    "- Handle common prompting challenges\n",
    "- Build a reusable prompt library\n",
    "- Evaluate and measure prompt effectiveness\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 1 labs\n",
    "- OpenAI API key configured\n",
    "- Understanding of API parameters\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88a0da",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83788f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ebae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456a629",
   "metadata": {},
   "source": [
    "## Part 1: Anatomy of a Good Prompt\n",
    "\n",
    "A well-structured prompt typically contains these components:\n",
    "\n",
    "1. **Instruction**: What you want the model to do\n",
    "2. **Context**: Background information or setting\n",
    "3. **Input Data**: The specific data to process\n",
    "4. **Output Format**: How you want the response structured\n",
    "5. **Constraints**: Limitations or requirements\n",
    "6. **Examples**: Sample inputs/outputs (optional but powerful)\n",
    "\n",
    "Let's explore each component:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abacc4",
   "metadata": {},
   "source": [
    "### Component 1: Instructions\n",
    "\n",
    "Clear, specific instructions are the foundation of good prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b317ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_instructions():\n",
    "    \"\"\"Compare vague vs specific instructions.\"\"\"\n",
    "    \n",
    "    # Example 1: Vague instruction\n",
    "    vague_prompt = \"Tell me about Python.\"\n",
    "    \n",
    "    # Example 2: Specific instruction\n",
    "    specific_prompt = \"Explain the three main differences between lists and tuples in Python, with code examples for each difference.\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"Vague\", vague_prompt),\n",
    "        (\"Specific\", specific_prompt)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label} Instruction:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nResponse:\\n{response.choices[0].message.content}\")\n",
    "        print(f\"\\nTokens used: {response.usage.total_tokens}\")\n",
    "\n",
    "test_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffbd4b",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Improve Vague Instructions\n",
    "\n",
    "Transform these vague instructions into specific, actionable prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e523918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve these vague instructions\n",
    "\n",
    "vague_instructions = [\n",
    "    \"Write about AI.\",\n",
    "    \"Make it better.\",\n",
    "    \"Explain this code.\",\n",
    "    \"Help me with my email.\",\n",
    "    \"What should I do?\"\n",
    "]\n",
    "\n",
    "# Example transformation:\n",
    "# Vague: \"Write about AI\"\n",
    "# Specific: \"Write a 3-paragraph explanation of how neural networks work, suitable for a high school student with no programming background.\"\n",
    "\n",
    "specific_instructions = [\n",
    "    # TODO: Add your improved versions here\n",
    "    \"\",  # Improved version of \"Write about AI\"\n",
    "    \"\",  # Improved version of \"Make it better\"\n",
    "    \"\",  # Improved version of \"Explain this code\"\n",
    "    \"\",  # Improved version of \"Help me with my email\"\n",
    "    \"\",  # Improved version of \"What should I do?\"\n",
    "]\n",
    "\n",
    "# Test your improved instructions\n",
    "# for i, instruction in enumerate(specific_instructions):\n",
    "#     if instruction:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": instruction}],\n",
    "#             temperature=0.7,\n",
    "#             max_tokens=200\n",
    "#         )\n",
    "#         print(f\"\\n{i+1}. {instruction}\")\n",
    "#         print(f\"Response: {response.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ffc68",
   "metadata": {},
   "source": [
    "### Component 2: Context\n",
    "\n",
    "Providing relevant context helps the model understand the situation and generate appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20133cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_without_context():\n",
    "    \"\"\"Compare responses with and without context.\"\"\"\n",
    "    \n",
    "    # Without context\n",
    "    no_context = \"Should I use async or sync functions?\"\n",
    "    \n",
    "    # With context\n",
    "    with_context = \"\"\"\n",
    "    Context: I'm building a web API with FastAPI that needs to handle 1000+ \n",
    "    concurrent requests. The API makes database queries and calls external APIs.\n",
    "    \n",
    "    Question: Should I use async or sync functions for my route handlers?\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"Without Context\", no_context),\n",
    "        (\"With Context\", with_context)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "compare_with_without_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb65347",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Add Context\n",
    "\n",
    "Add relevant context to make these questions more answerable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31635bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add context to these questions\n",
    "\n",
    "questions_needing_context = [\n",
    "    \"What's the best database to use?\",\n",
    "    \"How should I structure my code?\",\n",
    "    \"Is this a good approach?\",\n",
    "    \"What framework should I choose?\",\n",
    "]\n",
    "\n",
    "# Example:\n",
    "# Original: \"What's the best database to use?\"\n",
    "# With Context: \"\"\"\n",
    "# Context: I'm building a real-time chat application that needs to:\n",
    "# - Handle 10,000 concurrent users\n",
    "# - Store message history\n",
    "# - Support full-text search\n",
    "# - Deploy on AWS\n",
    "# \n",
    "# Question: What's the best database to use?\n",
    "# \"\"\"\n",
    "\n",
    "contextualized_questions = [\n",
    "    # TODO: Add your contextualized versions\n",
    "]\n",
    "\n",
    "# Test your questions\n",
    "# for question in contextualized_questions:\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": question}],\n",
    "#         temperature=0.7\n",
    "#     )\n",
    "#     print(f\"\\nQuestion:\\n{question}\")\n",
    "#     print(f\"\\nResponse:\\n{response.choices[0].message.content}\\n\")\n",
    "#     print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0199c",
   "metadata": {},
   "source": [
    "### Component 3: Output Format\n",
    "\n",
    "Specifying the desired output format ensures consistent, parseable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_output_formats():\n",
    "    \"\"\"Test different output format specifications.\"\"\"\n",
    "    \n",
    "    base_instruction = \"List the top 5 programming languages for web development.\"\n",
    "    \n",
    "    formats = {\n",
    "        \"Unstructured\": base_instruction,\n",
    "        \n",
    "        \"Numbered List\": base_instruction + \"\\n\\nFormat: Provide as a numbered list.\",\n",
    "        \n",
    "        \"Table\": base_instruction + \"\\n\\nFormat: Provide as a markdown table with columns: Language, Primary Use, Difficulty Level.\",\n",
    "        \n",
    "        \"JSON\": base_instruction + \"\"\"\n",
    "        \n",
    "        Format: Return as JSON array with objects containing:\n",
    "        - name: language name\n",
    "        - use_case: primary use case\n",
    "        - difficulty: beginner/intermediate/advanced\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Structured\": base_instruction + \"\"\"\n",
    "        \n",
    "        Format:\n",
    "        For each language provide:\n",
    "        1. Language name (bold)\n",
    "        2. One-sentence description\n",
    "        3. Best use case\n",
    "        4. Popularity score (1-10)\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    for format_name, prompt in formats.items():\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Format: {format_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "test_output_formats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4973c",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Design Output Formats\n",
    "\n",
    "Create prompts with specific output formats for these tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42499642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create prompts with specific output formats\n",
    "\n",
    "tasks = {\n",
    "    \"task1\": {\n",
    "        \"description\": \"Get a weather summary for a week\",\n",
    "        \"prompt\": \"\",  # TODO: Add prompt with JSON format\n",
    "        \"expected_format\": \"JSON\"\n",
    "    },\n",
    "    \"task2\": {\n",
    "        \"description\": \"Compare three products\",\n",
    "        \"prompt\": \"\",  # TODO: Add prompt with table format\n",
    "        \"expected_format\": \"Markdown Table\"\n",
    "    },\n",
    "    \"task3\": {\n",
    "        \"description\": \"Create a study plan\",\n",
    "        \"prompt\": \"\",  # TODO: Add prompt with structured format\n",
    "        \"expected_format\": \"Structured with sections\"\n",
    "    },\n",
    "    \"task4\": {\n",
    "        \"description\": \"Extract key information from text\",\n",
    "        \"prompt\": \"\",  # TODO: Add prompt with bullet points\n",
    "        \"expected_format\": \"Bullet points\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test your prompts\n",
    "# for task_name, task_info in tasks.items():\n",
    "#     if task_info[\"prompt\"]:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": task_info[\"prompt\"]}],\n",
    "#             temperature=0.3\n",
    "#         )\n",
    "#         print(f\"\\n{task_name} - {task_info['description']}\")\n",
    "#         print(f\"Expected format: {task_info['expected_format']}\")\n",
    "#         print(f\"\\nResponse:\\n{response.choices[0].message.content}\\n\")\n",
    "#         print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78715c",
   "metadata": {},
   "source": [
    "### Component 4: Constraints\n",
    "\n",
    "Constraints guide the model to stay within desired boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_constraints():\n",
    "    \"\"\"Test the impact of constraints.\"\"\"\n",
    "    \n",
    "    base_task = \"Explain quantum computing\"\n",
    "    \n",
    "    constraints_examples = [\n",
    "        {\n",
    "            \"name\": \"No constraints\",\n",
    "            \"prompt\": base_task\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Length constraint\",\n",
    "            \"prompt\": f\"{base_task} in exactly 3 sentences.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Audience constraint\",\n",
    "            \"prompt\": f\"{base_task} for a 10-year-old child.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Style constraint\",\n",
    "            \"prompt\": f\"{base_task} using only simple words (no jargon).\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multiple constraints\",\n",
    "            \"prompt\": f\"{base_task} in exactly 50 words, using an analogy, without technical terms.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for example in constraints_examples:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": example[\"prompt\"]}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Constraint: {example['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Prompt: {example['prompt']}\")\n",
    "        print(f\"\\nResponse:\\n{response.choices[0].message.content}\")\n",
    "        print(f\"\\nWord count: {len(response.choices[0].message.content.split())}\")\n",
    "\n",
    "test_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee83d32",
   "metadata": {},
   "source": [
    "### Exercise 1.4: Apply Constraints\n",
    "\n",
    "Add appropriate constraints to these prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add constraints to these prompts\n",
    "\n",
    "prompts_to_constrain = [\n",
    "    {\n",
    "        \"task\": \"Explain machine learning\",\n",
    "        \"original\": \"Explain machine learning.\",\n",
    "        \"constraints_to_add\": [\"length: 4 sentences\", \"no equations\", \"use everyday analogy\"],\n",
    "        \"improved\": \"\"  # TODO: Add your improved version\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Write a product description\",\n",
    "        \"original\": \"Write a product description for wireless headphones.\",\n",
    "        \"constraints_to_add\": [\"max 100 words\", \"highlight 3 key features\", \"call-to-action\"],\n",
    "        \"improved\": \"\"  # TODO: Add your improved version\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Summarize an article\",\n",
    "        \"original\": \"Summarize this article about climate change.\",\n",
    "        \"constraints_to_add\": [\"5 bullet points\", \"focus on solutions\", \"cite statistics\"],\n",
    "        \"improved\": \"\"  # TODO: Add your improved version\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test your constrained prompts\n",
    "# for item in prompts_to_constrain:\n",
    "#     if item[\"improved\"]:\n",
    "#         print(f\"\\nTask: {item['task']}\")\n",
    "#         print(f\"Original: {item['original']}\")\n",
    "#         print(f\"Constraints: {', '.join(item['constraints_to_add'])}\")\n",
    "#         print(f\"Improved: {item['improved']}\\n\")\n",
    "#         \n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": item[\"improved\"]}],\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "#         print(f\"Response:\\n{response.choices[0].message.content}\\n\")\n",
    "#         print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062df4b",
   "metadata": {},
   "source": [
    "## Part 2: Prompt Patterns\n",
    "\n",
    "Let's explore common, reusable prompt patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa237742",
   "metadata": {},
   "source": [
    "### Pattern 1: Persona Pattern\n",
    "\n",
    "Assign a specific role or persona to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_persona_pattern():\n",
    "    \"\"\"Test different persona patterns.\"\"\"\n",
    "    \n",
    "    question = \"How should I prepare for a job interview?\"\n",
    "    \n",
    "    personas = [\n",
    "        {\n",
    "            \"name\": \"Career Coach\",\n",
    "            \"system_message\": \"You are an experienced career coach with 15 years of experience helping people land their dream jobs.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"HR Manager\",\n",
    "            \"system_message\": \"You are an HR manager at a Fortune 500 company who has conducted over 1000 interviews.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Recruitment Consultant\",\n",
    "            \"system_message\": \"You are a recruitment consultant specializing in tech roles, known for giving practical, no-nonsense advice.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for persona in personas:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": persona[\"system_message\"]},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Persona: {persona['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "test_persona_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec1d75",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Create Persona Prompts\n",
    "\n",
    "Design persona-based prompts for these scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design persona prompts\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Code review feedback\",\n",
    "        \"user_message\": \"Review this Python function for potential improvements.\",\n",
    "        \"personas\": [\n",
    "            {\"name\": \"Senior Developer\", \"system_message\": \"\"},  # TODO\n",
    "            {\"name\": \"Security Expert\", \"system_message\": \"\"},  # TODO\n",
    "            {\"name\": \"Performance Engineer\", \"system_message\": \"\"}  # TODO\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Learning advice\",\n",
    "        \"user_message\": \"How should I learn data science?\",\n",
    "        \"personas\": [\n",
    "            {\"name\": \"University Professor\", \"system_message\": \"\"},  # TODO\n",
    "            {\"name\": \"Self-taught Data Scientist\", \"system_message\": \"\"},  # TODO\n",
    "            {\"name\": \"Bootcamp Instructor\", \"system_message\": \"\"}  # TODO\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test your personas\n",
    "# for scenario_info in scenarios:\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Scenario: {scenario_info['scenario']}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     \n",
    "#     for persona in scenario_info['personas']:\n",
    "#         if persona['system_message']:\n",
    "#             response = client.chat.completions.create(\n",
    "#                 model=\"gpt-3.5-turbo\",\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": persona['system_message']},\n",
    "#                     {\"role\": \"user\", \"content\": scenario_info['user_message']}\n",
    "#                 ],\n",
    "#                 temperature=0.7\n",
    "#             )\n",
    "#             print(f\"\\nPersona: {persona['name']}\")\n",
    "#             print(f\"Response: {response.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18726d22",
   "metadata": {},
   "source": [
    "### Pattern 2: Template Pattern\n",
    "\n",
    "Use templates with placeholders for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    \"\"\"\n",
    "    A reusable prompt template with placeholders.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, template: str, description: str = \"\"):\n",
    "        \"\"\"\n",
    "        Initialize template.\n",
    "        \n",
    "        Args:\n",
    "            template: Template string with {placeholders}\n",
    "            description: Description of the template's purpose\n",
    "        \"\"\"\n",
    "        self.template = template\n",
    "        self.description = description\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Fill in the template with values.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Values for placeholders\n",
    "        \n",
    "        Returns:\n",
    "            Formatted prompt\n",
    "        \"\"\"\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def get_placeholders(self) -> List[str]:\n",
    "        \"\"\"Extract placeholder names from template.\"\"\"\n",
    "        import re\n",
    "        return re.findall(r'\\{(\\w+)\\}', self.template)\n",
    "\n",
    "# Create some useful templates\n",
    "templates = {\n",
    "    \"summarizer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Summarize the following {content_type} in {style} style.\n",
    "        Length: {length}\n",
    "        Focus on: {focus_areas}\n",
    "        \n",
    "        Content:\n",
    "        {content}\n",
    "        \"\"\",\n",
    "        description=\"Generic summarization template\"\n",
    "    ),\n",
    "    \n",
    "    \"code_explainer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Explain this {language} code to a {audience_level} audience.\n",
    "        Focus on: {focus}\n",
    "        Include: {include_items}\n",
    "        \n",
    "        Code:\n",
    "        ```{language}\n",
    "        {code}\n",
    "        ```\n",
    "        \"\"\",\n",
    "        description=\"Code explanation template\"\n",
    "    ),\n",
    "    \n",
    "    \"text_transformer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Transform the following text:\n",
    "        - From style: {from_style}\n",
    "        - To style: {to_style}\n",
    "        - Maintain: {maintain}\n",
    "        - Change: {change}\n",
    "        \n",
    "        Text:\n",
    "        {text}\n",
    "        \"\"\",\n",
    "        description=\"Text style transformation template\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test the summarizer template\n",
    "summarizer = templates[\"summarizer\"]\n",
    "print(f\"Template: {summarizer.description}\")\n",
    "print(f\"Placeholders: {summarizer.get_placeholders()}\")\n",
    "\n",
    "prompt = summarizer.format(\n",
    "    content_type=\"article\",\n",
    "    style=\"executive\",\n",
    "    length=\"3 bullet points\",\n",
    "    focus_areas=\"key findings and recommendations\",\n",
    "    content=\"Machine learning models require large amounts of data for training...\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFormatted prompt:\\n{prompt}\")\n",
    "\n",
    "# Use the prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse:\\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24948b46",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Build Your Template Library\n",
    "\n",
    "Create reusable templates for common tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own prompt templates\n",
    "\n",
    "my_templates = {\n",
    "    \"email_writer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        # TODO: Create a template for writing emails\n",
    "        # Placeholders might include: {purpose}, {tone}, {recipient}, {key_points}\n",
    "        \"\"\",\n",
    "        description=\"Email writing template\"\n",
    "    ),\n",
    "    \n",
    "    \"bug_analyzer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        # TODO: Create a template for analyzing bugs\n",
    "        # Placeholders might include: {error_message}, {code_snippet}, {context}\n",
    "        \"\"\",\n",
    "        description=\"Bug analysis template\"\n",
    "    ),\n",
    "    \n",
    "    \"content_repurposer\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        # TODO: Create a template for repurposing content\n",
    "        # Placeholders might include: {original_format}, {target_format}, {content}\n",
    "        \"\"\",\n",
    "        description=\"Content repurposing template\"\n",
    "    ),\n",
    "    \n",
    "    \"learning_path\": PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        # TODO: Create a template for generating learning paths\n",
    "        # Placeholders might include: {topic}, {current_level}, {goal}, {timeframe}\n",
    "        \"\"\",\n",
    "        description=\"Learning path generator template\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test your templates\n",
    "# for name, template in my_templates.items():\n",
    "#     print(f\"\\nTemplate: {name}\")\n",
    "#     print(f\"Description: {template.description}\")\n",
    "#     print(f\"Placeholders: {template.get_placeholders()}\")\n",
    "#     \n",
    "#     # Fill in with your test values and get response\n",
    "#     # prompt = template.format(...)\n",
    "#     # response = client.chat.completions.create(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acf6e6",
   "metadata": {},
   "source": [
    "### Pattern 3: Instruction-Example Pattern\n",
    "\n",
    "Combine instructions with examples for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_instruction_example_pattern():\n",
    "    \"\"\"Test instruction with examples vs without.\"\"\"\n",
    "    \n",
    "    task = \"Extract the sentiment (positive/negative/neutral) and key topics from customer reviews.\"\n",
    "    \n",
    "    # Without examples\n",
    "    prompt_no_examples = f\"\"\"\n",
    "    Task: {task}\n",
    "    \n",
    "    Review: \"The product arrived late and the packaging was damaged, but the item itself works perfectly.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # With examples\n",
    "    prompt_with_examples = f\"\"\"\n",
    "    Task: {task}\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    Review: \"Love this product! Fast shipping and great quality.\"\n",
    "    Sentiment: Positive\n",
    "    Topics: product quality, shipping speed\n",
    "    \n",
    "    Review: \"Terrible experience. Product broke after one day.\"\n",
    "    Sentiment: Negative\n",
    "    Topics: product durability, reliability\n",
    "    \n",
    "    Review: \"It's okay, does what it says but nothing special.\"\n",
    "    Sentiment: Neutral\n",
    "    Topics: product functionality\n",
    "    \n",
    "    Now analyze this review:\n",
    "    Review: \"The product arrived late and the packaging was damaged, but the item itself works perfectly.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"Without Examples\", prompt_no_examples),\n",
    "        (\"With Examples\", prompt_with_examples)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "test_instruction_example_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6008a3",
   "metadata": {},
   "source": [
    "## Part 3: Iterative Prompt Improvement\n",
    "\n",
    "Learn to systematically improve prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptVersion:\n",
    "    \"\"\"Track different versions of a prompt.\"\"\"\n",
    "    version: int\n",
    "    prompt: str\n",
    "    response: str\n",
    "    tokens: int\n",
    "    evaluation: str\n",
    "    improvements: List[str]\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"\n",
    "    Systematically optimize prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.versions: List[PromptVersion] = []\n",
    "    \n",
    "    def test_prompt(self, prompt: str, evaluation_criteria: str = \"\") -> PromptVersion:\n",
    "        \"\"\"\n",
    "        Test a prompt and record results.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to test\n",
    "            evaluation_criteria: How to evaluate the response\n",
    "        \n",
    "        Returns:\n",
    "            PromptVersion object with results\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        version = PromptVersion(\n",
    "            version=len(self.versions) + 1,\n",
    "            prompt=prompt,\n",
    "            response=response.choices[0].message.content,\n",
    "            tokens=response.usage.total_tokens,\n",
    "            evaluation=evaluation_criteria,\n",
    "            improvements=[]\n",
    "        )\n",
    "        \n",
    "        self.versions.append(version)\n",
    "        return version\n",
    "    \n",
    "    def compare_versions(self):\n",
    "        \"\"\"Compare all tested versions.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PROMPT VERSION COMPARISON\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for v in self.versions:\n",
    "            print(f\"Version {v.version}\")\n",
    "            print(f\"Tokens: {v.tokens}\")\n",
    "            print(f\"Prompt: {v.prompt[:100]}...\")\n",
    "            print(f\"Response: {v.response[:150]}...\")\n",
    "            print(f\"Evaluation: {v.evaluation}\")\n",
    "            if v.improvements:\n",
    "                print(f\"Improvements made: {', '.join(v.improvements)}\")\n",
    "            print(f\"\\n{'-'*80}\\n\")\n",
    "    \n",
    "    def get_best_version(self, criteria: str = \"shortest\") -> PromptVersion:\n",
    "        \"\"\"\n",
    "        Get the best version based on criteria.\n",
    "        \n",
    "        Args:\n",
    "            criteria: 'shortest' (fewest tokens) or 'latest'\n",
    "        \"\"\"\n",
    "        if criteria == \"shortest\":\n",
    "            return min(self.versions, key=lambda v: v.tokens)\n",
    "        return self.versions[-1]\n",
    "\n",
    "# Example: Optimize a prompt iteratively\n",
    "optimizer = PromptOptimizer()\n",
    "\n",
    "# Version 1: Basic prompt\n",
    "v1 = optimizer.test_prompt(\n",
    "    \"Explain recursion\",\n",
    "    evaluation_criteria=\"Too vague, lacks specificity\"\n",
    ")\n",
    "v1.improvements = []\n",
    "\n",
    "# Version 2: Add specificity\n",
    "v2 = optimizer.test_prompt(\n",
    "    \"Explain recursion in programming with an example\",\n",
    "    evaluation_criteria=\"Better, but no format specification\"\n",
    ")\n",
    "v2.improvements = [\"Added example request\"]\n",
    "\n",
    "# Version 3: Add format\n",
    "v3 = optimizer.test_prompt(\n",
    "    \"\"\"Explain recursion in programming.\n",
    "    \n",
    "    Format:\n",
    "    1. Definition (1 sentence)\n",
    "    2. Simple code example\n",
    "    3. Explanation of how it works\n",
    "    4. When to use it (1 sentence)\n",
    "    \"\"\",\n",
    "    evaluation_criteria=\"Good structure, clear output format\"\n",
    ")\n",
    "v3.improvements = [\"Added format specification\", \"Structured output\"]\n",
    "\n",
    "# Version 4: Add constraints\n",
    "v4 = optimizer.test_prompt(\n",
    "    \"\"\"Explain recursion in programming for a beginner.\n",
    "    \n",
    "    Format:\n",
    "    1. Definition (1 sentence, no jargon)\n",
    "    2. Simple Python code example (factorial function)\n",
    "    3. Step-by-step explanation of execution\n",
    "    4. When to use it vs loops (1 sentence)\n",
    "    \n",
    "    Keep the total explanation under 150 words.\n",
    "    \"\"\",\n",
    "    evaluation_criteria=\"Excellent: audience-specific, constrained, structured\"\n",
    ")\n",
    "v4.improvements = [\"Added audience level\", \"Specific example\", \"Length constraint\"]\n",
    "\n",
    "# Compare all versions\n",
    "optimizer.compare_versions()\n",
    "\n",
    "# Get best version\n",
    "best = optimizer.get_best_version(\"latest\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST VERSION (Version {best.version}):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Prompt:\\n{best.prompt}\\n\")\n",
    "print(f\"Response:\\n{best.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ab4ae",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Optimize a Prompt\n",
    "\n",
    "Take this basic prompt through 4 iterations of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize this prompt through iterations\n",
    "\n",
    "# Starting prompt\n",
    "basic_prompt = \"Write a function to sort a list\"\n",
    "\n",
    "# Create your optimizer\n",
    "my_optimizer = PromptOptimizer()\n",
    "\n",
    "# Version 1: Original (test it)\n",
    "# v1 = my_optimizer.test_prompt(basic_prompt, \"Baseline - too vague\")\n",
    "\n",
    "# Version 2: TODO - Add specificity\n",
    "# What language? What sorting algorithm? What should the function signature be?\n",
    "# v2_prompt = \"\"\n",
    "# v2 = my_optimizer.test_prompt(v2_prompt, \"\")\n",
    "# v2.improvements = [\"\"]\n",
    "\n",
    "# Version 3: TODO - Add constraints and format\n",
    "# Add error handling? Input validation? Documentation?\n",
    "# v3_prompt = \"\"\n",
    "# v3 = my_optimizer.test_prompt(v3_prompt, \"\")\n",
    "# v3.improvements = [\"\"]\n",
    "\n",
    "# Version 4: TODO - Add examples and edge cases\n",
    "# Provide example inputs/outputs? Handle edge cases?\n",
    "# v4_prompt = \"\"\n",
    "# v4 = my_optimizer.test_prompt(v4_prompt, \"\")\n",
    "# v4.improvements = [\"\"]\n",
    "\n",
    "# Compare and analyze\n",
    "# my_optimizer.compare_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e68a7",
   "metadata": {},
   "source": [
    "## Part 4: Common Prompt Challenges\n",
    "\n",
    "Learn to handle typical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa2ff8",
   "metadata": {},
   "source": [
    "### Challenge 1: Handling Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a392d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_ambiguity():\n",
    "    \"\"\"Demonstrate handling ambiguous prompts.\"\"\"\n",
    "    \n",
    "    # Ambiguous prompt\n",
    "    ambiguous = \"How do I fix the error?\"\n",
    "    \n",
    "    # Clarified prompt\n",
    "    clarified = \"\"\"\n",
    "    I'm getting this Python error:\n",
    "    \n",
    "    ```\n",
    "    TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
    "    ```\n",
    "    \n",
    "    In this code:\n",
    "    ```python\n",
    "    age = 25\n",
    "    message = \"Age: \" + age\n",
    "    ```\n",
    "    \n",
    "    How do I fix this error?\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"Ambiguous\", ambiguous),\n",
    "        (\"Clarified\", clarified)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "handle_ambiguity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c087dc",
   "metadata": {},
   "source": [
    "### Challenge 2: Preventing Hallucinations\n",
    "\n",
    "Guide the model to admit uncertainty rather than fabricate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevent_hallucinations():\n",
    "    \"\"\"Techniques to reduce hallucinations.\"\"\"\n",
    "    \n",
    "    question = \"What are the key features of the Python 4.0 release?\"\n",
    "    # Note: Python 4.0 doesn't exist - testing if model will hallucinate\n",
    "    \n",
    "    # Without guidance\n",
    "    no_guidance = question\n",
    "    \n",
    "    # With uncertainty instruction\n",
    "    with_guidance = f\"\"\"\n",
    "    {question}\n",
    "    \n",
    "    Important: If you're not certain about the information or if Python 4.0 hasn't \n",
    "    been released yet, please say so clearly rather than speculating.\n",
    "    \"\"\"\n",
    "    \n",
    "    # With date constraint\n",
    "    with_date = f\"\"\"\n",
    "    Based on information available up to your knowledge cutoff date:\n",
    "    {question}\n",
    "    \n",
    "    If Python 4.0 hasn't been released yet, please state that clearly.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"No Guidance\", no_guidance),\n",
    "        (\"With Guidance\", with_guidance),\n",
    "        (\"With Date Constraint\", with_date)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "prevent_hallucinations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a5ca1",
   "metadata": {},
   "source": [
    "### Challenge 3: Maintaining Consistency\n",
    "\n",
    "Ensure consistent formatting and style across responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_consistency():\n",
    "    \"\"\"Use system messages and templates for consistency.\"\"\"\n",
    "    \n",
    "    # Inconsistent responses\n",
    "    questions = [\n",
    "        \"What is Python?\",\n",
    "        \"What is JavaScript?\",\n",
    "        \"What is SQL?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"WITHOUT CONSISTENCY MEASURES:\")\n",
    "    print(\"=\"*80)\n",
    "    for q in questions:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": q}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"\\n{q}\")\n",
    "        print(response.choices[0].message.content[:150] + \"...\")\n",
    "    \n",
    "    # With consistency measures\n",
    "    print(\"\\n\\nWITH CONSISTENCY MEASURES:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are a programming language expert. When asked about a programming language, \n",
    "    always respond in this exact format:\n",
    "    \n",
    "    **Language:** [Name]\n",
    "    **Type:** [Type of language]\n",
    "    **Primary Use:** [Main use case]\n",
    "    **Key Feature:** [One distinguishing feature]\n",
    "    **Difficulty:** [Beginner/Intermediate/Advanced]\n",
    "    \"\"\"\n",
    "    \n",
    "    for q in questions:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": q}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"\\n{q}\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "ensure_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686a86a",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Fix Problematic Prompts\n",
    "\n",
    "Identify and fix issues in these prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix these problematic prompts\n",
    "\n",
    "problematic_prompts = [\n",
    "    {\n",
    "        \"original\": \"Tell me everything about databases\",\n",
    "        \"problem\": \"Too broad, overwhelming\",\n",
    "        \"fixed\": \"\"  # TODO: Make it more focused\n",
    "    },\n",
    "    {\n",
    "        \"original\": \"Is machine learning better than deep learning?\",\n",
    "        \"problem\": \"False comparison, they're related not alternatives\",\n",
    "        \"fixed\": \"\"  # TODO: Reframe the question properly\n",
    "    },\n",
    "    {\n",
    "        \"original\": \"How much does a car cost?\",\n",
    "        \"problem\": \"Needs context and constraints\",\n",
    "        \"fixed\": \"\"  # TODO: Add relevant context\n",
    "    },\n",
    "    {\n",
    "        \"original\": \"Make my code faster\",\n",
    "        \"problem\": \"No code provided, no context\",\n",
    "        \"fixed\": \"\"  # TODO: Add code and context\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test your fixed prompts\n",
    "# for item in problematic_prompts:\n",
    "#     if item[\"fixed\"]:\n",
    "#         print(f\"\\nOriginal Problem: {item['problem']}\")\n",
    "#         print(f\"Original: {item['original']}\")\n",
    "#         print(f\"Fixed: {item['fixed']}\\n\")\n",
    "#         \n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": item[\"fixed\"]}],\n",
    "#             temperature=0.5\n",
    "#         )\n",
    "#         print(f\"Response:\\n{response.choices[0].message.content}\\n\")\n",
    "#         print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb04beb",
   "metadata": {},
   "source": [
    "## Part 5: Building a Prompt Library\n",
    "\n",
    "Create a reusable collection of tested prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLibrary:\n",
    "    \"\"\"\n",
    "    Manage a library of tested, reusable prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompts: Dict[str, Dict] = {}\n",
    "    \n",
    "    def add_prompt(\n",
    "        self,\n",
    "        name: str,\n",
    "        template: str,\n",
    "        category: str,\n",
    "        description: str,\n",
    "        example_inputs: Dict = None,\n",
    "        tags: List[str] = None\n",
    "    ):\n",
    "        \"\"\"Add a prompt to the library.\"\"\"\n",
    "        self.prompts[name] = {\n",
    "            \"template\": template,\n",
    "            \"category\": category,\n",
    "            \"description\": description,\n",
    "            \"example_inputs\": example_inputs or {},\n",
    "            \"tags\": tags or [],\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"usage_count\": 0\n",
    "        }\n",
    "    \n",
    "    def get_prompt(self, name: str) -> Optional[Dict]:\n",
    "        \"\"\"Retrieve a prompt from the library.\"\"\"\n",
    "        prompt = self.prompts.get(name)\n",
    "        if prompt:\n",
    "            prompt[\"usage_count\"] += 1\n",
    "        return prompt\n",
    "    \n",
    "    def search_prompts(self, query: str = \"\", category: str = \"\", tag: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Search prompts by query, category, or tag.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for name, prompt in self.prompts.items():\n",
    "            match = True\n",
    "            \n",
    "            if query and query.lower() not in name.lower() and query.lower() not in prompt[\"description\"].lower():\n",
    "                match = False\n",
    "            \n",
    "            if category and prompt[\"category\"] != category:\n",
    "                match = False\n",
    "            \n",
    "            if tag and tag not in prompt[\"tags\"]:\n",
    "                match = False\n",
    "            \n",
    "            if match:\n",
    "                results.append({\"name\": name, **prompt})\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def list_categories(self) -> List[str]:\n",
    "        \"\"\"List all categories.\"\"\"\n",
    "        return list(set(p[\"category\"] for p in self.prompts.values()))\n",
    "    \n",
    "    def export_to_json(self, filepath: str):\n",
    "        \"\"\"Export library to JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.prompts, f, indent=2)\n",
    "        print(f\"âœ“ Library exported to {filepath}\")\n",
    "    \n",
    "    def import_from_json(self, filepath: str):\n",
    "        \"\"\"Import library from JSON file.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.prompts = json.load(f)\n",
    "        print(f\"âœ“ Library imported from {filepath}\")\n",
    "\n",
    "# Create and populate a library\n",
    "library = PromptLibrary()\n",
    "\n",
    "# Add some prompts\n",
    "library.add_prompt(\n",
    "    name=\"code_reviewer\",\n",
    "    template=\"\"\"\n",
    "    Review the following {language} code for:\n",
    "    - Code quality and best practices\n",
    "    - Potential bugs or issues\n",
    "    - Performance considerations\n",
    "    - Security concerns\n",
    "    \n",
    "    Code:\n",
    "    ```{language}\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Provide specific, actionable feedback.\n",
    "    \"\"\",\n",
    "    category=\"code\",\n",
    "    description=\"Comprehensive code review prompt\",\n",
    "    example_inputs={\"language\": \"python\", \"code\": \"def example(): pass\"},\n",
    "    tags=[\"code\", \"review\", \"quality\"]\n",
    ")\n",
    "\n",
    "library.add_prompt(\n",
    "    name=\"learning_path_generator\",\n",
    "    template=\"\"\"\n",
    "    Create a personalized learning path for: {topic}\n",
    "    \n",
    "    Current level: {current_level}\n",
    "    Goal: {goal}\n",
    "    Time available: {timeframe}\n",
    "    Learning style: {learning_style}\n",
    "    \n",
    "    Provide:\n",
    "    1. Learning phases (beginner â†’ advanced)\n",
    "    2. Key concepts to master in order\n",
    "    3. Recommended resources for each phase\n",
    "    4. Practical projects to build\n",
    "    5. Estimated timeline\n",
    "    \"\"\",\n",
    "    category=\"education\",\n",
    "    description=\"Generate structured learning paths\",\n",
    "    example_inputs={\n",
    "        \"topic\": \"Machine Learning\",\n",
    "        \"current_level\": \"beginner\",\n",
    "        \"goal\": \"build ML models\",\n",
    "        \"timeframe\": \"3 months\",\n",
    "        \"learning_style\": \"hands-on\"\n",
    "    },\n",
    "    tags=[\"education\", \"learning\", \"career\"]\n",
    ")\n",
    "\n",
    "library.add_prompt(\n",
    "    name=\"bug_analyzer\",\n",
    "    template=\"\"\"\n",
    "    Analyze this bug:\n",
    "    \n",
    "    Error: {error_message}\n",
    "    \n",
    "    Code context:\n",
    "    ```{language}\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Environment: {environment}\n",
    "    \n",
    "    Provide:\n",
    "    1. Root cause analysis\n",
    "    2. Step-by-step solution\n",
    "    3. Prevention tips\n",
    "    4. Related issues to watch for\n",
    "    \"\"\",\n",
    "    category=\"debugging\",\n",
    "    description=\"Systematic bug analysis\",\n",
    "    example_inputs={\n",
    "        \"error_message\": \"TypeError: ...\",\n",
    "        \"code\": \"...\",\n",
    "        \"language\": \"python\",\n",
    "        \"environment\": \"Python 3.9, Ubuntu 20.04\"\n",
    "    },\n",
    "    tags=[\"debugging\", \"troubleshooting\", \"code\"]\n",
    ")\n",
    "\n",
    "# Search the library\n",
    "print(\"\\nAll prompts in 'code' category:\")\n",
    "code_prompts = library.search_prompts(category=\"code\")\n",
    "for p in code_prompts:\n",
    "    print(f\"- {p['name']}: {p['description']}\")\n",
    "\n",
    "print(\"\\nAll prompts with 'learning' tag:\")\n",
    "learning_prompts = library.search_prompts(tag=\"learning\")\n",
    "for p in learning_prompts:\n",
    "    print(f\"- {p['name']}: {p['description']}\")\n",
    "\n",
    "# Use a prompt from the library\n",
    "prompt_info = library.get_prompt(\"code_reviewer\")\n",
    "if prompt_info:\n",
    "    template = prompt_info[\"template\"]\n",
    "    filled_prompt = template.format(\n",
    "        language=\"python\",\n",
    "        code=\"\"\"\n",
    "def calculate_total(items):\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total = total + item\n",
    "    return total\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Using prompt from library: code_reviewer\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "# Export library\n",
    "library.export_to_json(\"my_prompt_library.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d4810",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Build Your Own Prompt Library\n",
    "\n",
    "Create a library with at least 5 prompts for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your personal prompt library\n",
    "\n",
    "my_library = PromptLibrary()\n",
    "\n",
    "# TODO: Add prompts for:\n",
    "# 1. Email writing (professional, casual, follow-up)\n",
    "# 2. Content creation (blog posts, social media, documentation)\n",
    "# 3. Data analysis (summarize findings, generate insights)\n",
    "# 4. Problem solving (brainstorming, decision making)\n",
    "# 5. Communication (meeting notes, status updates, explanations)\n",
    "\n",
    "# Example:\n",
    "# my_library.add_prompt(\n",
    "#     name=\"professional_email\",\n",
    "#     template=\"...\",\n",
    "#     category=\"communication\",\n",
    "#     description=\"...\",\n",
    "#     tags=[\"email\", \"professional\"]\n",
    "# )\n",
    "\n",
    "# Test your library\n",
    "# my_library.list_categories()\n",
    "# my_library.search_prompts(category=\"communication\")\n",
    "# my_library.export_to_json(\"personal_prompt_library.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc92625",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Prompt A/B Testing Framework\n",
    "\n",
    "Build a system to test multiple prompt variations and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptABTester:\n",
    "    \"\"\"\n",
    "    A/B test different prompt variations.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Create prompt variations\n",
    "    2. Run tests with multiple inputs\n",
    "    3. Collect metrics (tokens, quality, consistency)\n",
    "    4. Statistical comparison\n",
    "    5. Generate report with winner\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tests = []\n",
    "    \n",
    "    # TODO: Implement methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# tester = PromptABTester()\n",
    "# tester.add_variation(\"v1\", \"Explain {topic}\")\n",
    "# tester.add_variation(\"v2\", \"Explain {topic} in simple terms\")\n",
    "# tester.run_test(inputs=[{\"topic\": \"AI\"}, {\"topic\": \"ML\"}])\n",
    "# tester.get_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a22cde",
   "metadata": {},
   "source": [
    "### Challenge 2: Prompt Quality Analyzer\n",
    "\n",
    "Create a tool that analyzes prompts and suggests improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptQualityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze prompt quality and suggest improvements.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Check for common issues (vagueness, ambiguity, missing context)\n",
    "    2. Analyze structure and completeness\n",
    "    3. Suggest specific improvements\n",
    "    4. Provide quality score\n",
    "    5. Generate improved version\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checks = []\n",
    "    \n",
    "    # TODO: Implement analysis methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# analyzer = PromptQualityAnalyzer()\n",
    "# report = analyzer.analyze(\"Tell me about Python\")\n",
    "# print(report[\"issues\"])\n",
    "# print(report[\"suggestions\"])\n",
    "# print(report[\"improved_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d66aa",
   "metadata": {},
   "source": [
    "### Challenge 3: Domain-Specific Prompt Generator\n",
    "\n",
    "Build a generator that creates optimized prompts for specific domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainPromptGenerator:\n",
    "    \"\"\"\n",
    "    Generate domain-specific prompts.\n",
    "    \n",
    "    TODO: Implement for domains like:\n",
    "    1. Healthcare (medical Q&A, diagnosis support)\n",
    "    2. Legal (contract analysis, legal research)\n",
    "    3. Education (lesson plans, quiz generation)\n",
    "    4. Business (market analysis, strategy)\n",
    "    5. Technical (code generation, system design)\n",
    "    \n",
    "    Each domain should have:\n",
    "    - Specialized templates\n",
    "    - Domain-appropriate constraints\n",
    "    - Expert personas\n",
    "    - Compliance considerations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str):\n",
    "        self.domain = domain\n",
    "        self.templates = {}\n",
    "    \n",
    "    # TODO: Implement domain-specific generation\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# generator = DomainPromptGenerator(domain=\"healthcare\")\n",
    "# prompt = generator.generate(\"patient_symptoms_analysis\",\n",
    "#                            symptoms=[\"fever\", \"cough\"],\n",
    "#                            duration=\"3 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fe838",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. âœ… The anatomy of well-structured prompts\n",
    "2. âœ… Essential prompt components (instructions, context, format, constraints)\n",
    "3. âœ… Common prompt patterns (persona, template, instruction-example)\n",
    "4. âœ… Iterative prompt improvement methodology\n",
    "5. âœ… Handling common challenges (ambiguity, hallucinations, consistency)\n",
    "6. âœ… Building and managing a prompt library\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Specificity matters**: Vague prompts yield vague results\n",
    "- **Context is crucial**: Background information improves accuracy\n",
    "- **Format your outputs**: Structured responses are more useful\n",
    "- **Use constraints**: Guide the model with clear boundaries\n",
    "- **Iterate systematically**: Test, measure, improve\n",
    "- **Build reusable assets**: Create a library of tested prompts\n",
    "- **Consistency requires structure**: Use templates and system messages\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start simple, iterate**: Begin with basic prompt, refine based on results\n",
    "2. **Be specific**: Clear instructions > vague requests\n",
    "3. **Provide examples**: Show don't just tell\n",
    "4. **Structure outputs**: Specify exactly how you want responses formatted\n",
    "5. **Add constraints**: Length, style, content boundaries\n",
    "6. **Use templates**: Consistency and reusability\n",
    "7. **Test variations**: A/B test different approaches\n",
    "8. **Document what works**: Build your prompt library\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Build your personal prompt library with 20+ prompts\n",
    "- Move on to Lab 2: Few-Shot Learning Experiments\n",
    "- Practice prompt engineering daily\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
