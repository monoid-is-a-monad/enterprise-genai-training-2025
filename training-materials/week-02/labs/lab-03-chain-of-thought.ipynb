{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9f0582",
   "metadata": {},
   "source": [
    "# Lab 3: Chain-of-Thought Implementation\n",
    "\n",
    "**Week 2 - Prompt Engineering & LLM Basics**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand chain-of-thought (CoT) prompting technique\n",
    "- Implement zero-shot and few-shot CoT\n",
    "- Master step-by-step reasoning patterns\n",
    "- Use CoT for complex problem solving\n",
    "- Implement self-consistency techniques\n",
    "- Build reasoning chains for different domains\n",
    "- Evaluate and improve reasoning quality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 2 Labs 1 & 2\n",
    "- Understanding of few-shot learning\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00460b0",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken pandas numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ea66c",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Chain-of-Thought\n",
    "\n",
    "Chain-of-Thought prompting encourages the model to show its reasoning process step-by-step, leading to more accurate and explainable results.\n",
    "\n",
    "### Why CoT Works\n",
    "\n",
    "1. **Breaks down complexity**: Divides hard problems into manageable steps\n",
    "2. **Improves accuracy**: Reduces errors through explicit reasoning\n",
    "3. **Provides transparency**: Shows how the answer was reached\n",
    "4. **Enables debugging**: Identify where reasoning went wrong\n",
    "5. **Handles multi-step problems**: Natural fit for complex reasoning\n",
    "\n",
    "Let's compare standard prompting vs CoT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_standard_vs_cot():\n",
    "    \"\"\"Compare standard prompting with chain-of-thought.\"\"\"\n",
    "    \n",
    "    problem = \"\"\"\n",
    "    A bakery sells cupcakes in boxes of 6 and 8. \n",
    "    Sarah wants to buy exactly 28 cupcakes for a party.\n",
    "    What combination of boxes should she buy?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard prompting\n",
    "    standard_prompt = f\"Problem: {problem}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Chain-of-thought prompting\n",
    "    cot_prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Let's solve this step by step:\n",
    "1. First, identify what we know and what we need to find\n",
    "2. Consider possible combinations\n",
    "3. Check which combination gives exactly 28\n",
    "4. Verify the solution\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompts = [\n",
    "        (\"Standard Prompting\", standard_prompt),\n",
    "        (\"Chain-of-Thought\", cot_prompt)\n",
    "    ]\n",
    "    \n",
    "    for label, prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{label}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(f\"\\nTokens: {response.usage.total_tokens}\")\n",
    "\n",
    "compare_standard_vs_cot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed8e1a",
   "metadata": {},
   "source": [
    "## Part 2: Zero-Shot Chain-of-Thought\n",
    "\n",
    "Zero-shot CoT uses trigger phrases like \"Let's think step by step\" to activate reasoning without examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotCoT:\n",
    "    \"\"\"\n",
    "    Zero-shot chain-of-thought reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize ZeroShotCoT.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.trigger_phrases = [\n",
    "            \"Let's think step by step.\",\n",
    "            \"Let's break this down:\",\n",
    "            \"Let's solve this systematically:\",\n",
    "            \"Let's approach this methodically:\",\n",
    "            \"Let's reason through this:\"\n",
    "        ]\n",
    "    \n",
    "    def solve(\n",
    "        self,\n",
    "        problem: str,\n",
    "        trigger: str = \"Let's think step by step.\",\n",
    "        temperature: float = 0.3\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve problem using zero-shot CoT.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem statement\n",
    "            trigger: Trigger phrase for CoT\n",
    "            temperature: Sampling temperature\n",
    "        \n",
    "        Returns:\n",
    "            Dict with reasoning and answer\n",
    "        \"\"\"\n",
    "        prompt = f\"{problem}\\n\\n{trigger}\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        full_response = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract final answer\n",
    "        answer_patterns = [\n",
    "            r\"(?:final answer|answer|therefore|thus|so):\\s*(.+?)(?:\\n|$)\",\n",
    "            r\"(?:the answer is)\\s*(.+?)(?:\\n|$)\",\n",
    "        ]\n",
    "        \n",
    "        final_answer = None\n",
    "        for pattern in answer_patterns:\n",
    "            match = re.search(pattern, full_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                final_answer = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            \"problem\": problem,\n",
    "            \"trigger\": trigger,\n",
    "            \"reasoning\": full_response,\n",
    "            \"answer\": final_answer,\n",
    "            \"tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    def batch_solve(\n",
    "        self,\n",
    "        problems: List[str],\n",
    "        trigger: str = \"Let's think step by step.\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Solve multiple problems.\"\"\"\n",
    "        return [self.solve(p, trigger) for p in problems]\n",
    "\n",
    "# Test zero-shot CoT\n",
    "zero_shot = ZeroShotCoT()\n",
    "\n",
    "# Mathematical reasoning\n",
    "math_problem = \"\"\"\n",
    "If a train travels 120 km in 2 hours, then stops for 30 minutes, \n",
    "then travels another 90 km in 1.5 hours, what is its average speed \n",
    "for the entire journey?\n",
    "\"\"\"\n",
    "\n",
    "result = zero_shot.solve(math_problem)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ZERO-SHOT CHAIN-OF-THOUGHT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProblem:\\n{result['problem']}\")\n",
    "print(f\"\\nTrigger: {result['trigger']}\")\n",
    "print(f\"\\nReasoning:\\n{result['reasoning']}\")\n",
    "print(f\"\\nExtracted Answer: {result['answer']}\")\n",
    "print(f\"\\nTokens used: {result['tokens']}\")\n",
    "\n",
    "# Logical reasoning\n",
    "logic_problem = \"\"\"\n",
    "All roses are flowers.\n",
    "Some flowers fade quickly.\n",
    "Does this mean some roses fade quickly?\n",
    "\"\"\"\n",
    "\n",
    "result = zero_shot.solve(\n",
    "    logic_problem,\n",
    "    trigger=\"Let's reason through this step by step:\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOGICAL REASONING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProblem:\\n{result['problem']}\")\n",
    "print(f\"\\nReasoning:\\n{result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430ca34",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Test Different Trigger Phrases\n",
    "\n",
    "Experiment with different trigger phrases to see which works best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different trigger phrases\n",
    "\n",
    "test_problem = \"\"\"\n",
    "A store offers a 20% discount on all items, then adds 8% sales tax.\n",
    "If an item originally costs $50, what is the final price?\n",
    "\"\"\"\n",
    "\n",
    "triggers_to_test = [\n",
    "    \"Let's think step by step.\",\n",
    "    \"Let's break this down:\",\n",
    "    \"Let's solve this systematically:\",\n",
    "    # TODO: Add 3 more creative trigger phrases\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "# TODO: Test each trigger and compare results\n",
    "# zero_shot = ZeroShotCoT()\n",
    "# for trigger in triggers_to_test:\n",
    "#     if trigger:\n",
    "#         result = zero_shot.solve(test_problem, trigger)\n",
    "#         print(f\"\\nTrigger: {trigger}\")\n",
    "#         print(f\"Answer: {result['answer']}\")\n",
    "#         print(f\"Tokens: {result['tokens']}\")\n",
    "#         print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7a5d0",
   "metadata": {},
   "source": [
    "## Part 3: Few-Shot Chain-of-Thought\n",
    "\n",
    "Few-shot CoT provides examples of step-by-step reasoning to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoTExample:\n",
    "    \"\"\"Chain-of-thought example with reasoning steps.\"\"\"\n",
    "    problem: str\n",
    "    reasoning: str\n",
    "    answer: str\n",
    "\n",
    "class FewShotCoT:\n",
    "    \"\"\"\n",
    "    Few-shot chain-of-thought reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize FewShotCoT.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.examples: List[CoTExample] = []\n",
    "    \n",
    "    def add_example(self, problem: str, reasoning: str, answer: str):\n",
    "        \"\"\"\n",
    "        Add a chain-of-thought example.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem statement\n",
    "            reasoning: Step-by-step reasoning\n",
    "            answer: Final answer\n",
    "        \"\"\"\n",
    "        self.examples.append(CoTExample(problem, reasoning, answer))\n",
    "    \n",
    "    def build_prompt(self, problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Build prompt with examples.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem to solve\n",
    "        \n",
    "        Returns:\n",
    "            Complete prompt with examples\n",
    "        \"\"\"\n",
    "        prompt_parts = []\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(self.examples, 1):\n",
    "            prompt_parts.append(f\"Example {i}:\")\n",
    "            prompt_parts.append(f\"Problem: {example.problem}\")\n",
    "            prompt_parts.append(f\"Reasoning: {example.reasoning}\")\n",
    "            prompt_parts.append(f\"Answer: {example.answer}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        # Add the actual problem\n",
    "        prompt_parts.append(\"Now solve this problem:\")\n",
    "        prompt_parts.append(f\"Problem: {problem}\")\n",
    "        prompt_parts.append(\"Reasoning:\")\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    def solve(self, problem: str, temperature: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve problem using few-shot CoT.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem statement\n",
    "            temperature: Sampling temperature\n",
    "        \n",
    "        Returns:\n",
    "            Dict with reasoning and answer\n",
    "        \"\"\"\n",
    "        prompt = self.build_prompt(problem)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        full_response = response.choices[0].message.content\n",
    "        \n",
    "        # Extract answer\n",
    "        answer_match = re.search(\n",
    "            r\"(?:Answer|Therefore|Thus|So):\\s*(.+?)(?:\\n|$)\",\n",
    "            full_response,\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        answer = answer_match.group(1).strip() if answer_match else None\n",
    "        \n",
    "        return {\n",
    "            \"problem\": problem,\n",
    "            \"prompt\": prompt,\n",
    "            \"reasoning\": full_response,\n",
    "            \"answer\": answer,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"num_examples\": len(self.examples)\n",
    "        }\n",
    "\n",
    "# Create few-shot CoT for math word problems\n",
    "few_shot = FewShotCoT()\n",
    "\n",
    "# Add examples with reasoning\n",
    "few_shot.add_example(\n",
    "    problem=\"John has 5 apples. He buys 3 more. How many does he have?\",\n",
    "    reasoning=\"\"\"\n",
    "Step 1: Identify what we know\n",
    "- John starts with 5 apples\n",
    "- He buys 3 more apples\n",
    "\n",
    "Step 2: Determine the operation needed\n",
    "- We need to add the new apples to the original amount\n",
    "- Operation: addition\n",
    "\n",
    "Step 3: Calculate\n",
    "- 5 + 3 = 8\n",
    "\n",
    "Step 4: State the answer clearly\n",
    "- John has 8 apples\n",
    "    \"\"\",\n",
    "    answer=\"8 apples\"\n",
    ")\n",
    "\n",
    "few_shot.add_example(\n",
    "    problem=\"A box contains 24 chocolates. If Sarah eats 1/3 of them, how many are left?\",\n",
    "    reasoning=\"\"\"\n",
    "Step 1: Identify what we know\n",
    "- Total chocolates: 24\n",
    "- Sarah eats: 1/3 of the total\n",
    "\n",
    "Step 2: Calculate how many Sarah eats\n",
    "- 1/3 of 24 = 24 ÷ 3 = 8 chocolates\n",
    "\n",
    "Step 3: Calculate how many are left\n",
    "- Remaining = Total - Eaten\n",
    "- Remaining = 24 - 8 = 16\n",
    "\n",
    "Step 4: State the answer\n",
    "- 16 chocolates are left\n",
    "    \"\"\",\n",
    "    answer=\"16 chocolates\"\n",
    ")\n",
    "\n",
    "# Solve a new problem\n",
    "new_problem = \"\"\"\n",
    "A pizza is cut into 8 equal slices. Tom eats 2 slices, Jerry eats 3 slices.\n",
    "What fraction of the pizza is left?\n",
    "\"\"\"\n",
    "\n",
    "result = few_shot.solve(new_problem)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEW-SHOT CHAIN-OF-THOUGHT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProblem:\\n{result['problem']}\")\n",
    "print(f\"\\nUsing {result['num_examples']} examples\")\n",
    "print(f\"\\nReasoning:\\n{result['reasoning']}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(f\"\\nTokens: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6586be0",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Build Domain-Specific CoT\n",
    "\n",
    "Create few-shot CoT examples for a specific domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66931a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build few-shot CoT for logical reasoning\n",
    "\n",
    "logic_cot = FewShotCoT()\n",
    "\n",
    "# TODO: Add at least 3 examples of logical reasoning problems with step-by-step solutions\n",
    "# Example domains: syllogisms, conditional statements, set theory, probability\n",
    "\n",
    "# Example structure:\n",
    "# logic_cot.add_example(\n",
    "#     problem=\"If all A are B, and all B are C, are all A also C?\",\n",
    "#     reasoning=\"\"\"\n",
    "#     Step 1: Identify the logical statements\n",
    "#     ...\n",
    "#     \"\"\",\n",
    "#     answer=\"Yes, all A are C\"\n",
    "# )\n",
    "\n",
    "# TODO: Test with a new logical reasoning problem\n",
    "# test_problem = \"...\"\n",
    "# result = logic_cot.solve(test_problem)\n",
    "# print(result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a296324",
   "metadata": {},
   "source": [
    "## Part 4: Complex Multi-Step Reasoning\n",
    "\n",
    "Use CoT for problems requiring multiple reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8671ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexCoTSolver:\n",
    "    \"\"\"\n",
    "    Solver for complex multi-step problems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize ComplexCoTSolver.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def solve_with_structure(\n",
    "        self,\n",
    "        problem: str,\n",
    "        reasoning_structure: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve problem with explicit reasoning structure.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem statement\n",
    "            reasoning_structure: List of reasoning steps to follow\n",
    "        \n",
    "        Returns:\n",
    "            Solution with structured reasoning\n",
    "        \"\"\"\n",
    "        # Build structured prompt\n",
    "        prompt_parts = [\n",
    "            f\"Problem: {problem}\",\n",
    "            \"\",\n",
    "            \"Solve this by following these steps:\"\n",
    "        ]\n",
    "        \n",
    "        for i, step in enumerate(reasoning_structure, 1):\n",
    "            prompt_parts.append(f\"{i}. {step}\")\n",
    "        \n",
    "        prompt_parts.append(\"\")\n",
    "        prompt_parts.append(\"Provide detailed reasoning for each step:\")\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"problem\": problem,\n",
    "            \"structure\": reasoning_structure,\n",
    "            \"reasoning\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    def solve_planning_problem(self, problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"Solve planning and scheduling problems.\"\"\"\n",
    "        structure = [\n",
    "            \"List all constraints and requirements\",\n",
    "            \"Identify dependencies between tasks\",\n",
    "            \"Determine critical path\",\n",
    "            \"Allocate resources\",\n",
    "            \"Create timeline\",\n",
    "            \"Identify potential risks\"\n",
    "        ]\n",
    "        return self.solve_with_structure(problem, structure)\n",
    "    \n",
    "    def solve_decision_problem(self, problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"Solve decision-making problems.\"\"\"\n",
    "        structure = [\n",
    "            \"Define the decision to be made\",\n",
    "            \"List all available options\",\n",
    "            \"Identify evaluation criteria\",\n",
    "            \"Analyze pros and cons of each option\",\n",
    "            \"Consider constraints and trade-offs\",\n",
    "            \"Make recommendation with justification\"\n",
    "        ]\n",
    "        return self.solve_with_structure(problem, structure)\n",
    "    \n",
    "    def solve_debugging_problem(self, problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"Solve code debugging problems.\"\"\"\n",
    "        structure = [\n",
    "            \"Understand the expected behavior\",\n",
    "            \"Identify the actual behavior (error/bug)\",\n",
    "            \"Analyze the code logic\",\n",
    "            \"Identify potential causes\",\n",
    "            \"Test hypotheses\",\n",
    "            \"Propose solution and explain why it works\"\n",
    "        ]\n",
    "        return self.solve_with_structure(problem, structure)\n",
    "\n",
    "# Test with different problem types\n",
    "solver = ComplexCoTSolver()\n",
    "\n",
    "# Planning problem\n",
    "planning_problem = \"\"\"\n",
    "You need to organize a 2-day conference with:\n",
    "- 3 keynote speakers (1 hour each)\n",
    "- 6 breakout sessions (45 min each)\n",
    "- 4 networking breaks (30 min each)\n",
    "- Opening and closing ceremonies (30 min each)\n",
    "\n",
    "Working hours: 9 AM - 5 PM each day\n",
    "Lunch: 12 PM - 1 PM each day\n",
    "\n",
    "Create an optimal schedule.\n",
    "\"\"\"\n",
    "\n",
    "result = solver.solve_planning_problem(planning_problem)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLEX MULTI-STEP REASONING: Planning\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProblem:\\n{planning_problem}\")\n",
    "print(f\"\\nReasoning Structure:\")\n",
    "for i, step in enumerate(result['structure'], 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "print(f\"\\nDetailed Reasoning:\\n{result['reasoning']}\")\n",
    "\n",
    "# Decision problem\n",
    "decision_problem = \"\"\"\n",
    "A startup needs to choose a tech stack for their mobile app:\n",
    "\n",
    "Option A: Native development (Swift for iOS, Kotlin for Android)\n",
    "- Best performance\n",
    "- Full platform features\n",
    "- Requires 2 separate teams\n",
    "- Higher cost\n",
    "- Longer development time\n",
    "\n",
    "Option B: Cross-platform (React Native)\n",
    "- Single codebase\n",
    "- Faster development\n",
    "- Lower cost\n",
    "- Some performance trade-offs\n",
    "- Limited access to latest platform features\n",
    "\n",
    "Option C: Progressive Web App (PWA)\n",
    "- Works everywhere\n",
    "- Lowest cost\n",
    "- No app store presence\n",
    "- Limited offline capabilities\n",
    "- Less native feel\n",
    "\n",
    "Budget: $150k, Timeline: 6 months, Team: 3 developers\n",
    "\n",
    "Which option should they choose?\n",
    "\"\"\"\n",
    "\n",
    "result = solver.solve_decision_problem(decision_problem)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLEX MULTI-STEP REASONING: Decision Making\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDetailed Reasoning:\\n{result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914774f",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Solve Complex Problems\n",
    "\n",
    "Use structured CoT to solve these complex problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0185e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solve these complex problems with structured reasoning\n",
    "\n",
    "solver = ComplexCoTSolver()\n",
    "\n",
    "problems = [\n",
    "    {\n",
    "        \"type\": \"optimization\",\n",
    "        \"problem\": \"\"\"\n",
    "        A company has $100,000 to invest in marketing across 4 channels:\n",
    "        - Social Media: $5 per lead, 30% conversion rate\n",
    "        - Google Ads: $8 per lead, 40% conversion rate  \n",
    "        - Email: $2 per lead, 15% conversion rate\n",
    "        - Content Marketing: $10 per lead, 50% conversion rate\n",
    "        \n",
    "        Each channel has a maximum capacity:\n",
    "        - Social Media: 5,000 leads max\n",
    "        - Google Ads: 3,000 leads max\n",
    "        - Email: 10,000 leads max\n",
    "        - Content Marketing: 2,000 leads max\n",
    "        \n",
    "        How should they allocate the budget to maximize conversions?\n",
    "        \"\"\",\n",
    "        \"structure\": [\n",
    "            # TODO: Define reasoning steps for optimization\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"system_design\",\n",
    "        \"problem\": \"\"\"\n",
    "        Design a URL shortening service like bit.ly that needs to:\n",
    "        - Handle 1 million new URLs per day\n",
    "        - Serve 100 million redirects per day\n",
    "        - Store URLs for 5 years\n",
    "        - Provide custom aliases\n",
    "        - Track click analytics\n",
    "        - Be highly available (99.9% uptime)\n",
    "        \n",
    "        Propose the architecture and explain design decisions.\n",
    "        \"\"\",\n",
    "        \"structure\": [\n",
    "            # TODO: Define reasoning steps for system design\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# TODO: Solve each problem\n",
    "# for problem_info in problems:\n",
    "#     result = solver.solve_with_structure(\n",
    "#         problem_info['problem'],\n",
    "#         problem_info['structure']\n",
    "#     )\n",
    "#     print(f\"\\nProblem Type: {problem_info['type']}\")\n",
    "#     print(result['reasoning'])\n",
    "#     print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce345c6c",
   "metadata": {},
   "source": [
    "## Part 5: Self-Consistency\n",
    "\n",
    "Self-consistency generates multiple reasoning paths and selects the most consistent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfConsistencyCoT:\n",
    "    \"\"\"\n",
    "    Self-consistency chain-of-thought.\n",
    "    \n",
    "    Generates multiple reasoning paths and aggregates answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize SelfConsistencyCoT.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def solve_multiple_paths(\n",
    "        self,\n",
    "        problem: str,\n",
    "        num_paths: int = 5,\n",
    "        temperature: float = 0.7\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve problem using multiple reasoning paths.\n",
    "        \n",
    "        Args:\n",
    "            problem: Problem statement\n",
    "            num_paths: Number of reasoning paths to generate\n",
    "            temperature: Higher temperature for diverse reasoning\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all paths and consensus answer\n",
    "        \"\"\"\n",
    "        prompt = f\"{problem}\\n\\nLet's think step by step.\"\n",
    "        \n",
    "        paths = []\n",
    "        answers = []\n",
    "        \n",
    "        for i in range(num_paths):\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            \n",
    "            reasoning = response.choices[0].message.content\n",
    "            \n",
    "            # Extract answer\n",
    "            answer_patterns = [\n",
    "                r\"(?:answer|therefore|thus|so):\\s*([^\\n]+)\",\n",
    "                r\"(?:the answer is)\\s*([^\\n]+)\",\n",
    "                r\"(?:final answer):\\s*([^\\n]+)\"\n",
    "            ]\n",
    "            \n",
    "            answer = None\n",
    "            for pattern in answer_patterns:\n",
    "                match = re.search(pattern, reasoning, re.IGNORECASE)\n",
    "                if match:\n",
    "                    answer = match.group(1).strip()\n",
    "                    break\n",
    "            \n",
    "            if answer:\n",
    "                # Normalize answer for comparison\n",
    "                normalized = self._normalize_answer(answer)\n",
    "                answers.append(normalized)\n",
    "                paths.append({\n",
    "                    \"path_number\": i + 1,\n",
    "                    \"reasoning\": reasoning,\n",
    "                    \"answer\": answer,\n",
    "                    \"normalized_answer\": normalized\n",
    "                })\n",
    "        \n",
    "        # Find consensus answer\n",
    "        answer_counts = Counter(answers)\n",
    "        consensus_answer, consensus_count = answer_counts.most_common(1)[0]\n",
    "        confidence = consensus_count / num_paths\n",
    "        \n",
    "        return {\n",
    "            \"problem\": problem,\n",
    "            \"num_paths\": num_paths,\n",
    "            \"paths\": paths,\n",
    "            \"all_answers\": answers,\n",
    "            \"consensus_answer\": consensus_answer,\n",
    "            \"consensus_count\": consensus_count,\n",
    "            \"confidence\": confidence,\n",
    "            \"agreement\": f\"{consensus_count}/{num_paths}\"\n",
    "        }\n",
    "    \n",
    "    def _normalize_answer(self, answer: str) -> str:\n",
    "        \"\"\"Normalize answer for comparison.\"\"\"\n",
    "        # Remove punctuation, convert to lowercase, strip whitespace\n",
    "        normalized = answer.lower().strip()\n",
    "        normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
    "        normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "        return normalized\n",
    "    \n",
    "    def display_results(self, result: Dict[str, Any]):\n",
    "        \"\"\"Display results in readable format.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SELF-CONSISTENCY RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nProblem:\\n{result['problem']}\")\n",
    "        print(f\"\\nGenerated {result['num_paths']} reasoning paths:\")\n",
    "        \n",
    "        for path in result['paths']:\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Path {path['path_number']}:\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            print(f\"Reasoning:\\n{path['reasoning'][:200]}...\")\n",
    "            print(f\"\\nAnswer: {path['answer']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CONSENSUS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Consensus Answer: {result['consensus_answer']}\")\n",
    "        print(f\"Agreement: {result['agreement']} paths\")\n",
    "        print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "        \n",
    "        if result['confidence'] < 0.6:\n",
    "            print(\"\\n⚠️  Low confidence - answers vary significantly\")\n",
    "        elif result['confidence'] < 0.8:\n",
    "            print(\"\\n✓ Moderate confidence - some variation in answers\")\n",
    "        else:\n",
    "            print(\"\\n✓✓ High confidence - strong agreement across paths\")\n",
    "\n",
    "# Test self-consistency\n",
    "self_consistency = SelfConsistencyCoT()\n",
    "\n",
    "# Math problem\n",
    "math_problem = \"\"\"\n",
    "A farmer has 17 sheep. All but 9 die. How many sheep are left?\n",
    "\"\"\"\n",
    "\n",
    "result = self_consistency.solve_multiple_paths(math_problem, num_paths=5)\n",
    "self_consistency.display_results(result)\n",
    "\n",
    "# Logical reasoning\n",
    "logic_problem = \"\"\"\n",
    "If it takes 5 machines 5 minutes to make 5 widgets, \n",
    "how long would it take 100 machines to make 100 widgets?\n",
    "\"\"\"\n",
    "\n",
    "result = self_consistency.solve_multiple_paths(logic_problem, num_paths=5)\n",
    "self_consistency.display_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598fc10",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Test Self-Consistency\n",
    "\n",
    "Test self-consistency on these challenging problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c84df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test self-consistency on challenging problems\n",
    "\n",
    "self_consistency = SelfConsistencyCoT()\n",
    "\n",
    "challenging_problems = [\n",
    "    \"\"\"\n",
    "    A bat and a ball cost $1.10 in total.\n",
    "    The bat costs $1.00 more than the ball.\n",
    "    How much does the ball cost?\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    In a lake, there is a patch of lily pads.\n",
    "    Every day, the patch doubles in size.\n",
    "    If it takes 48 days for the patch to cover the entire lake,\n",
    "    how long would it take for the patch to cover half of the lake?\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    You have a 3-gallon jug and a 5-gallon jug.\n",
    "    How can you measure exactly 4 gallons of water?\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# TODO: Test each problem and analyze confidence levels\n",
    "# for i, problem in enumerate(challenging_problems, 1):\n",
    "#     print(f\"\\n\\nCHALLENGING PROBLEM {i}\")\n",
    "#     result = self_consistency.solve_multiple_paths(problem, num_paths=5)\n",
    "#     self_consistency.display_results(result)\n",
    "#     \n",
    "#     # TODO: If confidence is low, try with more paths\n",
    "#     if result['confidence'] < 0.6:\n",
    "#         print(\"\\nLow confidence detected. Trying with 10 paths...\")\n",
    "#         # result = self_consistency.solve_multiple_paths(problem, num_paths=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7caa0",
   "metadata": {},
   "source": [
    "## Part 6: Reasoning Chain Evaluation\n",
    "\n",
    "Evaluate the quality of reasoning chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate quality of chain-of-thought reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize ReasoningEvaluator.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def evaluate_reasoning(\n",
    "        self,\n",
    "        problem: str,\n",
    "        reasoning: str,\n",
    "        answer: str,\n",
    "        correct_answer: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate reasoning quality.\n",
    "        \n",
    "        Args:\n",
    "            problem: Original problem\n",
    "            reasoning: Chain-of-thought reasoning\n",
    "            answer: Proposed answer\n",
    "            correct_answer: Known correct answer (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Evaluation results\n",
    "        \"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "Evaluate this chain-of-thought reasoning:\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Reasoning: {reasoning}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Evaluate on these criteria (rate 1-5 for each):\n",
    "1. Completeness: Are all necessary steps included?\n",
    "2. Clarity: Is each step clearly explained?\n",
    "3. Logic: Is the reasoning logically sound?\n",
    "4. Correctness: Does it lead to the right answer?\n",
    "5. Efficiency: Is the solution path efficient?\n",
    "\n",
    "Provide:\n",
    "- Score for each criterion (1-5)\n",
    "- Overall quality (1-5)\n",
    "- Strengths\n",
    "- Weaknesses\n",
    "- Suggestions for improvement\n",
    "\n",
    "Format as JSON.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        evaluation_text = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract JSON\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', evaluation_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                evaluation = json.loads(json_match.group())\n",
    "            else:\n",
    "                evaluation = {\"raw_evaluation\": evaluation_text}\n",
    "        except json.JSONDecodeError:\n",
    "            evaluation = {\"raw_evaluation\": evaluation_text}\n",
    "        \n",
    "        result = {\n",
    "            \"problem\": problem,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"answer\": answer,\n",
    "            \"evaluation\": evaluation,\n",
    "            \"correct_answer\": correct_answer\n",
    "        }\n",
    "        \n",
    "        if correct_answer:\n",
    "            result[\"is_correct\"] = self._compare_answers(answer, correct_answer)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compare_answers(self, answer: str, correct: str) -> bool:\n",
    "        \"\"\"Compare if answers match (fuzzy).\"\"\"\n",
    "        answer_norm = re.sub(r'[^\\w\\s]', '', answer.lower()).strip()\n",
    "        correct_norm = re.sub(r'[^\\w\\s]', '', correct.lower()).strip()\n",
    "        return answer_norm in correct_norm or correct_norm in answer_norm\n",
    "    \n",
    "    def display_evaluation(self, result: Dict[str, Any]):\n",
    "        \"\"\"Display evaluation results.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"REASONING EVALUATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nProblem:\\n{result['problem']}\")\n",
    "        print(f\"\\nAnswer: {result['answer']}\")\n",
    "        \n",
    "        if result.get('correct_answer'):\n",
    "            print(f\"Correct Answer: {result['correct_answer']}\")\n",
    "            if result.get('is_correct'):\n",
    "                print(\"✓ Answer is correct\")\n",
    "            else:\n",
    "                print(\"✗ Answer is incorrect\")\n",
    "        \n",
    "        print(f\"\\nEvaluation:\")\n",
    "        \n",
    "        if isinstance(result['evaluation'], dict) and 'raw_evaluation' not in result['evaluation']:\n",
    "            for key, value in result['evaluation'].items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(result['evaluation'].get('raw_evaluation', result['evaluation']))\n",
    "\n",
    "# Test reasoning evaluation\n",
    "evaluator = ReasoningEvaluator()\n",
    "\n",
    "# Example reasoning to evaluate\n",
    "problem = \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours at the same speed?\"\n",
    "\n",
    "reasoning = \"\"\"\n",
    "Step 1: Identify the speed\n",
    "The car travels 60 miles in 1 hour, so speed = 60 mph\n",
    "\n",
    "Step 2: Multiply speed by time\n",
    "Distance = Speed × Time\n",
    "Distance = 60 mph × 2.5 hours\n",
    "\n",
    "Step 3: Calculate\n",
    "60 × 2.5 = 150\n",
    "\n",
    "Therefore, the car will travel 150 miles.\n",
    "\"\"\"\n",
    "\n",
    "answer = \"150 miles\"\n",
    "correct_answer = \"150 miles\"\n",
    "\n",
    "result = evaluator.evaluate_reasoning(problem, reasoning, answer, correct_answer)\n",
    "evaluator.display_evaluation(result)\n",
    "\n",
    "# Example with flawed reasoning\n",
    "flawed_reasoning = \"\"\"\n",
    "Step 1: The car goes 60 miles per hour\n",
    "Step 2: For 2.5 hours, we add 60 + 2.5\n",
    "Step 3: 60 + 2.5 = 62.5\n",
    "\n",
    "Answer: 62.5 miles\n",
    "\"\"\"\n",
    "\n",
    "flawed_result = evaluator.evaluate_reasoning(\n",
    "    problem,\n",
    "    flawed_reasoning,\n",
    "    \"62.5 miles\",\n",
    "    correct_answer\n",
    ")\n",
    "evaluator.display_evaluation(flawed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5769bc",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Evaluate Your Reasoning\n",
    "\n",
    "Evaluate reasoning chains you created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate reasoning from previous exercises\n",
    "\n",
    "evaluator = ReasoningEvaluator()\n",
    "\n",
    "# TODO: Take a problem you solved earlier and evaluate it\n",
    "# Example:\n",
    "# problem = \"...\"\n",
    "# reasoning = \"...\"\n",
    "# answer = \"...\"\n",
    "# correct_answer = \"...\"\n",
    "# \n",
    "# result = evaluator.evaluate_reasoning(problem, reasoning, answer, correct_answer)\n",
    "# evaluator.display_evaluation(result)\n",
    "\n",
    "# TODO: Compare zero-shot vs few-shot reasoning quality\n",
    "# Generate both approaches for the same problem and evaluate each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e205168",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Adaptive CoT System\n",
    "\n",
    "Build a system that adapts reasoning strategy based on problem type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217aafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCoTSystem:\n",
    "    \"\"\"\n",
    "    Automatically selects best CoT strategy for each problem type.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Problem classification (math, logic, planning, coding, etc.)\n",
    "    2. Strategy selection (zero-shot, few-shot, structured, self-consistency)\n",
    "    3. Dynamic example retrieval for few-shot\n",
    "    4. Confidence-based fallback (if low confidence, try different strategy)\n",
    "    5. Performance tracking and learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zero_shot = ZeroShotCoT()\n",
    "        self.few_shot = FewShotCoT()\n",
    "        self.complex_solver = ComplexCoTSolver()\n",
    "        self.self_consistency = SelfConsistencyCoT()\n",
    "        self.evaluator = ReasoningEvaluator()\n",
    "        \n",
    "        # TODO: Add example libraries for different domains\n",
    "        self.example_libraries = {}\n",
    "    \n",
    "    def classify_problem(self, problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify problem type.\n",
    "        \n",
    "        TODO: Implement classification logic\n",
    "        Returns: 'math', 'logic', 'planning', 'coding', 'decision', etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def select_strategy(self, problem_type: str, complexity: str) -> str:\n",
    "        \"\"\"\n",
    "        Select best strategy for problem type.\n",
    "        \n",
    "        TODO: Implement strategy selection\n",
    "        Returns: 'zero-shot', 'few-shot', 'structured', 'self-consistency'\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def solve(self, problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve problem with adaptive strategy.\n",
    "        \n",
    "        TODO: Implement adaptive solving:\n",
    "        1. Classify problem\n",
    "        2. Select initial strategy\n",
    "        3. Attempt solution\n",
    "        4. Evaluate confidence\n",
    "        5. If low confidence, try alternative strategy\n",
    "        6. Return best result\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# adaptive = AdaptiveCoTSystem()\n",
    "# result = adaptive.solve(\"Your problem here\")\n",
    "# print(f\"Strategy used: {result['strategy']}\")\n",
    "# print(f\"Answer: {result['answer']}\")\n",
    "# print(f\"Confidence: {result['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7635f",
   "metadata": {},
   "source": [
    "### Challenge 2: Interactive Reasoning Debugger\n",
    "\n",
    "Create a tool to debug and improve reasoning chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningDebugger:\n",
    "    \"\"\"\n",
    "    Interactive tool to debug and improve reasoning chains.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Step-by-step reasoning breakdown\n",
    "    2. Identify logical flaws in each step\n",
    "    3. Suggest improvements for weak steps\n",
    "    4. Test alternative reasoning paths\n",
    "    5. Compare multiple approaches side-by-side\n",
    "    6. Generate improved reasoning chain\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = ReasoningEvaluator()\n",
    "    \n",
    "    def analyze_step(self, step: str, context: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze individual reasoning step.\n",
    "        \n",
    "        TODO: Check for:\n",
    "        - Logical validity\n",
    "        - Missing information\n",
    "        - Assumptions made\n",
    "        - Potential errors\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def identify_weak_steps(self, reasoning: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Identify weak or problematic steps.\n",
    "        \n",
    "        TODO: Return list of issues with line numbers\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def suggest_improvements(self, reasoning: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate improved reasoning chain.\n",
    "        \n",
    "        TODO: Fix identified issues\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compare_approaches(self, problem: str, approaches: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare multiple reasoning approaches.\n",
    "        \n",
    "        TODO: Evaluate and rank different approaches\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# debugger = ReasoningDebugger()\n",
    "# issues = debugger.identify_weak_steps(reasoning_chain)\n",
    "# improved = debugger.suggest_improvements(reasoning_chain)\n",
    "# print(f\"Found {len(issues)} issues\")\n",
    "# print(f\"Improved reasoning:\\n{improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09736363",
   "metadata": {},
   "source": [
    "### Challenge 3: Domain-Specific Reasoning Library\n",
    "\n",
    "Build a comprehensive reasoning library for specific domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4328ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainReasoningLibrary:\n",
    "    \"\"\"\n",
    "    Domain-specific reasoning templates and examples.\n",
    "    \n",
    "    TODO: Implement for domains:\n",
    "    1. Scientific reasoning (hypothesis, experiment, conclusion)\n",
    "    2. Legal reasoning (precedent, statute, application)\n",
    "    3. Medical diagnosis (symptoms, differential, tests, diagnosis)\n",
    "    4. Engineering design (requirements, constraints, solutions)\n",
    "    5. Financial analysis (data, metrics, insights, recommendations)\n",
    "    \n",
    "    Each domain should have:\n",
    "    - Reasoning structure templates\n",
    "    - Common patterns and pitfalls\n",
    "    - Example problems with solutions\n",
    "    - Evaluation criteria\n",
    "    - Domain-specific validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str):\n",
    "        self.domain = domain\n",
    "        self.templates = {}\n",
    "        self.examples = []\n",
    "        self.validation_rules = []\n",
    "    \n",
    "    def load_domain_templates(self, domain: str):\n",
    "        \"\"\"\n",
    "        Load templates for specific domain.\n",
    "        \n",
    "        TODO: Load reasoning structures for domain\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_reasoning(self, problem: str, template: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate domain-specific reasoning.\n",
    "        \n",
    "        TODO: Apply domain template to problem\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate_reasoning(self, reasoning: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate reasoning against domain rules.\n",
    "        \n",
    "        TODO: Check domain-specific validity\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# medical = DomainReasoningLibrary(domain=\"medical\")\n",
    "# problem = \"Patient presents with fever, cough, and fatigue...\"\n",
    "# reasoning = medical.generate_reasoning(problem, template=\"diagnosis\")\n",
    "# validation = medical.validate_reasoning(reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5631b2a0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ Chain-of-thought prompting fundamentals\n",
    "2. ✅ Zero-shot CoT with trigger phrases\n",
    "3. ✅ Few-shot CoT with reasoning examples\n",
    "4. ✅ Complex multi-step reasoning structures\n",
    "5. ✅ Self-consistency for improved accuracy\n",
    "6. ✅ Reasoning evaluation and quality assessment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**When to Use CoT:**\n",
    "- Complex reasoning problems\n",
    "- Multi-step calculations\n",
    "- Logical deduction tasks\n",
    "- Planning and scheduling\n",
    "- Decision making with trade-offs\n",
    "- Debugging and problem diagnosis\n",
    "\n",
    "**Zero-Shot vs Few-Shot:**\n",
    "- **Zero-shot**: Quick, simple problems; when no examples available\n",
    "- **Few-shot**: Complex domains; consistent formatting needed; multiple similar problems\n",
    "\n",
    "**Self-Consistency Benefits:**\n",
    "- Higher accuracy on challenging problems\n",
    "- Confidence estimation\n",
    "- Robust to individual reasoning errors\n",
    "- Works well with ambiguous problems\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Clear trigger phrases**: \"Let's think step by step\" or \"Let's break this down\"\n",
    "2. **Explicit structure**: Define reasoning steps for complex problems\n",
    "3. **Quality examples**: For few-shot, use diverse, well-reasoned examples\n",
    "4. **Evaluate reasoning**: Don't just check answers, validate reasoning\n",
    "5. **Use self-consistency**: For high-stakes or difficult problems\n",
    "6. **Temperature matters**: Lower (0.3) for consistency, higher (0.7) for diversity\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ **Too vague**: \"Solve this problem\" → Need step-by-step guidance\n",
    "❌ **Skipping steps**: Reasoning jumps to conclusions\n",
    "❌ **No validation**: Accepting first answer without verification\n",
    "❌ **Wrong temperature**: Too low reduces diversity, too high reduces coherence\n",
    "❌ **Ignoring confidence**: Not checking self-consistency agreement\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Build domain-specific reasoning libraries for your work\n",
    "- Experiment with reasoning structures for different problem types\n",
    "- Move on to Week 3: Advanced API Usage & Function Calling\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
