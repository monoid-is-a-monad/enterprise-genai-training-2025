{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57778db",
   "metadata": {},
   "source": [
    "# Lab 2: OpenAI API Deep Dive\n",
    "\n",
    "**Week 3 - Advanced Prompting & OpenAI API**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Master all OpenAI API parameters and their effects\n",
    "- Implement robust error handling and retry logic\n",
    "- Optimize API usage for cost and performance\n",
    "- Use streaming responses effectively\n",
    "- Handle rate limits and token management\n",
    "- Implement request batching and caching\n",
    "- Monitor and log API usage\n",
    "- Build production-ready API wrappers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Week 2 labs\n",
    "- Understanding of async programming (helpful)\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e3469",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken tenacity aiohttp asyncio pandas matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from typing import List, Dict, Optional, Any, AsyncIterator, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI clients\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "async_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9c75e",
   "metadata": {},
   "source": [
    "## Part 1: Complete Parameter Guide\n",
    "\n",
    "Let's explore every OpenAI API parameter in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063af92",
   "metadata": {},
   "source": [
    "### Core Parameters\n",
    "\n",
    "**model**: Which model to use\n",
    "**messages**: The conversation history\n",
    "**temperature**: Randomness (0.0 = deterministic, 2.0 = very random)\n",
    "**max_tokens**: Maximum tokens in response\n",
    "**top_p**: Nucleus sampling (alternative to temperature)\n",
    "**frequency_penalty**: Reduce repetition of tokens (-2.0 to 2.0)\n",
    "**presence_penalty**: Encourage new topics (-2.0 to 2.0)\n",
    "**n**: Number of completions to generate\n",
    "**stop**: Sequences where API will stop generating\n",
    "**stream**: Whether to stream responses\n",
    "**logprobs**: Return log probabilities\n",
    "**user**: Unique identifier for end-user (for monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterExplorer:\n",
    "    \"\"\"\n",
    "    Explore and demonstrate OpenAI API parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize ParameterExplorer.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.results = []\n",
    "    \n",
    "    def explore_temperature(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperatures: List[float] = [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Show effect of temperature parameter.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt to test\n",
    "            temperatures: Temperature values to test\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TEMPERATURE EXPLORATION\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(f\"Temperature: {temp}\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def explore_top_p(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        top_p_values: List[float] = [0.1, 0.5, 0.9, 1.0]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Show effect of top_p (nucleus sampling).\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt to test\n",
    "            top_p_values: top_p values to test\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TOP_P EXPLORATION\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        for top_p in top_p_values:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=1.0,  # Use with temperature\n",
    "                top_p=top_p,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(f\"top_p: {top_p}\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def explore_penalties(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        frequency_penalties: List[float] = [0.0, 1.0, 2.0],\n",
    "        presence_penalties: List[float] = [0.0, 1.0, 2.0]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Show effect of frequency and presence penalties.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt to test\n",
    "            frequency_penalties: Frequency penalty values\n",
    "            presence_penalties: Presence penalty values\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PENALTY EXPLORATION\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        configs = [\n",
    "            (\"No penalties\", 0.0, 0.0),\n",
    "            (\"High frequency penalty\", 2.0, 0.0),\n",
    "            (\"High presence penalty\", 0.0, 2.0),\n",
    "            (\"Both high\", 2.0, 2.0)\n",
    "        ]\n",
    "        \n",
    "        for name, freq_penalty, pres_penalty in configs:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                frequency_penalty=freq_penalty,\n",
    "                presence_penalty=pres_penalty,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            print(f\"{name} (freq={freq_penalty}, pres={pres_penalty})\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def explore_n_parameter(self, prompt: str, n: int = 3):\n",
    "        \"\"\"\n",
    "        Generate multiple completions at once.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt to test\n",
    "            n: Number of completions\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"N PARAMETER EXPLORATION (n={n})\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8,\n",
    "            n=n,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        for i, choice in enumerate(response.choices, 1):\n",
    "            print(f\"Completion {i}:\")\n",
    "            print(choice.message.content)\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def explore_stop_sequences(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Show effect of stop sequences.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt to test\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STOP SEQUENCES EXPLORATION\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        configs = [\n",
    "            (\"No stop\", None),\n",
    "            (\"Stop at period\", [\".\"]),\n",
    "            (\"Stop at newline\", [\"\\n\"]),\n",
    "            (\"Multiple stops\", [\".\", \"\\n\", \"!\"])\n",
    "        ]\n",
    "        \n",
    "        for name, stop in configs:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                stop=stop,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            print(f\"{name}: {stop}\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Test parameter exploration\n",
    "explorer = ParameterExplorer()\n",
    "\n",
    "# Temperature\n",
    "explorer.explore_temperature(\n",
    "    \"Write a creative opening line for a science fiction story.\",\n",
    "    temperatures=[0.0, 0.7, 1.5]\n",
    ")\n",
    "\n",
    "# Penalties (test repetition)\n",
    "repetitive_prompt = \"List reasons why Python is popular. Start each reason with 'Python is'\"\n",
    "explorer.explore_penalties(repetitive_prompt)\n",
    "\n",
    "# N parameter\n",
    "explorer.explore_n_parameter(\n",
    "    \"Suggest a creative name for a coffee shop.\",\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36673255",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Parameter Experimentation\n",
    "\n",
    "Experiment with different parameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb71d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test parameter combinations for different use cases\n",
    "\n",
    "explorer = ParameterExplorer()\n",
    "\n",
    "# Use case 1: Deterministic code generation\n",
    "# TODO: What parameters would you use for consistent code generation?\n",
    "# Hint: Low temperature, maybe frequency_penalty for avoiding repetition\n",
    "\n",
    "# Use case 2: Creative writing\n",
    "# TODO: What parameters for maximum creativity?\n",
    "# Hint: Higher temperature or top_p\n",
    "\n",
    "# Use case 3: Formal business writing\n",
    "# TODO: What parameters for professional, concise responses?\n",
    "\n",
    "# Use case 4: Brainstorming (multiple diverse ideas)\n",
    "# TODO: Use n parameter with appropriate temperature\n",
    "\n",
    "# Test your configurations\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Your prompt\"}],\n",
    "#     temperature=...,\n",
    "#     # Add other parameters\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a5fd2",
   "metadata": {},
   "source": [
    "## Part 2: Error Handling and Retries\n",
    "\n",
    "Production applications need robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class APICallResult:\n",
    "    \"\"\"Result of an API call.\"\"\"\n",
    "    success: bool\n",
    "    response: Optional[Any] = None\n",
    "    error: Optional[str] = None\n",
    "    attempts: int = 1\n",
    "    total_time: float = 0.0\n",
    "\n",
    "class RobustAPIClient:\n",
    "    \"\"\"\n",
    "    OpenAI API client with comprehensive error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        max_retries: int = 3,\n",
    "        base_delay: float = 1.0,\n",
    "        max_delay: float = 60.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RobustAPIClient.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "            max_retries: Maximum retry attempts\n",
    "            base_delay: Base delay for exponential backoff (seconds)\n",
    "            max_delay: Maximum delay between retries (seconds)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.call_history: List[APICallResult] = []\n",
    "    \n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs\n",
    "    ) -> APICallResult:\n",
    "        \"\"\"\n",
    "        Make a chat completion with retry logic.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            APICallResult with response or error\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=kwargs.get('model', self.model),\n",
    "                    messages=messages,\n",
    "                    **{k: v for k, v in kwargs.items() if k != 'model'}\n",
    "                )\n",
    "                \n",
    "                result = APICallResult(\n",
    "                    success=True,\n",
    "                    response=response,\n",
    "                    attempts=attempt,\n",
    "                    total_time=time.time() - start_time\n",
    "                )\n",
    "                self.call_history.append(result)\n",
    "                return result\n",
    "                \n",
    "            except RateLimitError as e:\n",
    "                last_error = f\"Rate limit exceeded: {str(e)}\"\n",
    "                if attempt < self.max_retries:\n",
    "                    delay = min(self.base_delay * (2 ** (attempt - 1)), self.max_delay)\n",
    "                    print(f\"Rate limit hit. Retrying in {delay}s... (attempt {attempt}/{self.max_retries})\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except APITimeoutError as e:\n",
    "                last_error = f\"Request timeout: {str(e)}\"\n",
    "                if attempt < self.max_retries:\n",
    "                    delay = min(self.base_delay * (2 ** (attempt - 1)), self.max_delay)\n",
    "                    print(f\"Timeout. Retrying in {delay}s... (attempt {attempt}/{self.max_retries})\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except APIConnectionError as e:\n",
    "                last_error = f\"Connection error: {str(e)}\"\n",
    "                if attempt < self.max_retries:\n",
    "                    delay = min(self.base_delay * (2 ** (attempt - 1)), self.max_delay)\n",
    "                    print(f\"Connection error. Retrying in {delay}s... (attempt {attempt}/{self.max_retries})\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except APIError as e:\n",
    "                last_error = f\"API error: {str(e)}\"\n",
    "                # Don't retry on general API errors (might be client error)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = f\"Unexpected error: {str(e)}\"\n",
    "                break\n",
    "        \n",
    "        # All retries failed\n",
    "        result = APICallResult(\n",
    "            success=False,\n",
    "            error=last_error,\n",
    "            attempts=attempt,\n",
    "            total_time=time.time() - start_time\n",
    "        )\n",
    "        self.call_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about API calls.\"\"\"\n",
    "        total_calls = len(self.call_history)\n",
    "        successful = sum(1 for r in self.call_history if r.success)\n",
    "        failed = total_calls - successful\n",
    "        \n",
    "        avg_attempts = sum(r.attempts for r in self.call_history) / total_calls if total_calls > 0 else 0\n",
    "        avg_time = sum(r.total_time for r in self.call_history) / total_calls if total_calls > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": total_calls,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"success_rate\": f\"{(successful/total_calls*100):.1f}%\" if total_calls > 0 else \"0%\",\n",
    "            \"avg_attempts\": f\"{avg_attempts:.2f}\",\n",
    "            \"avg_time\": f\"{avg_time:.2f}s\"\n",
    "        }\n",
    "\n",
    "# Test robust client\n",
    "robust_client = RobustAPIClient(max_retries=3, base_delay=1.0)\n",
    "\n",
    "# Make some calls\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is Python?\"}]\n",
    "result = robust_client.chat_completion(messages, temperature=0.7)\n",
    "\n",
    "if result.success:\n",
    "    print(f\"✓ Success after {result.attempts} attempt(s)\")\n",
    "    print(f\"Response: {result.response.choices[0].message.content[:100]}...\")\n",
    "else:\n",
    "    print(f\"✗ Failed after {result.attempts} attempts\")\n",
    "    print(f\"Error: {result.error}\")\n",
    "\n",
    "# Make more calls\n",
    "for i in range(5):\n",
    "    result = robust_client.chat_completion(\n",
    "        [{\"role\": \"user\", \"content\": f\"Tell me a fact about {['Python', 'AI', 'ML', 'Data', 'Code'][i]}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# Get statistics\n",
    "stats = robust_client.get_statistics()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"API CALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94648a5",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Implement Advanced Error Handling\n",
    "\n",
    "Enhance the error handling with circuit breaker pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a90131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement circuit breaker pattern\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker to prevent cascading failures.\n",
    "    \n",
    "    States:\n",
    "    - CLOSED: Normal operation\n",
    "    - OPEN: Too many failures, reject requests\n",
    "    - HALF_OPEN: Testing if service recovered\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Track failure rate\n",
    "    2. Open circuit after threshold\n",
    "    3. Attempt recovery after timeout\n",
    "    4. Close circuit if recovery successful\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        failure_threshold: int = 5,\n",
    "        timeout: float = 60.0,\n",
    "        success_threshold: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize circuit breaker.\n",
    "        \n",
    "        Args:\n",
    "            failure_threshold: Failures before opening circuit\n",
    "            timeout: Seconds before attempting recovery\n",
    "            success_threshold: Successes needed to close circuit\n",
    "        \"\"\"\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.success_threshold = success_threshold\n",
    "        \n",
    "        # TODO: Initialize state tracking\n",
    "        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n",
    "        self.failures = 0\n",
    "        self.successes = 0\n",
    "        self.last_failure_time = None\n",
    "    \n",
    "    def call(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Execute function with circuit breaker protection.\n",
    "        \n",
    "        TODO: Implement circuit breaker logic\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"TODO: Record successful call.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"TODO: Record failed call.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# breaker = CircuitBreaker(failure_threshold=3, timeout=30.0)\n",
    "# result = breaker.call(robust_client.chat_completion, messages, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569bfa9",
   "metadata": {},
   "source": [
    "## Part 3: Streaming Responses\n",
    "\n",
    "Stream responses for better user experience with long outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0baf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingHandler:\n",
    "    \"\"\"\n",
    "    Handle streaming responses from OpenAI API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize StreamingHandler.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def stream_response(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        callback: Optional[Callable[[str], None]] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Stream response with optional callback.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            callback: Function called with each chunk\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            Complete response text\n",
    "        \"\"\"\n",
    "        full_response = \"\"\n",
    "        \n",
    "        stream = client.chat.completions.create(\n",
    "            model=kwargs.get('model', self.model),\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            **{k: v for k, v in kwargs.items() if k not in ['model', 'stream']}\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                \n",
    "                if callback:\n",
    "                    callback(content)\n",
    "                else:\n",
    "                    print(content, end='', flush=True)\n",
    "        \n",
    "        print()  # New line after streaming\n",
    "        return full_response\n",
    "    \n",
    "    async def async_stream_response(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        callback: Optional[Callable[[str], None]] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Async version of stream_response.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            callback: Function called with each chunk\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            Complete response text\n",
    "        \"\"\"\n",
    "        full_response = \"\"\n",
    "        \n",
    "        stream = await async_client.chat.completions.create(\n",
    "            model=kwargs.get('model', self.model),\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            **{k: v for k, v in kwargs.items() if k not in ['model', 'stream']}\n",
    "        )\n",
    "        \n",
    "        async for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                \n",
    "                if callback:\n",
    "                    callback(content)\n",
    "                else:\n",
    "                    print(content, end='', flush=True)\n",
    "        \n",
    "        print()\n",
    "        return full_response\n",
    "    \n",
    "    def stream_with_indicators(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        show_speed: bool = True,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Stream with speed and token indicators.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            show_speed: Show streaming speed\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            Dict with response and metadata\n",
    "        \"\"\"\n",
    "        full_response = \"\"\n",
    "        start_time = time.time()\n",
    "        chunk_count = 0\n",
    "        \n",
    "        print(\"Streaming response...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        stream = client.chat.completions.create(\n",
    "            model=kwargs.get('model', self.model),\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            **{k: v for k, v in kwargs.items() if k not in ['model', 'stream']}\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                chunk_count += 1\n",
    "                print(content, end='', flush=True)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print()\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        if show_speed:\n",
    "            chars_per_sec = len(full_response) / elapsed if elapsed > 0 else 0\n",
    "            print(f\"✓ Completed in {elapsed:.2f}s\")\n",
    "            print(f\"✓ Speed: {chars_per_sec:.0f} chars/sec\")\n",
    "            print(f\"✓ Chunks: {chunk_count}\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": full_response,\n",
    "            \"elapsed_time\": elapsed,\n",
    "            \"chunk_count\": chunk_count,\n",
    "            \"chars_per_second\": len(full_response) / elapsed if elapsed > 0 else 0\n",
    "        }\n",
    "\n",
    "# Test streaming\n",
    "streamer = StreamingHandler()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BASIC STREAMING\")\n",
    "print(\"=\"*80)\n",
    "response = streamer.stream_response(\n",
    "    [{\"role\": \"user\", \"content\": \"Write a short story about a robot learning to paint (3 paragraphs).\"}],\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STREAMING WITH INDICATORS\")\n",
    "print(\"=\"*80)\n",
    "result = streamer.stream_with_indicators(\n",
    "    [{\"role\": \"user\", \"content\": \"Explain how neural networks work in 5 steps.\"}],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb7149",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Build Custom Streaming UI\n",
    "\n",
    "Create a custom streaming handler with rich formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build custom streaming handler with formatting\n",
    "\n",
    "class RichStreamingHandler(StreamingHandler):\n",
    "    \"\"\"\n",
    "    Enhanced streaming with formatting and progress.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Token-by-token display with syntax highlighting\n",
    "    2. Progress indicator showing estimated completion\n",
    "    3. Ability to pause/resume streaming\n",
    "    4. Save stream to file in real-time\n",
    "    5. Handle markdown formatting as it streams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        super().__init__(model)\n",
    "        # TODO: Add formatting utilities\n",
    "    \n",
    "    def stream_with_formatting(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        format_type: str = \"markdown\",\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Stream with real-time formatting.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            format_type: Type of formatting (markdown, code, plain)\n",
    "            **kwargs: Additional API parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def stream_to_file(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        filepath: str,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Stream response directly to file.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            filepath: Output file path\n",
    "            **kwargs: Additional API parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# rich_streamer = RichStreamingHandler()\n",
    "# response = rich_streamer.stream_with_formatting(\n",
    "#     [{\"role\": \"user\", \"content\": \"Write a Python function with documentation.\"}],\n",
    "#     format_type=\"code\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb82815",
   "metadata": {},
   "source": [
    "## Part 4: Rate Limiting and Token Management\n",
    "\n",
    "Handle rate limits and optimize token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenBudget:\n",
    "    \"\"\"Token budget for rate limiting.\"\"\"\n",
    "    tokens_per_minute: int\n",
    "    requests_per_minute: int\n",
    "    tokens_used: int = 0\n",
    "    requests_made: int = 0\n",
    "    window_start: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "class RateLimitedClient:\n",
    "    \"\"\"\n",
    "    API client with rate limiting and token management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        tokens_per_minute: int = 90000,\n",
    "        requests_per_minute: int = 3500\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RateLimitedClient.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "            tokens_per_minute: Token limit per minute\n",
    "            requests_per_minute: Request limit per minute\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.budget = TokenBudget(\n",
    "            tokens_per_minute=tokens_per_minute,\n",
    "            requests_per_minute=requests_per_minute\n",
    "        )\n",
    "        self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, messages: List[Dict[str, str]]) -> int:\n",
    "        \"\"\"\n",
    "        Count tokens in messages.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "        \n",
    "        Returns:\n",
    "            Token count\n",
    "        \"\"\"\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            # Every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            num_tokens += 4\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(self.encoding.encode(value))\n",
    "        num_tokens += 2  # Every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    \n",
    "    def reset_budget_if_needed(self):\n",
    "        \"\"\"Reset budget if window expired.\"\"\"\n",
    "        now = datetime.now()\n",
    "        elapsed = (now - self.budget.window_start).total_seconds()\n",
    "        \n",
    "        if elapsed >= 60:\n",
    "            self.budget.tokens_used = 0\n",
    "            self.budget.requests_made = 0\n",
    "            self.budget.window_start = now\n",
    "    \n",
    "    def can_make_request(self, estimated_tokens: int) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Check if request can be made within limits.\n",
    "        \n",
    "        Args:\n",
    "            estimated_tokens: Estimated tokens for request\n",
    "        \n",
    "        Returns:\n",
    "            (can_proceed, reason)\n",
    "        \"\"\"\n",
    "        self.reset_budget_if_needed()\n",
    "        \n",
    "        if self.budget.requests_made >= self.budget.requests_per_minute:\n",
    "            return False, \"Request limit reached\"\n",
    "        \n",
    "        if self.budget.tokens_used + estimated_tokens > self.budget.tokens_per_minute:\n",
    "            return False, \"Token limit would be exceeded\"\n",
    "        \n",
    "        return True, \"OK\"\n",
    "    \n",
    "    def wait_if_needed(self, estimated_tokens: int):\n",
    "        \"\"\"Wait if rate limit would be exceeded.\"\"\"\n",
    "        can_proceed, reason = self.can_make_request(estimated_tokens)\n",
    "        \n",
    "        if not can_proceed:\n",
    "            elapsed = (datetime.now() - self.budget.window_start).total_seconds()\n",
    "            wait_time = max(60 - elapsed, 0)\n",
    "            print(f\"Rate limit reached ({reason}). Waiting {wait_time:.1f}s...\")\n",
    "            time.sleep(wait_time + 1)\n",
    "            self.reset_budget_if_needed()\n",
    "    \n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make chat completion with rate limiting.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            API response\n",
    "        \"\"\"\n",
    "        # Count input tokens\n",
    "        input_tokens = self.count_tokens(messages)\n",
    "        max_tokens = kwargs.get('max_tokens', 1000)\n",
    "        estimated_total = input_tokens + max_tokens\n",
    "        \n",
    "        # Wait if needed\n",
    "        self.wait_if_needed(estimated_total)\n",
    "        \n",
    "        # Make request\n",
    "        response = client.chat.completions.create(\n",
    "            model=kwargs.get('model', self.model),\n",
    "            messages=messages,\n",
    "            **{k: v for k, v in kwargs.items() if k != 'model'}\n",
    "        )\n",
    "        \n",
    "        # Update budget\n",
    "        actual_tokens = response.usage.total_tokens\n",
    "        self.budget.tokens_used += actual_tokens\n",
    "        self.budget.requests_made += 1\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_usage_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current usage statistics.\"\"\"\n",
    "        self.reset_budget_if_needed()\n",
    "        \n",
    "        return {\n",
    "            \"tokens_used\": self.budget.tokens_used,\n",
    "            \"tokens_limit\": self.budget.tokens_per_minute,\n",
    "            \"tokens_remaining\": self.budget.tokens_per_minute - self.budget.tokens_used,\n",
    "            \"tokens_usage_pct\": f\"{(self.budget.tokens_used / self.budget.tokens_per_minute * 100):.1f}%\",\n",
    "            \"requests_made\": self.budget.requests_made,\n",
    "            \"requests_limit\": self.budget.requests_per_minute,\n",
    "            \"requests_remaining\": self.budget.requests_per_minute - self.budget.requests_made,\n",
    "            \"window_reset_in\": f\"{max(60 - (datetime.now() - self.budget.window_start).total_seconds(), 0):.1f}s\"\n",
    "        }\n",
    "\n",
    "# Test rate-limited client\n",
    "rate_limited = RateLimitedClient(\n",
    "    tokens_per_minute=10000,  # Lower limit for testing\n",
    "    requests_per_minute=10\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RATE-LIMITED API CLIENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make several requests\n",
    "for i in range(5):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"Tell me fact #{i+1} about Python.\"}]\n",
    "    \n",
    "    response = rate_limited.chat_completion(messages, max_tokens=100)\n",
    "    print(f\"\\nRequest {i+1}:\")\n",
    "    print(response.choices[0].message.content[:100] + \"...\")\n",
    "    \n",
    "    stats = rate_limited.get_usage_stats()\n",
    "    print(f\"Tokens used: {stats['tokens_used']}/{stats['tokens_limit']} ({stats['tokens_usage_pct']})\")\n",
    "    print(f\"Requests made: {stats['requests_made']}/{stats['requests_limit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f90b0",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Implement Token Optimization\n",
    "\n",
    "Create a system to optimize token usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aae9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement token optimization strategies\n",
    "\n",
    "class TokenOptimizer:\n",
    "    \"\"\"\n",
    "    Optimize token usage across requests.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Conversation summarization when context gets too long\n",
    "    2. Smart truncation of messages\n",
    "    3. Compression techniques for prompts\n",
    "    4. Batch similar requests together\n",
    "    5. Cache common responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", max_context_tokens: int = 4000):\n",
    "        \"\"\"\n",
    "        Initialize TokenOptimizer.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "            max_context_tokens: Maximum tokens to keep in context\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def summarize_conversation(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]]\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        TODO: Summarize conversation to reduce tokens.\n",
    "        \n",
    "        Args:\n",
    "            messages: Conversation history\n",
    "        \n",
    "        Returns:\n",
    "            Compressed messages\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compress_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Compress prompt while preserving meaning.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Original prompt\n",
    "        \n",
    "        Returns:\n",
    "            Compressed prompt\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def should_compress(self, messages: List[Dict[str, str]]) -> bool:\n",
    "        \"\"\"\n",
    "        TODO: Determine if compression is needed.\n",
    "        \n",
    "        Args:\n",
    "            messages: Current messages\n",
    "        \n",
    "        Returns:\n",
    "            True if compression needed\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# optimizer = TokenOptimizer(max_context_tokens=1000)\n",
    "# \n",
    "# long_conversation = [\n",
    "#     {\"role\": \"user\", \"content\": \"Tell me about Python.\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Python is a high-level...\"},\n",
    "#     # ... many more messages\n",
    "# ]\n",
    "# \n",
    "# if optimizer.should_compress(long_conversation):\n",
    "#     compressed = optimizer.summarize_conversation(long_conversation)\n",
    "#     print(f\"Reduced from {len(long_conversation)} to {len(compressed)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fa894",
   "metadata": {},
   "source": [
    "## Part 5: Request Batching and Async Operations\n",
    "\n",
    "Optimize throughput with batching and async requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchAPIClient:\n",
    "    \"\"\"\n",
    "    Process multiple API requests efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize BatchAPIClient.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    async def async_chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Async chat completion.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            API response\n",
    "        \"\"\"\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=kwargs.get('model', self.model),\n",
    "            messages=messages,\n",
    "            **{k: v for k, v in kwargs.items() if k != 'model'}\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    async def process_batch(\n",
    "        self,\n",
    "        message_list: List[List[Dict[str, str]]],\n",
    "        **kwargs\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Process multiple requests concurrently.\n",
    "        \n",
    "        Args:\n",
    "            message_list: List of message lists\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            self.async_chat_completion(messages, **kwargs)\n",
    "            for messages in message_list\n",
    "        ]\n",
    "        \n",
    "        responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return responses\n",
    "    \n",
    "    def batch_process(\n",
    "        self,\n",
    "        message_list: List[List[Dict[str, str]]],\n",
    "        batch_size: int = 10,\n",
    "        **kwargs\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Process requests in batches (handles event loop).\n",
    "        \n",
    "        Args:\n",
    "            message_list: List of message lists\n",
    "            batch_size: Requests per batch\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        all_responses = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(message_list), batch_size):\n",
    "            batch = message_list[i:i+batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1} ({len(batch)} requests)...\")\n",
    "            \n",
    "            # Run async batch\n",
    "            responses = asyncio.run(self.process_batch(batch, **kwargs))\n",
    "            all_responses.extend(responses)\n",
    "            \n",
    "            print(f\"✓ Batch complete\")\n",
    "        \n",
    "        return all_responses\n",
    "\n",
    "# Test batch processing\n",
    "batch_client = BatchAPIClient()\n",
    "\n",
    "# Create multiple requests\n",
    "requests = [\n",
    "    [{\"role\": \"user\", \"content\": f\"What is {topic}?\"}]\n",
    "    for topic in [\"Python\", \"JavaScript\", \"SQL\", \"HTML\", \"CSS\", \"React\", \"Node.js\", \"MongoDB\"]\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Processing {len(requests)} requests...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "responses = batch_client.batch_process(requests, batch_size=4, temperature=0.5, max_tokens=100)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completed {len(responses)} requests in {elapsed:.2f}s\")\n",
    "print(f\"Average: {elapsed/len(responses):.2f}s per request\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Show some results\n",
    "for i, response in enumerate(responses[:3]):\n",
    "    if not isinstance(response, Exception):\n",
    "        print(f\"Request {i+1}:\")\n",
    "        print(response.choices[0].message.content[:100] + \"...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728315c",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Build Smart Batch Processor\n",
    "\n",
    "Create an intelligent batch processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build smart batch processor\n",
    "\n",
    "class SmartBatchProcessor(BatchAPIClient):\n",
    "    \"\"\"\n",
    "    Intelligent batch processing with optimization.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Automatic batch size optimization based on rate limits\n",
    "    2. Priority queue for urgent requests\n",
    "    3. Request deduplication (skip identical requests)\n",
    "    4. Result caching\n",
    "    5. Adaptive retry for failed requests in batch\n",
    "    6. Progress tracking with ETA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        cache_results: bool = True\n",
    "    ):\n",
    "        super().__init__(model)\n",
    "        self.cache: Dict[str, Any] = {}\n",
    "        self.cache_results = cache_results\n",
    "    \n",
    "    def _cache_key(self, messages: List[Dict[str, str]], **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Generate cache key for request.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            **kwargs: API parameters\n",
    "        \n",
    "        Returns:\n",
    "            Cache key\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    async def process_with_cache(\n",
    "        self,\n",
    "        message_list: List[List[Dict[str, str]]],\n",
    "        **kwargs\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"\n",
    "        TODO: Process batch with caching and deduplication.\n",
    "        \n",
    "        Args:\n",
    "            message_list: List of message lists\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of responses\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def process_with_priority(\n",
    "        self,\n",
    "        requests: List[tuple[List[Dict[str, str]], int]],  # (messages, priority)\n",
    "        **kwargs\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"\n",
    "        TODO: Process requests respecting priority.\n",
    "        \n",
    "        Args:\n",
    "            requests: List of (messages, priority) tuples\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of responses in original order\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# smart_processor = SmartBatchProcessor(cache_results=True)\n",
    "# \n",
    "# requests_with_priority = [\n",
    "#     ([{\"role\": \"user\", \"content\": \"Urgent: System status?\"}], 1),  # High priority\n",
    "#     ([{\"role\": \"user\", \"content\": \"What is Python?\"}], 3),  # Low priority\n",
    "#     ([{\"role\": \"user\", \"content\": \"What is Python?\"}], 3),  # Duplicate\n",
    "# ]\n",
    "# \n",
    "# responses = smart_processor.process_with_priority(requests_with_priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4a39e",
   "metadata": {},
   "source": [
    "## Part 6: Monitoring and Logging\n",
    "\n",
    "Track API usage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class APICallLog:\n",
    "    \"\"\"Log entry for API call.\"\"\"\n",
    "    timestamp: datetime\n",
    "    model: str\n",
    "    messages: List[Dict[str, str]]\n",
    "    parameters: Dict[str, Any]\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "    latency: float\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class APIMonitor:\n",
    "    \"\"\"\n",
    "    Monitor and log API usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize APIMonitor.\n",
    "        \n",
    "        Args:\n",
    "            model: OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.logs: List[APICallLog] = []\n",
    "        \n",
    "        # Pricing (per 1K tokens) - update as needed\n",
    "        self.pricing = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
    "        }\n",
    "    \n",
    "    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate cost of API call.\"\"\"\n",
    "        if model not in self.pricing:\n",
    "            model = \"gpt-3.5-turbo\"  # Default\n",
    "        \n",
    "        input_cost = (input_tokens / 1000) * self.pricing[model][\"input\"]\n",
    "        output_cost = (output_tokens / 1000) * self.pricing[model][\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Make chat completion with logging.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages\n",
    "            **kwargs: Additional API parameters\n",
    "        \n",
    "        Returns:\n",
    "            API response\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        model = kwargs.get('model', self.model)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **{k: v for k, v in kwargs.items() if k != 'model'}\n",
    "            )\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            usage = response.usage\n",
    "            cost = self.calculate_cost(\n",
    "                model,\n",
    "                usage.prompt_tokens,\n",
    "                usage.completion_tokens\n",
    "            )\n",
    "            \n",
    "            log = APICallLog(\n",
    "                timestamp=datetime.now(),\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                parameters={k: v for k, v in kwargs.items() if k != 'model'},\n",
    "                tokens_used=usage.total_tokens,\n",
    "                cost=cost,\n",
    "                latency=latency,\n",
    "                success=True\n",
    "            )\n",
    "            self.logs.append(log)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            log = APICallLog(\n",
    "                timestamp=datetime.now(),\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                parameters={k: v for k, v in kwargs.items() if k != 'model'},\n",
    "                tokens_used=0,\n",
    "                cost=0.0,\n",
    "                latency=latency,\n",
    "                success=False,\n",
    "                error=str(e)\n",
    "            )\n",
    "            self.logs.append(log)\n",
    "            raise\n",
    "    \n",
    "    def get_summary(self, hours: Optional[float] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get usage summary.\n",
    "        \n",
    "        Args:\n",
    "            hours: Hours to look back (None = all time)\n",
    "        \n",
    "        Returns:\n",
    "            Summary statistics\n",
    "        \"\"\"\n",
    "        logs = self.logs\n",
    "        \n",
    "        if hours:\n",
    "            cutoff = datetime.now() - timedelta(hours=hours)\n",
    "            logs = [log for log in logs if log.timestamp >= cutoff]\n",
    "        \n",
    "        if not logs:\n",
    "            return {\"message\": \"No API calls logged\"}\n",
    "        \n",
    "        total_calls = len(logs)\n",
    "        successful = sum(1 for log in logs if log.success)\n",
    "        failed = total_calls - successful\n",
    "        \n",
    "        total_tokens = sum(log.tokens_used for log in logs)\n",
    "        total_cost = sum(log.cost for log in logs)\n",
    "        avg_latency = sum(log.latency for log in logs) / total_calls\n",
    "        \n",
    "        # Model breakdown\n",
    "        model_counts = defaultdict(int)\n",
    "        for log in logs:\n",
    "            model_counts[log.model] += 1\n",
    "        \n",
    "        return {\n",
    "            \"time_period\": f\"Last {hours} hours\" if hours else \"All time\",\n",
    "            \"total_calls\": total_calls,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"success_rate\": f\"{(successful/total_calls*100):.1f}%\",\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"total_cost\": f\"${total_cost:.4f}\",\n",
    "            \"avg_latency\": f\"{avg_latency:.2f}s\",\n",
    "            \"models_used\": dict(model_counts)\n",
    "        }\n",
    "    \n",
    "    def export_logs(self, filepath: str):\n",
    "        \"\"\"Export logs to JSON file.\"\"\"\n",
    "        logs_data = [\n",
    "            {\n",
    "                \"timestamp\": log.timestamp.isoformat(),\n",
    "                \"model\": log.model,\n",
    "                \"tokens\": log.tokens_used,\n",
    "                \"cost\": log.cost,\n",
    "                \"latency\": log.latency,\n",
    "                \"success\": log.success,\n",
    "                \"error\": log.error\n",
    "            }\n",
    "            for log in self.logs\n",
    "        ]\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(logs_data, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Exported {len(logs_data)} logs to {filepath}\")\n",
    "\n",
    "# Test monitor\n",
    "monitor = APIMonitor()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"API MONITORING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make some calls\n",
    "test_messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is Python?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain machine learning.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Write a haiku about coding.\"}]\n",
    "]\n",
    "\n",
    "for messages in test_messages:\n",
    "    response = monitor.chat_completion(messages, temperature=0.7, max_tokens=150)\n",
    "    print(f\"\\n✓ Request completed: {messages[0]['content'][:30]}...\")\n",
    "\n",
    "# Get summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"USAGE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "summary = monitor.get_summary()\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Export logs\n",
    "monitor.export_logs(\"api_usage_logs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2f962",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Build Dashboard\n",
    "\n",
    "Create a usage dashboard with visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build usage dashboard\n",
    "\n",
    "class APIDashboard(APIMonitor):\n",
    "    \"\"\"\n",
    "    Visual dashboard for API usage.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Real-time usage charts (tokens, cost, latency over time)\n",
    "    2. Model comparison view\n",
    "    3. Cost projections\n",
    "    4. Alert system for unusual usage\n",
    "    5. Export reports in multiple formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def plot_usage_over_time(self, metric: str = \"tokens\"):\n",
    "        \"\"\"\n",
    "        TODO: Plot usage metric over time.\n",
    "        \n",
    "        Args:\n",
    "            metric: Metric to plot (tokens, cost, latency, calls)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def plot_model_comparison(self):\n",
    "        \"\"\"TODO: Compare usage across models.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_report(self, format: str = \"html\") -> str:\n",
    "        \"\"\"\n",
    "        TODO: Generate usage report.\n",
    "        \n",
    "        Args:\n",
    "            format: Report format (html, pdf, markdown)\n",
    "        \n",
    "        Returns:\n",
    "            Report content or file path\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_alerts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        TODO: Check for unusual patterns.\n",
    "        \n",
    "        Returns:\n",
    "            List of alert messages\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your dashboard\n",
    "# dashboard = APIDashboard()\n",
    "# # ... make API calls ...\n",
    "# dashboard.plot_usage_over_time(\"cost\")\n",
    "# dashboard.plot_model_comparison()\n",
    "# alerts = dashboard.check_alerts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d50ef",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Production API Wrapper\n",
    "\n",
    "Build a complete production-ready API wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAPIWrapper:\n",
    "    \"\"\"\n",
    "    Production-ready OpenAI API wrapper.\n",
    "    \n",
    "    TODO: Combine all features:\n",
    "    1. Robust error handling with circuit breaker\n",
    "    2. Rate limiting and token management\n",
    "    3. Request caching and deduplication\n",
    "    4. Streaming support\n",
    "    5. Batch processing\n",
    "    6. Comprehensive monitoring and logging\n",
    "    7. Cost optimization\n",
    "    8. Health checks and metrics\n",
    "    9. Graceful degradation\n",
    "    10. Configuration management\n",
    "    \n",
    "    Should be:\n",
    "    - Thread-safe\n",
    "    - Async-compatible\n",
    "    - Well-documented\n",
    "    - Thoroughly tested\n",
    "    - Easy to configure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dict with all settings\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement all methods\n",
    "\n",
    "# Usage example:\n",
    "# config = {\n",
    "#     \"model\": \"gpt-3.5-turbo\",\n",
    "#     \"rate_limits\": {\"tokens_per_minute\": 90000},\n",
    "#     \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"},\n",
    "#     \"cache\": {\"enabled\": True, \"ttl\": 3600},\n",
    "#     \"monitoring\": {\"enabled\": True, \"log_level\": \"INFO\"}\n",
    "# }\n",
    "# \n",
    "# api = ProductionAPIWrapper(config)\n",
    "# response = api.chat_completion([{\"role\": \"user\", \"content\": \"Hello!\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fc123",
   "metadata": {},
   "source": [
    "### Challenge 2: API Cost Optimizer\n",
    "\n",
    "Build a system that minimizes API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6806731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APICostOptimizer:\n",
    "    \"\"\"\n",
    "    Optimize API usage costs.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Model selection based on task complexity\n",
    "    2. Automatic prompt compression\n",
    "    3. Aggressive caching strategy\n",
    "    4. Batch similar requests\n",
    "    5. Use cheaper models for simple tasks\n",
    "    6. Context window optimization\n",
    "    7. Cost prediction before requests\n",
    "    8. Budget enforcement\n",
    "    9. Cost analytics and recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, budget: float, period: str = \"daily\"):\n",
    "        \"\"\"\n",
    "        Initialize with budget.\n",
    "        \n",
    "        Args:\n",
    "            budget: Budget amount in USD\n",
    "            period: Budget period (hourly, daily, monthly)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement optimization methods\n",
    "\n",
    "# Usage:\n",
    "# optimizer = APICostOptimizer(budget=10.0, period=\"daily\")\n",
    "# response = optimizer.optimized_completion(\n",
    "#     prompt=\"Your prompt\",\n",
    "#     complexity=\"simple\"  # or \"medium\", \"complex\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86023de6",
   "metadata": {},
   "source": [
    "### Challenge 3: Multi-Model Orchestrator\n",
    "\n",
    "Build a system that intelligently routes requests to different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelOrchestrator:\n",
    "    \"\"\"\n",
    "    Route requests to optimal models.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Task classification (simple/medium/complex)\n",
    "    2. Model routing based on task and budget\n",
    "    3. Fallback to cheaper models\n",
    "    4. A/B testing between models\n",
    "    5. Performance tracking per model\n",
    "    6. Automatic model selection optimization\n",
    "    7. Support multiple providers (OpenAI, Anthropic, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Initialize with model configurations.\n",
    "        \n",
    "        Args:\n",
    "            models: List of model configs with capabilities and costs\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement routing logic\n",
    "\n",
    "# Usage:\n",
    "# models = [\n",
    "#     {\"name\": \"gpt-3.5-turbo\", \"cost\": \"low\", \"capabilities\": [\"general\"]},\n",
    "#     {\"name\": \"gpt-4\", \"cost\": \"high\", \"capabilities\": [\"reasoning\", \"coding\"]},\n",
    "# ]\n",
    "# \n",
    "# orchestrator = MultiModelOrchestrator(models)\n",
    "# response = orchestrator.route_and_complete(\n",
    "#     prompt=\"Your prompt\",\n",
    "#     preferred_cost=\"low\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6909d3f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ Complete OpenAI API parameter reference\n",
    "2. ✅ Production-grade error handling and retries\n",
    "3. ✅ Streaming responses for better UX\n",
    "4. ✅ Rate limiting and token management\n",
    "5. ✅ Batch processing and async operations\n",
    "6. ✅ Comprehensive monitoring and logging\n",
    "7. ✅ Cost optimization strategies\n",
    "8. ✅ Building production-ready API wrappers\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Parameter Mastery:**\n",
    "- **temperature**: Control randomness (0.0-2.0)\n",
    "- **top_p**: Alternative sampling method\n",
    "- **frequency_penalty**: Reduce repetition\n",
    "- **presence_penalty**: Encourage topic diversity\n",
    "- **n**: Generate multiple completions\n",
    "- **stop**: Control generation endpoints\n",
    "\n",
    "**Production Best Practices:**\n",
    "1. **Always handle errors**: Use retry logic with exponential backoff\n",
    "2. **Respect rate limits**: Track usage, implement throttling\n",
    "3. **Stream when possible**: Better UX for long responses\n",
    "4. **Monitor everything**: Log calls, track costs, measure performance\n",
    "5. **Optimize costs**: Cache, batch, compress, choose right models\n",
    "6. **Test thoroughly**: Handle edge cases, network issues, rate limits\n",
    "\n",
    "**Performance Optimization:**\n",
    "- Use async for concurrent requests\n",
    "- Batch similar requests together\n",
    "- Cache common responses\n",
    "- Compress long contexts\n",
    "- Choose appropriate models for task complexity\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ **No retry logic**: Fails on temporary errors\n",
    "❌ **Ignoring rate limits**: Gets blocked by API\n",
    "❌ **No monitoring**: Can't debug issues or track costs\n",
    "❌ **Synchronous processing**: Slow for multiple requests\n",
    "❌ **Poor error messages**: Hard to debug problems\n",
    "❌ **No token tracking**: Unexpected costs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Build your own production API wrapper\n",
    "- Implement cost optimization for your use case\n",
    "- Move on to Lab 3: Function Calling System\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
