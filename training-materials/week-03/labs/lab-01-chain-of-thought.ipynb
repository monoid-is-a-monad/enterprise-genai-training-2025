{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04f6d98",
   "metadata": {},
   "source": [
    "# Lab 1: Chain-of-Thought Implementation\n",
    "\n",
    "**Week 3 - Advanced Prompting & OpenAI API**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Implement zero-shot chain-of-thought prompting\n",
    "- Compare CoT vs non-CoT performance\n",
    "- Build a self-consistency system\n",
    "- Create a prompt chain for complex tasks\n",
    "- Measure and analyze reasoning quality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- OpenAI API key configured\n",
    "- Understanding of prompt engineering basics\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d2f90",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eda894",
   "metadata": {},
   "source": [
    "## Part 1: Zero-Shot Chain-of-Thought\n",
    "\n",
    "Let's implement and test zero-shot CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model=\"gpt-4\", temperature=0.3):\n",
    "    \"\"\"Generate a response from the OpenAI API.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def zero_shot_cot(problem, model=\"gpt-4\"):\n",
    "    \"\"\"Apply zero-shot chain-of-thought.\"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step by step.\"\n",
    "    return generate_response(prompt, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005b257",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Test Zero-Shot CoT on Math Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problem\n",
    "problem = \"\"\"\n",
    "A bakery sells cupcakes for $3 each and cookies for $2 each.\n",
    "Sarah buys 4 cupcakes and has $15 total to spend.\n",
    "How many cookies can she buy with the remaining money?\n",
    "\"\"\"\n",
    "\n",
    "# Without CoT\n",
    "print(\"=== Without Chain-of-Thought ===\")\n",
    "result_no_cot = generate_response(problem)\n",
    "print(result_no_cot)\n",
    "print()\n",
    "\n",
    "# With CoT\n",
    "print(\"=== With Chain-of-Thought ===\")\n",
    "result_cot = zero_shot_cot(problem)\n",
    "print(result_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a9ccb",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Logic Puzzle with CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_problem = \"\"\"\n",
    "Three switches outside a room control three light bulbs inside.\n",
    "You can flip the switches as many times as you want, but you can only\n",
    "enter the room once. How can you determine which switch controls which bulb?\n",
    "\"\"\"\n",
    "\n",
    "solution = zero_shot_cot(logic_problem)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e94ca1",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Code Debugging with CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_problem = \"\"\"\n",
    "This function should check if a string is a palindrome, but it's not working:\n",
    "\n",
    "def is_palindrome(s):\n",
    "    return s == s.reverse()\n",
    "\n",
    "What's wrong and how can we fix it?\n",
    "\"\"\"\n",
    "\n",
    "debugging_result = zero_shot_cot(code_problem)\n",
    "print(debugging_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93db5c",
   "metadata": {},
   "source": [
    "## Part 2: Self-Consistency\n",
    "\n",
    "Implement self-consistency to improve accuracy through multiple reasoning paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text):\n",
    "    \"\"\"Extract the final answer from reasoning text.\"\"\"\n",
    "    # Look for common answer patterns\n",
    "    patterns = [\n",
    "        r'(?:Answer|Final answer|Therefore|Thus|So):?\\s*(.+?)(?:\\n|$)',\n",
    "        r'(?:is|equals?)\\s+(\\d+)',\n",
    "        r'(\\d+)\\s+(?:is the answer|is correct)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: return last line\n",
    "    lines = [l.strip() for l in text.strip().split('\\n') if l.strip()]\n",
    "    return lines[-1] if lines else text.strip()\n",
    "\n",
    "def self_consistency_cot(problem, num_samples=5, model=\"gpt-4\"):\n",
    "    \"\"\"Generate multiple reasoning paths and select most consistent answer.\"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step by step.\"\n",
    "    \n",
    "    responses = []\n",
    "    answers = []\n",
    "    \n",
    "    print(f\"Generating {num_samples} reasoning paths...\\n\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        response = generate_response(prompt, model=model, temperature=0.7)\n",
    "        responses.append(response)\n",
    "        \n",
    "        answer = extract_final_answer(response)\n",
    "        answers.append(answer)\n",
    "        \n",
    "        print(f\"Path {i+1} answer: {answer}\")\n",
    "    \n",
    "    # Find most common answer\n",
    "    answer_counts = Counter(answers)\n",
    "    most_common_answer, count = answer_counts.most_common(1)[0]\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": most_common_answer,\n",
    "        \"confidence\": count / num_samples,\n",
    "        \"all_responses\": responses,\n",
    "        \"answer_distribution\": dict(answer_counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bda29c",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Test Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tricky problem where reasoning might vary\n",
    "tricky_problem = \"\"\"\n",
    "A farmer has 15 cows. All but 8 die. How many cows does the farmer have left?\n",
    "\"\"\"\n",
    "\n",
    "result = self_consistency_cot(tricky_problem, num_samples=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final Answer: {result['final_answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.0%}\")\n",
    "print(f\"Answer Distribution: {result['answer_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5625964",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Compare Accuracy\n",
    "\n",
    "Compare single-shot vs self-consistency on multiple problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a33dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problems with known answers\n",
    "test_problems = [\n",
    "    {\n",
    "        \"problem\": \"If you have 3 apples and get 4 more, then give away 2, how many do you have?\",\n",
    "        \"correct_answer\": \"5\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A book costs $12. If you have $50 and buy 3 books, how much money is left?\",\n",
    "        \"correct_answer\": \"14\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If there are 24 hours in a day, how many hours are in 3.5 days?\",\n",
    "        \"correct_answer\": \"84\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Comparing approaches...\\n\")\n",
    "\n",
    "for i, test in enumerate(test_problems, 1):\n",
    "    print(f\"\\nProblem {i}: {test['problem']}\")\n",
    "    print(f\"Correct answer: {test['correct_answer']}\")\n",
    "    \n",
    "    # Single shot\n",
    "    single_result = zero_shot_cot(test['problem'])\n",
    "    single_answer = extract_final_answer(single_result)\n",
    "    print(f\"Single-shot answer: {single_answer}\")\n",
    "    \n",
    "    # Self-consistency\n",
    "    sc_result = self_consistency_cot(test['problem'], num_samples=3)\n",
    "    print(f\"Self-consistency answer: {sc_result['final_answer']} (confidence: {sc_result['confidence']:.0%})\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839ba42",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Chaining\n",
    "\n",
    "Break complex tasks into manageable steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptChain:\n",
    "    \"\"\"Build and execute chains of prompts.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4\", temperature=0.3):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.steps = []\n",
    "    \n",
    "    def add_step(self, name, prompt_template, extract_fn=None):\n",
    "        \"\"\"Add a step to the chain.\"\"\"\n",
    "        self.steps.append({\n",
    "            \"name\": name,\n",
    "            \"template\": prompt_template,\n",
    "            \"extract_fn\": extract_fn or (lambda x: x)\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def execute(self, initial_input):\n",
    "        \"\"\"Execute the prompt chain.\"\"\"\n",
    "        current_input = initial_input\n",
    "        results = []\n",
    "        \n",
    "        for step in self.steps:\n",
    "            print(f\"\\nExecuting: {step['name']}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Format prompt\n",
    "            if isinstance(current_input, dict):\n",
    "                prompt = step[\"template\"].format(**current_input)\n",
    "            else:\n",
    "                prompt = step[\"template\"].format(input=current_input)\n",
    "            \n",
    "            # Generate response\n",
    "            output = generate_response(prompt, model=self.model, temperature=self.temperature)\n",
    "            \n",
    "            # Extract relevant data\n",
    "            extracted = step[\"extract_fn\"](output)\n",
    "            \n",
    "            results.append({\n",
    "                \"step_name\": step[\"name\"],\n",
    "                \"prompt\": prompt,\n",
    "                \"output\": output,\n",
    "                \"extracted\": extracted\n",
    "            })\n",
    "            \n",
    "            print(f\"Output:\\n{output}\\n\")\n",
    "            \n",
    "            current_input = extracted\n",
    "        \n",
    "        return {\n",
    "            \"final_output\": current_input,\n",
    "            \"steps\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5c38e",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Customer Feedback Analysis Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b706462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain for analyzing customer feedback\n",
    "feedback_chain = PromptChain()\n",
    "\n",
    "# Step 1: Extract key information\n",
    "feedback_chain.add_step(\n",
    "    \"Extract Information\",\n",
    "    \"\"\"\n",
    "Extract the following from this customer feedback:\n",
    "- Main issue or topic\n",
    "- Sentiment (positive/negative/neutral)\n",
    "- Urgency level (high/medium/low)\n",
    "- Key details\n",
    "\n",
    "Feedback: {input}\n",
    "\n",
    "Provide structured output.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Categorize and prioritize\n",
    "feedback_chain.add_step(\n",
    "    \"Categorize\",\n",
    "    \"\"\"\n",
    "Based on this analysis:\n",
    "\n",
    "{input}\n",
    "\n",
    "Categorize into: TECHNICAL, BILLING, PRODUCT, SERVICE, OTHER\n",
    "Assign priority: P1 (critical), P2 (high), P3 (medium), P4 (low)\n",
    "Suggest department: Support, Sales, Engineering, Finance\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Generate action items\n",
    "feedback_chain.add_step(\n",
    "    \"Action Items\",\n",
    "    \"\"\"\n",
    "Based on this categorization:\n",
    "\n",
    "{input}\n",
    "\n",
    "Generate 3 specific, actionable next steps for the team.\n",
    "Include estimated timeframe for each action.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "customer_feedback = \"\"\"\n",
    "I've been trying to access my account for 3 days now. The password reset\n",
    "email never arrives, and when I try to contact support, I just get a\n",
    "generic automated response. This is extremely frustrating as I need to\n",
    "download my invoice for accounting purposes. I'm considering switching\n",
    "to a competitor if this isn't resolved by end of week.\n",
    "\"\"\"\n",
    "\n",
    "result = feedback_chain.execute(customer_feedback)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL OUTPUT:\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"final_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461cbdcc",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Research Paper Analysis Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chain for research paper analysis\n",
    "paper_chain = PromptChain(temperature=0.4)\n",
    "\n",
    "paper_chain.add_step(\n",
    "    \"Extract Core Elements\",\n",
    "    \"\"\"\n",
    "Extract from this abstract:\n",
    "1. Research question/hypothesis\n",
    "2. Methodology\n",
    "3. Key findings (3-5 points)\n",
    "4. Stated limitations\n",
    "\n",
    "Abstract: {input}\n",
    "    \"\"\"\n",
    ").add_step(\n",
    "    \"Analyze Significance\",\n",
    "    \"\"\"\n",
    "Based on this paper summary:\n",
    "\n",
    "{input}\n",
    "\n",
    "Analyze:\n",
    "- Scientific significance (how does it advance the field?)\n",
    "- Practical applications (real-world use cases)\n",
    "- Limitations and concerns\n",
    "    \"\"\"\n",
    ").add_step(\n",
    "    \"Executive Summary\",\n",
    "    \"\"\"\n",
    "Create a 150-word executive summary for business leaders based on:\n",
    "\n",
    "{input}\n",
    "\n",
    "Focus on practical implications and business value. Use clear, non-technical language.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Test with a sample abstract\n",
    "abstract = \"\"\"\n",
    "This study examines the effectiveness of chain-of-thought prompting in\n",
    "improving large language model performance on complex reasoning tasks.\n",
    "Using a dataset of 1,000 multi-step problems across mathematics, logic,\n",
    "and common sense reasoning, we demonstrate that explicit step-by-step\n",
    "reasoning improves accuracy by 23% compared to direct prompting. We\n",
    "introduce self-consistency decoding, which samples multiple reasoning\n",
    "paths and selects the most consistent answer, further improving\n",
    "accuracy by 12%. However, these approaches increase computational\n",
    "costs by 3-5x due to longer prompts and multiple samples.\n",
    "\"\"\"\n",
    "\n",
    "result = paper_chain.execute(abstract)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXECUTIVE SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"final_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80093d",
   "metadata": {},
   "source": [
    "## Part 4: Challenge Exercises\n",
    "\n",
    "Apply what you've learned to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cb8ee",
   "metadata": {},
   "source": [
    "### Challenge 1: Build a Reasoning Evaluator\n",
    "\n",
    "Create a system that evaluates the quality of reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning(problem, reasoning):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of reasoning for a given problem.\n",
    "    \n",
    "    TODO: Implement this function to:\n",
    "    1. Check if each step follows logically from the previous\n",
    "    2. Verify calculations and facts\n",
    "    3. Identify any logical gaps or errors\n",
    "    4. Provide a quality score (1-10)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your evaluator\n",
    "test_problem = \"If 5 workers can build a wall in 10 days, how long would it take 10 workers?\"\n",
    "test_reasoning = zero_shot_cot(test_problem)\n",
    "\n",
    "# evaluation = evaluate_reasoning(test_problem, test_reasoning)\n",
    "# print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaadf4",
   "metadata": {},
   "source": [
    "### Challenge 2: Adaptive Reasoning Strategy\n",
    "\n",
    "Build a system that automatically chooses the best reasoning approach based on the problem type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37607ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_reasoning(problem):\n",
    "    \"\"\"\n",
    "    Automatically select and apply the best reasoning strategy.\n",
    "    \n",
    "    TODO: Implement to:\n",
    "    1. Analyze the problem type (math, logic, planning, etc.)\n",
    "    2. Choose appropriate technique (CoT, self-consistency, chain, etc.)\n",
    "    3. Apply the chosen technique\n",
    "    4. Return result with explanation of why that approach was chosen\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"What is 25% of 80?\",  # Simple math\n",
    "    \"Design a system to reduce office energy consumption.\",  # Complex planning\n",
    "    \"If all A are B, and all B are C, are all A also C?\",  # Logic\n",
    "]\n",
    "\n",
    "# for problem in test_cases:\n",
    "#     result = adaptive_reasoning(problem)\n",
    "#     print(f\"\\nProblem: {problem}\")\n",
    "#     print(f\"Strategy: {result['strategy']}\")\n",
    "#     print(f\"Answer: {result['answer']}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d482e6e",
   "metadata": {},
   "source": [
    "### Challenge 3: Multi-Step Problem Solver\n",
    "\n",
    "Create a sophisticated system that combines multiple techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9091f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedProblemSolver:\n",
    "    \"\"\"\n",
    "    Advanced problem solver combining multiple techniques.\n",
    "    \n",
    "    TODO: Implement to:\n",
    "    1. Break complex problems into sub-problems\n",
    "    2. Apply appropriate reasoning technique to each\n",
    "    3. Combine results\n",
    "    4. Verify final answer\n",
    "    5. Provide confidence score\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self, problem):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test with a complex problem\n",
    "complex_problem = \"\"\"\n",
    "A company needs to optimize its supply chain. They have 3 warehouses,\n",
    "10 retail locations, and transportation costs vary by distance and volume.\n",
    "Warehouse 1 has 1000 units, Warehouse 2 has 1500 units, Warehouse 3 has 2000 units.\n",
    "Each retail location needs 400 units.\n",
    "What's the optimal distribution strategy to minimize costs while meeting demand?\n",
    "\"\"\"\n",
    "\n",
    "# solver = AdvancedProblemSolver()\n",
    "# result = solver.solve(complex_problem)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44860006",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. **Zero-Shot CoT**: Simple \"Let's think step by step\" dramatically improves reasoning\n",
    "2. **Self-Consistency**: Multiple reasoning paths increase accuracy\n",
    "3. **Prompt Chaining**: Breaking complex tasks into steps improves results\n",
    "4. **Trade-offs**: Advanced techniques improve accuracy but increase costs\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Start with simple techniques and add complexity only if needed\n",
    "- Always verify reasoning steps for important tasks\n",
    "- Monitor token usage and costs\n",
    "- Test multiple approaches to find what works best\n",
    "- Document successful prompt patterns for reuse\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge exercises\n",
    "- Experiment with different problem types\n",
    "- Build your own reasoning toolkit\n",
    "- Move on to Lab 2: OpenAI API Deep Dive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
