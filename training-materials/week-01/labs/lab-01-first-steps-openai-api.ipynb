{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f232b4b5",
   "metadata": {},
   "source": [
    "# Lab 1: First Steps with OpenAI API\n",
    "\n",
    "**Week 1 - GenAI Introduction & Fundamentals**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Set up your OpenAI API environment\n",
    "- Make your first API calls\n",
    "- Understand API parameters (temperature, max_tokens, etc.)\n",
    "- Experiment with different models\n",
    "- Handle API responses and errors\n",
    "- Monitor token usage and costs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- OpenAI API key\n",
    "- Python 3.9+\n",
    "- Basic Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902d5d4",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb610cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fca205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")\n",
    "print(f\"✓ API Key configured: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae626e05",
   "metadata": {},
   "source": [
    "## Part 1: Your First API Call\n",
    "\n",
    "Let's make your first call to the OpenAI API using the Chat Completions endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! What can you help me with?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62765c3e",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's explore what the API returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the full response object\n",
    "print(\"Full Response Object:\")\n",
    "print(f\"ID: {response.id}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Created: {response.created}\")\n",
    "print(f\"\\nUsage:\")\n",
    "print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"\\nFinish reason: {response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b4292",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Make Your Own API Call\n",
    "\n",
    "Create a function that sends a message and returns the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Send a message to the OpenAI API and return the response.\n",
    "    \n",
    "    Args:\n",
    "        message: The user's message\n",
    "        model: The model to use (default: gpt-3.5-turbo)\n",
    "    \n",
    "    Returns:\n",
    "        The assistant's response text\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Hint: Use client.chat.completions.create()\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# result = chat(\"What is machine learning?\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171096ec",
   "metadata": {},
   "source": [
    "## Part 2: Understanding API Parameters\n",
    "\n",
    "The OpenAI API has several important parameters that control the model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec77ccf",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls randomness:\n",
    "- **0.0**: Deterministic, always picks the most likely token\n",
    "- **0.7**: Balanced creativity and consistency\n",
    "- **1.0+**: More random and creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_temperature(prompt, temperatures=[0.0, 0.5, 1.0]):\n",
    "    \"\"\"Test how temperature affects responses.\"\"\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTemperature: {temp}\")\n",
    "        print(\"-\"*80)\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"=\"*80)\n",
    "\n",
    "# Test with a creative prompt\n",
    "creative_prompt = \"Write the opening sentence of a science fiction story.\"\n",
    "test_temperature(creative_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b9033",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Find the Right Temperature\n",
    "\n",
    "For each task below, what temperature would you use?\n",
    "\n",
    "1. **Translation**: Translate \"Hello, how are you?\" to French\n",
    "2. **Creative Writing**: Write a poem about autumn\n",
    "3. **Factual Q&A**: What is the capital of France?\n",
    "4. **Brainstorming**: Give me 5 unique business ideas\n",
    "\n",
    "Test your hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different tasks with different temperatures\n",
    "\n",
    "tasks = {\n",
    "    \"translation\": {\n",
    "        \"prompt\": \"Translate to French: 'Hello, how are you?'\",\n",
    "        \"temperature\": 0.0  # TODO: Adjust this\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"prompt\": \"Write a haiku about technology.\",\n",
    "        \"temperature\": 0.0  # TODO: Adjust this\n",
    "    },\n",
    "    \"factual\": {\n",
    "        \"prompt\": \"What is the speed of light?\",\n",
    "        \"temperature\": 0.0  # TODO: Adjust this\n",
    "    },\n",
    "    \"brainstorm\": {\n",
    "        \"prompt\": \"List 3 creative uses for a paperclip.\",\n",
    "        \"temperature\": 0.0  # TODO: Adjust this\n",
    "    }\n",
    "}\n",
    "\n",
    "for task_name, task_config in tasks.items():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": task_config[\"prompt\"]}],\n",
    "        temperature=task_config[\"temperature\"],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{task_name.upper()} (temp={task_config['temperature']}):\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecfcfd",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Controls the maximum length of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_max_tokens(prompt, token_limits=[50, 100, 200]):\n",
    "    \"\"\"Test how max_tokens affects response length.\"\"\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    for max_tokens in token_limits:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        actual_tokens = response.usage.completion_tokens\n",
    "        \n",
    "        print(f\"\\nMax tokens: {max_tokens}, Actual: {actual_tokens}\")\n",
    "        print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "        print(f\"Response: {content}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "# Test\n",
    "test_max_tokens(\"Explain what machine learning is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd441e6",
   "metadata": {},
   "source": [
    "## Part 3: System Messages\n",
    "\n",
    "System messages set the behavior and persona of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_system(user_message, system_message):\n",
    "    \"\"\"Chat with a custom system message.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test different system messages\n",
    "user_msg = \"What is Python?\"\n",
    "\n",
    "# Professional assistant\n",
    "system_msg_1 = \"You are a professional technical writer. Provide clear, concise explanations.\"\n",
    "print(\"Professional Assistant:\")\n",
    "print(chat_with_system(user_msg, system_msg_1))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Fun assistant  \n",
    "system_msg_2 = \"You are a friendly teacher who explains things using fun analogies and examples.\"\n",
    "print(\"Fun Teacher:\")\n",
    "print(chat_with_system(user_msg, system_msg_2))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Expert assistant\n",
    "system_msg_3 = \"You are a senior software engineer with 20 years of experience.\"\n",
    "print(\"Expert Engineer:\")\n",
    "print(chat_with_system(user_msg, system_msg_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f771d",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Design System Messages\n",
    "\n",
    "Create system messages for these personas:\n",
    "\n",
    "1. **Customer Support Agent**: Helpful, polite, empathetic\n",
    "2. **Code Reviewer**: Critical, detail-oriented, constructive\n",
    "3. **Creative Writer**: Imaginative, eloquent, poetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8da536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define system messages\n",
    "customer_support_system = \"You are...\"\n",
    "code_reviewer_system = \"You are...\"\n",
    "creative_writer_system = \"You are...\"\n",
    "\n",
    "# Test them\n",
    "test_message = \"The application crashed when I clicked submit.\"\n",
    "\n",
    "print(\"Customer Support Response:\")\n",
    "# TODO: Call chat_with_system with customer_support_system\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Try with code review\n",
    "code_snippet = \"\"\"\n",
    "def calculate_total(items):\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total = total + item\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "print(\"Code Review:\")\n",
    "# TODO: Call chat_with_system with code_reviewer_system\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Try with creative writer\n",
    "topic = \"A rainy day\"\n",
    "print(\"Creative Writing:\")\n",
    "# TODO: Call chat_with_system with creative_writer_system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d0bd3",
   "metadata": {},
   "source": [
    "## Part 4: Token Counting and Cost Estimation\n",
    "\n",
    "Understanding token usage is crucial for managing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Count the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test token counting\n",
    "texts = [\n",
    "    \"Hello!\",\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"In the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = count_tokens(text)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Characters: {len(text)}\")\n",
    "    print(f\"Ratio: {len(text)/tokens:.2f} chars/token\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(prompt_tokens, completion_tokens, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Estimate the cost of an API call.\n",
    "    \n",
    "    Pricing (as of 2024):\n",
    "    - GPT-3.5-turbo: $0.0005/1K input, $0.0015/1K output\n",
    "    - GPT-4: $0.03/1K input, $0.06/1K output\n",
    "    \"\"\"\n",
    "    \n",
    "    pricing = {\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06}\n",
    "    }\n",
    "    \n",
    "    if model not in pricing:\n",
    "        return None\n",
    "    \n",
    "    input_cost = (prompt_tokens / 1000) * pricing[model][\"input\"]\n",
    "    output_cost = (completion_tokens / 1000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"total_tokens\": prompt_tokens + completion_tokens\n",
    "    }\n",
    "\n",
    "# Example cost calculation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a 200-word article about renewable energy.\"}\n",
    "    ],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "cost_info = estimate_cost(\n",
    "    response.usage.prompt_tokens,\n",
    "    response.usage.completion_tokens,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {cost_info['total_tokens']}\")\n",
    "print(f\"\\nCost Breakdown:\")\n",
    "print(f\"  Input cost: ${cost_info['input_cost']:.6f}\")\n",
    "print(f\"  Output cost: ${cost_info['output_cost']:.6f}\")\n",
    "print(f\"  Total cost: ${cost_info['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942f0a8",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Cost Comparison\n",
    "\n",
    "Compare the cost of using GPT-3.5-turbo vs GPT-4 for the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0db27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "for model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    cost_info = estimate_cost(\n",
    "        response.usage.prompt_tokens,\n",
    "        response.usage.completion_tokens,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Tokens: {cost_info['total_tokens']}\")\n",
    "    print(f\"  Cost: ${cost_info['total_cost']:.6f}\")\n",
    "    print(f\"  Response preview: {response.choices[0].message.content[:100]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b4909",
   "metadata": {},
   "source": [
    "## Part 5: Error Handling\n",
    "\n",
    "Production code needs robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a57848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAIError, RateLimitError, APIError\n",
    "import time\n",
    "\n",
    "def safe_chat(message, max_retries=3, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Make an API call with error handling and retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": response.usage\n",
    "            }\n",
    "            \n",
    "        except RateLimitError:\n",
    "            print(f\"Rate limit hit. Waiting before retry {attempt + 1}/{max_retries}...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            \n",
    "        except APIError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"success\": False, \"error\": str(e)}\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    return {\"success\": False, \"error\": \"Max retries exceeded\"}\n",
    "\n",
    "# Test the safe function\n",
    "result = safe_chat(\"What is the meaning of life?\")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"Response:\", result[\"content\"])\n",
    "    print(f\"Tokens used: {result['usage'].total_tokens}\")\n",
    "else:\n",
    "    print(\"Error:\", result[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256251e8",
   "metadata": {},
   "source": [
    "## Part 6: Multi-turn Conversations\n",
    "\n",
    "Build a simple conversational interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87af245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(system_message=\"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Simple conversation loop.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message}\n",
    "    ]\n",
    "    \n",
    "    print(\"Chatbot ready! Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Get response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add assistant message\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        print(f\"\\nAssistant: {assistant_message}\\n\")\n",
    "\n",
    "# Uncomment to run the conversation\n",
    "# conversation(\"You are a Python programming tutor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b279c93",
   "metadata": {},
   "source": [
    "## Challenge Exercises\n",
    "\n",
    "### Challenge 1: Smart Chatbot\n",
    "\n",
    "Create a chatbot that:\n",
    "1. Remembers conversation history\n",
    "2. Tracks token usage\n",
    "3. Warns when approaching token limits\n",
    "4. Automatically summarizes old messages to save tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot with conversation management.\n",
    "    \n",
    "    TODO: Implement the following methods:\n",
    "    - __init__: Initialize with system message and token limit\n",
    "    - add_message: Add a message to history\n",
    "    - get_response: Get response from API\n",
    "    - summarize_history: Summarize old messages when limit approached\n",
    "    - get_token_count: Count tokens in conversation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message, max_tokens=4000):\n",
    "        self.system_message = system_message\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # TODO: Implement remaining methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# bot = SmartChatbot(\"You are a helpful assistant.\")\n",
    "# response = bot.get_response(\"Hello!\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d906ae4",
   "metadata": {},
   "source": [
    "### Challenge 2: API Usage Logger\n",
    "\n",
    "Create a logger that tracks:\n",
    "- All API calls made\n",
    "- Tokens used per call\n",
    "- Cumulative costs\n",
    "- Average response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class APILogger:\n",
    "    \"\"\"\n",
    "    Log and analyze API usage.\n",
    "    \n",
    "    TODO: Implement methods to:\n",
    "    - Log each API call\n",
    "    - Calculate cumulative costs\n",
    "    - Generate usage reports\n",
    "    - Export to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calls = []\n",
    "    \n",
    "    def log_call(self, model, prompt_tokens, completion_tokens, duration):\n",
    "        \"\"\"Log an API call.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate usage report.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# logger = APILogger()\n",
    "# \n",
    "# start = time.time()\n",
    "# response = client.chat.completions.create(...)\n",
    "# duration = time.time() - start\n",
    "# \n",
    "# logger.log_call(\"gpt-3.5-turbo\", prompt_tokens, completion_tokens, duration)\n",
    "# print(logger.get_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d116a",
   "metadata": {},
   "source": [
    "### Challenge 3: Model Comparison Tool\n",
    "\n",
    "Build a tool that:\n",
    "1. Sends the same prompt to multiple models\n",
    "2. Compares responses\n",
    "3. Analyzes quality, speed, and cost\n",
    "4. Recommends the best model for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, models=[\"gpt-3.5-turbo\", \"gpt-4\"]):\n",
    "    \"\"\"\n",
    "    Compare different models for the same task.\n",
    "    \n",
    "    TODO: Implement to:\n",
    "    - Call each model with the same prompt\n",
    "    - Measure response time\n",
    "    - Calculate costs\n",
    "    - Compare response quality (length, coherence, etc.)\n",
    "    - Return comparison report\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # TODO: Implement comparison logic\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "# prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "# comparison = compare_models(prompt)\n",
    "# print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f2ee2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. ✅ How to make basic OpenAI API calls\n",
    "2. ✅ Understanding key parameters (temperature, max_tokens)\n",
    "3. ✅ Using system messages to control behavior\n",
    "4. ✅ Counting tokens and estimating costs\n",
    "5. ✅ Error handling and retries\n",
    "6. ✅ Building multi-turn conversations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Temperature**: Lower for factual, higher for creative tasks\n",
    "- **System Messages**: Define the assistant's behavior and persona\n",
    "- **Token Management**: Critical for cost control\n",
    "- **Error Handling**: Always implement retries and proper error handling\n",
    "- **Model Selection**: Choose based on task requirements and budget\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge exercises\n",
    "- Experiment with different parameters\n",
    "- Build your own chatbot with custom features\n",
    "- Move on to Lab 2: Text Generation Experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
