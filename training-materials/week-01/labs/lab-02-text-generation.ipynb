{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de750680",
   "metadata": {},
   "source": [
    "# Lab 2: Text Generation Experiments\n",
    "\n",
    "**Week 1 - GenAI Introduction & Fundamentals**\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand the difference between completion and chat models\n",
    "- Experiment with various text generation parameters\n",
    "- Build a simple chatbot with conversation memory\n",
    "- Implement token counting and cost tracking\n",
    "- Explore different text generation use cases\n",
    "- Create streaming responses for better UX\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 1\n",
    "- OpenAI API key configured\n",
    "- Python 3.9+\n",
    "- Basic understanding of API parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f52ef",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bab23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88001cfd",
   "metadata": {},
   "source": [
    "## Part 1: Completion vs Chat Models\n",
    "\n",
    "OpenAI offers two types of models:\n",
    "- **Completion models**: Continue text from a prompt (legacy)\n",
    "- **Chat models**: Conversational, with roles (system, user, assistant)\n",
    "\n",
    "Modern applications primarily use **chat models** (gpt-3.5-turbo, gpt-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98d558",
   "metadata": {},
   "source": [
    "### Chat Model Architecture\n",
    "\n",
    "Chat models use a message-based format with three roles:\n",
    "- **system**: Sets behavior and instructions\n",
    "- **user**: The human's input\n",
    "- **assistant**: The AI's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9368453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple chat interaction\n",
    "def simple_chat(user_message: str) -> str:\n",
    "    \"\"\"Basic chat completion.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test it\n",
    "result = simple_chat(\"What are the three laws of robotics?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc796e37",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Understanding Message Roles\n",
    "\n",
    "Create a chat function that demonstrates how different system messages change behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_persona(user_message: str, persona: str) -> str:\n",
    "    \"\"\"\n",
    "    Chat with different personas.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's question\n",
    "        persona: One of 'formal', 'casual', 'technical', 'creative'\n",
    "    \n",
    "    Returns:\n",
    "        The assistant's response\n",
    "    \"\"\"\n",
    "    \n",
    "    personas = {\n",
    "        'formal': \"You are a formal business consultant. Use professional language and structure.\",\n",
    "        'casual': \"You are a friendly buddy. Use casual language, emojis, and be conversational.\",\n",
    "        'technical': \"You are a senior engineer. Provide technical, detailed explanations with examples.\",\n",
    "        'creative': \"You are a creative writer. Use vivid imagery, metaphors, and engaging narratives.\"\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # Hint: Use the personas dictionary to get the system message\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test different personas\n",
    "question = \"Explain what cloud computing is.\"\n",
    "\n",
    "for persona in ['formal', 'casual', 'technical', 'creative']:\n",
    "    print(f\"\\n{persona.upper()} PERSONA:\")\n",
    "    print(\"-\" * 80)\n",
    "    # TODO: Call chat_with_persona and print the result\n",
    "    # response = chat_with_persona(question, persona)\n",
    "    # print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d703d",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Parameter Exploration\n",
    "\n",
    "Let's explore parameters beyond temperature and max_tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded526b",
   "metadata": {},
   "source": [
    "### Top P (Nucleus Sampling)\n",
    "\n",
    "An alternative to temperature that controls randomness by selecting from the top tokens that sum to probability P.\n",
    "\n",
    "- **top_p = 0.1**: Very focused, only top 10% probability tokens\n",
    "- **top_p = 0.9**: Balanced (default)\n",
    "- **top_p = 1.0**: Consider all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_top_p(prompt: str, top_p_values: List[float] = [0.1, 0.5, 0.9]):\n",
    "    \"\"\"Experiment with top_p sampling.\"\"\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for top_p in top_p_values:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=1.0,  # High temperature to see top_p effect\n",
    "            top_p=top_p,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTop P: {top_p}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Test with a creative prompt\n",
    "creative_prompt = \"Complete this story: 'The door creaked open, revealing...'\"\n",
    "explore_top_p(creative_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762e05d",
   "metadata": {},
   "source": [
    "### Frequency and Presence Penalties\n",
    "\n",
    "Control repetition in generated text:\n",
    "- **frequency_penalty** (-2.0 to 2.0): Penalize tokens based on how often they appear\n",
    "- **presence_penalty** (-2.0 to 2.0): Penalize tokens that have appeared at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_penalties(prompt: str):\n",
    "    \"\"\"Test frequency and presence penalties.\"\"\"\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"No penalties\", \"frequency\": 0.0, \"presence\": 0.0},\n",
    "        {\"name\": \"High frequency penalty\", \"frequency\": 2.0, \"presence\": 0.0},\n",
    "        {\"name\": \"High presence penalty\", \"frequency\": 0.0, \"presence\": 2.0},\n",
    "        {\"name\": \"Both high\", \"frequency\": 1.5, \"presence\": 1.5},\n",
    "    ]\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    for config in configs:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            frequency_penalty=config[\"frequency\"],\n",
    "            presence_penalty=config[\"presence\"],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{config['name']}\")\n",
    "        print(f\"(frequency={config['frequency']}, presence={config['presence']})\")\n",
    "        print(\"-\" * 80)\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Test with a prompt that tends to be repetitive\n",
    "repetitive_prompt = \"List 10 benefits of exercise.\"\n",
    "explore_penalties(repetitive_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715a461",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Parameter Tuning Challenge\n",
    "\n",
    "For each use case, determine the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d02e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cases and their optimal parameters\n",
    "\n",
    "use_cases = {\n",
    "    \"legal_document\": {\n",
    "        \"prompt\": \"Draft a privacy policy section about data collection.\",\n",
    "        \"temperature\": 0.0,  # TODO: Adjust\n",
    "        \"top_p\": 1.0,  # TODO: Adjust\n",
    "        \"frequency_penalty\": 0.0,  # TODO: Adjust\n",
    "        \"presence_penalty\": 0.0  # TODO: Adjust\n",
    "    },\n",
    "    \"creative_story\": {\n",
    "        \"prompt\": \"Write an opening paragraph for a mystery novel.\",\n",
    "        \"temperature\": 0.0,  # TODO: Adjust\n",
    "        \"top_p\": 1.0,  # TODO: Adjust\n",
    "        \"frequency_penalty\": 0.0,  # TODO: Adjust\n",
    "        \"presence_penalty\": 0.0  # TODO: Adjust\n",
    "    },\n",
    "    \"technical_docs\": {\n",
    "        \"prompt\": \"Explain how to implement binary search in Python.\",\n",
    "        \"temperature\": 0.0,  # TODO: Adjust\n",
    "        \"top_p\": 1.0,  # TODO: Adjust\n",
    "        \"frequency_penalty\": 0.0,  # TODO: Adjust\n",
    "        \"presence_penalty\": 0.0  # TODO: Adjust\n",
    "    },\n",
    "    \"marketing_copy\": {\n",
    "        \"prompt\": \"Write 5 taglines for an eco-friendly water bottle.\",\n",
    "        \"temperature\": 0.0,  # TODO: Adjust\n",
    "        \"top_p\": 1.0,  # TODO: Adjust\n",
    "        \"frequency_penalty\": 0.0,  # TODO: Adjust\n",
    "        \"presence_penalty\": 0.0  # TODO: Adjust\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test your parameter choices\n",
    "for use_case, config in use_cases.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"USE CASE: {use_case.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": config[\"prompt\"]}],\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config[\"top_p\"],\n",
    "        frequency_penalty=config[\"frequency_penalty\"],\n",
    "        presence_penalty=config[\"presence_penalty\"],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {config['prompt']}\")\n",
    "    print(f\"\\nParameters:\")\n",
    "    print(f\"  Temperature: {config['temperature']}\")\n",
    "    print(f\"  Top P: {config['top_p']}\")\n",
    "    print(f\"  Frequency Penalty: {config['frequency_penalty']}\")\n",
    "    print(f\"  Presence Penalty: {config['presence_penalty']}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa9d26",
   "metadata": {},
   "source": [
    "## Part 3: Building a Chatbot with Memory\n",
    "\n",
    "Create a conversational chatbot that remembers context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    A simple chatbot with conversation memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str = \"You are a helpful assistant.\", model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot.\n",
    "        \n",
    "        Args:\n",
    "            system_message: The system prompt defining behavior\n",
    "            model: The OpenAI model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message}\n",
    "        ]\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a message and get a response.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's message\n",
    "        \n",
    "        Returns:\n",
    "            The assistant's response\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Get response\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Extract assistant's message\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add to history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        # Track tokens\n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_conversation(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the full conversation history.\"\"\"\n",
    "        return self.messages[1:]  # Exclude system message\n",
    "    \n",
    "    def get_token_usage(self) -> int:\n",
    "        \"\"\"Get total tokens used.\"\"\"\n",
    "        return self.total_tokens\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history (keep system message).\"\"\"\n",
    "        self.messages = self.messages[:1]\n",
    "        self.total_tokens = 0\n",
    "\n",
    "# Test the chatbot\n",
    "bot = Chatbot(system_message=\"You are a friendly Python tutor.\")\n",
    "\n",
    "print(\"Chatbot initialized!\\n\")\n",
    "\n",
    "# Conversation 1\n",
    "print(\"User: What is a list in Python?\")\n",
    "response1 = bot.chat(\"What is a list in Python?\")\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "# Conversation 2 (should remember context)\n",
    "print(\"User: Can you give me an example?\")\n",
    "response2 = bot.chat(\"Can you give me an example?\")\n",
    "print(f\"Assistant: {response2}\\n\")\n",
    "\n",
    "# Conversation 3 (testing memory)\n",
    "print(\"User: What was my first question?\")\n",
    "response3 = bot.chat(\"What was my first question?\")\n",
    "print(f\"Assistant: {response3}\\n\")\n",
    "\n",
    "print(f\"Total tokens used: {bot.get_token_usage()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7caea",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Enhanced Chatbot\n",
    "\n",
    "Enhance the Chatbot class with:\n",
    "1. Token limit management\n",
    "2. Automatic summarization when approaching limits\n",
    "3. Export conversation to file\n",
    "4. Load conversation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02494610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedChatbot(Chatbot):\n",
    "    \"\"\"\n",
    "    Enhanced chatbot with token management and persistence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str = \"You are a helpful assistant.\", \n",
    "                 model: str = \"gpt-3.5-turbo\", max_tokens: int = 4000):\n",
    "        super().__init__(system_message, model)\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def get_current_tokens(self) -> int:\n",
    "        \"\"\"\n",
    "        Count tokens in current conversation.\n",
    "        \n",
    "        TODO: Implement using tiktoken\n",
    "        Hint: Count tokens in all messages\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def summarize_conversation(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarize the conversation to reduce token count.\n",
    "        \n",
    "        TODO: Implement\n",
    "        Hint: Ask the model to summarize previous messages\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_and_manage_tokens(self):\n",
    "        \"\"\"\n",
    "        Check token count and summarize if needed.\n",
    "        \n",
    "        TODO: Implement\n",
    "        Hint: If current_tokens > 80% of max_tokens, summarize\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save_conversation(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save conversation to JSON file.\n",
    "        \n",
    "        TODO: Implement\n",
    "        Hint: Use json.dump()\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load_conversation(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load conversation from JSON file.\n",
    "        \n",
    "        TODO: Implement\n",
    "        Hint: Use json.load()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# bot = EnhancedChatbot(max_tokens=500)\n",
    "# response = bot.chat(\"Tell me about Python.\")\n",
    "# print(f\"Current tokens: {bot.get_current_tokens()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5c140",
   "metadata": {},
   "source": [
    "## Part 4: Token Counting and Cost Tracking\n",
    "\n",
    "Implement comprehensive token counting and cost estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCounter:\n",
    "    \"\"\"\n",
    "    Utility class for token counting and cost estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a text string.\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def count_message_tokens(self, messages: List[Dict[str, str]]) -> int:\n",
    "        \"\"\"\n",
    "        Count tokens in a list of messages.\n",
    "        \n",
    "        Note: This is an approximation. The actual count may vary slightly.\n",
    "        \"\"\"\n",
    "        tokens = 0\n",
    "        \n",
    "        for message in messages:\n",
    "            # Every message has some overhead\n",
    "            tokens += 4  # Message formatting tokens\n",
    "            \n",
    "            for key, value in message.items():\n",
    "                tokens += self.count_tokens(value)\n",
    "        \n",
    "        tokens += 2  # Conversation priming tokens\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def estimate_cost(self, input_tokens: int, output_tokens: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Estimate cost based on token usage.\n",
    "        \n",
    "        Pricing (as of 2024):\n",
    "        - gpt-3.5-turbo: $0.0005/1K input, $0.0015/1K output\n",
    "        - gpt-4: $0.03/1K input, $0.06/1K output\n",
    "        \"\"\"\n",
    "        \n",
    "        pricing = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
    "        }\n",
    "        \n",
    "        if self.model not in pricing:\n",
    "            return {\"error\": f\"Pricing not available for {self.model}\"}\n",
    "        \n",
    "        input_cost = (input_tokens / 1000) * pricing[self.model][\"input\"]\n",
    "        output_cost = (output_tokens / 1000) * pricing[self.model][\"output\"]\n",
    "        \n",
    "        return {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": input_tokens + output_tokens,\n",
    "            \"input_cost\": input_cost,\n",
    "            \"output_cost\": output_cost,\n",
    "            \"total_cost\": input_cost + output_cost\n",
    "        }\n",
    "\n",
    "# Test token counter\n",
    "counter = TokenCounter(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Count tokens in a message\n",
    "text = \"Hello, how are you doing today?\"\n",
    "token_count = counter.count_tokens(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {token_count}\\n\")\n",
    "\n",
    "# Count tokens in messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}\n",
    "]\n",
    "\n",
    "message_tokens = counter.count_message_tokens(messages)\n",
    "print(f\"Message tokens: {message_tokens}\\n\")\n",
    "\n",
    "# Estimate cost\n",
    "cost_info = counter.estimate_cost(input_tokens=100, output_tokens=200)\n",
    "print(\"Cost estimation:\")\n",
    "for key, value in cost_info.items():\n",
    "    if 'cost' in key:\n",
    "        print(f\"  {key}: ${value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ead561",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Cost-Aware Chatbot\n",
    "\n",
    "Create a chatbot that tracks and displays costs in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96785fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostAwareChatbot:\n",
    "    \"\"\"\n",
    "    Chatbot with real-time cost tracking.\n",
    "    \n",
    "    TODO: Implement a chatbot that:\n",
    "    1. Tracks tokens and costs for each message\n",
    "    2. Shows cumulative costs\n",
    "    3. Warns when costs exceed a threshold\n",
    "    4. Generates cost reports\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str, model: str = \"gpt-3.5-turbo\", \n",
    "                 cost_threshold: float = 0.10):\n",
    "        self.model = model\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "        self.cost_threshold = cost_threshold\n",
    "        self.total_cost = 0.0\n",
    "        self.conversation_history = []  # Track each exchange\n",
    "        self.counter = TokenCounter(model)\n",
    "    \n",
    "    # TODO: Implement methods:\n",
    "    # - chat(user_message) -> response with cost info\n",
    "    # - get_total_cost() -> total cost so far\n",
    "    # - get_cost_report() -> detailed cost breakdown\n",
    "    # - check_threshold() -> warn if threshold exceeded\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# bot = CostAwareChatbot(\"You are a helpful assistant.\", cost_threshold=0.01)\n",
    "# response = bot.chat(\"Explain quantum computing.\")\n",
    "# print(f\"Total cost so far: ${bot.get_total_cost():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c754c42",
   "metadata": {},
   "source": [
    "## Part 5: Streaming Responses\n",
    "\n",
    "Implement streaming for better user experience with long responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(user_message: str, system_message: str = \"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Stream a chat response token by token.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "    \n",
    "    print(\"\\n\")  # New line at the end\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Test streaming\n",
    "print(\"User: Write a short poem about coding.\\n\")\n",
    "response = stream_chat(\"Write a short poem about coding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatbot:\n",
    "    \"\"\"\n",
    "    Chatbot with streaming support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str = \"You are a helpful assistant.\", model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    def chat_stream(self, user_message: str):\n",
    "        \"\"\"\n",
    "        Stream a response and yield chunks.\n",
    "        \"\"\"\n",
    "        # Add user message\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Create streaming response\n",
    "        stream = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                yield content\n",
    "        \n",
    "        # Add complete response to history\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "# Test streaming chatbot\n",
    "bot = StreamingChatbot(\"You are a creative storyteller.\")\n",
    "\n",
    "print(\"User: Tell me a very short story about a robot.\\n\")\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "for chunk in bot.chat_stream(\"Tell me a very short story about a robot.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e012db",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Streaming with Indicators\n",
    "\n",
    "Enhance the streaming chatbot to show:\n",
    "1. Typing indicator while waiting\n",
    "2. Token count updates in real-time\n",
    "3. Estimated cost as response generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class EnhancedStreamingChatbot(StreamingChatbot):\n",
    "    \"\"\"\n",
    "    Streaming chatbot with visual indicators.\n",
    "    \n",
    "    TODO: Implement enhanced streaming features:\n",
    "    1. Show \"Thinking...\" before response starts\n",
    "    2. Display token count as response generates\n",
    "    3. Show estimated cost in real-time\n",
    "    4. Add progress indicator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str = \"You are a helpful assistant.\", \n",
    "                 model: str = \"gpt-3.5-turbo\"):\n",
    "        super().__init__(system_message, model)\n",
    "        self.counter = TokenCounter(model)\n",
    "    \n",
    "    # TODO: Implement enhanced streaming methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# bot = EnhancedStreamingChatbot()\n",
    "# bot.chat_stream_with_indicators(\"Explain how neural networks work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed286807",
   "metadata": {},
   "source": [
    "## Part 6: Text Generation Use Cases\n",
    "\n",
    "Explore different text generation applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a73162",
   "metadata": {},
   "source": [
    "### Use Case 1: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ce189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, style: str = \"concise\") -> str:\n",
    "    \"\"\"\n",
    "    Summarize a text with different styles.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        style: 'concise', 'bullet', 'executive', 'eli5'\n",
    "    \"\"\"\n",
    "    \n",
    "    styles = {\n",
    "        'concise': \"Summarize the following text in 2-3 sentences.\",\n",
    "        'bullet': \"Summarize the following text as bullet points highlighting key information.\",\n",
    "        'executive': \"Provide an executive summary suitable for a business audience.\",\n",
    "        'eli5': \"Explain the following text like I'm 5 years old.\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"{styles.get(style, styles['concise'])}\\n\\nText: {text}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test summarization\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming the modern world in unprecedented ways. \n",
    "From healthcare to finance, transportation to entertainment, AI systems are being \n",
    "deployed to solve complex problems and enhance human capabilities. Machine learning, \n",
    "a subset of AI, enables computers to learn from data without explicit programming. \n",
    "Deep learning, which uses neural networks with multiple layers, has achieved \n",
    "remarkable results in image recognition, natural language processing, and game playing. \n",
    "However, the rapid advancement of AI also raises important ethical questions about \n",
    "privacy, bias, job displacement, and the future of human-AI interaction. As we continue \n",
    "to develop more sophisticated AI systems, it's crucial to ensure they are designed \n",
    "with human values in mind and remain beneficial to society.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text length:\", len(long_text), \"characters\\n\")\n",
    "\n",
    "for style in ['concise', 'bullet', 'executive', 'eli5']:\n",
    "    print(f\"\\n{style.upper()} SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    summary = summarize_text(long_text, style)\n",
    "    print(summary)\n",
    "    print(f\"Summary length: {len(summary)} characters\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce0a94",
   "metadata": {},
   "source": [
    "### Use Case 2: Content Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(content_type: str, topic: str, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Generate different types of content.\n",
    "    \n",
    "    Args:\n",
    "        content_type: 'blog', 'email', 'social', 'ad_copy'\n",
    "        topic: The subject matter\n",
    "        **kwargs: Additional parameters (tone, length, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        'blog': f\"Write a {kwargs.get('length', 'short')} blog post about {topic}. Tone: {kwargs.get('tone', 'professional')}.\",\n",
    "        'email': f\"Write a {kwargs.get('tone', 'professional')} email about {topic}.\",\n",
    "        'social': f\"Write a {kwargs.get('platform', 'Twitter')} post about {topic}. Keep it engaging and concise.\",\n",
    "        'ad_copy': f\"Write compelling ad copy for {topic}. Focus on benefits and include a call-to-action.\"\n",
    "    }\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompts[content_type]}],\n",
    "        temperature=0.8,\n",
    "        max_tokens=kwargs.get('max_tokens', 300)\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test content generation\n",
    "print(\"BLOG POST:\")\n",
    "print(\"=\" * 80)\n",
    "blog = generate_content('blog', 'benefits of remote work', length='short', tone='professional')\n",
    "print(blog)\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"EMAIL:\")\n",
    "print(\"=\" * 80)\n",
    "email = generate_content('email', 'quarterly meeting invitation', tone='friendly')\n",
    "print(email)\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"SOCIAL MEDIA POST:\")\n",
    "print(\"=\" * 80)\n",
    "social = generate_content('social', 'new product launch', platform='LinkedIn')\n",
    "print(social)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f8cfc",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Multi-Purpose Text Generator\n",
    "\n",
    "Create a comprehensive text generation tool that can:\n",
    "1. Generate content for multiple use cases\n",
    "2. Apply different tones and styles\n",
    "3. Track token usage and costs\n",
    "4. Save generated content to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a07f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    \"\"\"\n",
    "    Multi-purpose text generation tool.\n",
    "    \n",
    "    TODO: Implement a text generator with:\n",
    "    1. Multiple content types (blog, email, social, docs, etc.)\n",
    "    2. Customizable parameters (tone, length, style)\n",
    "    3. Cost tracking\n",
    "    4. Export functionality\n",
    "    5. Batch generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.counter = TokenCounter(model)\n",
    "        self.generation_history = []\n",
    "    \n",
    "    # TODO: Implement methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# generator = TextGenerator()\n",
    "# content = generator.generate('blog', 'AI in healthcare', tone='professional', length='medium')\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8fca7",
   "metadata": {},
   "source": [
    "## Challenge Projects\n",
    "\n",
    "### Challenge 1: Interactive Story Generator\n",
    "\n",
    "Build an interactive story generator where:\n",
    "- User makes choices that affect the story\n",
    "- AI generates story segments based on choices\n",
    "- Track multiple story branches\n",
    "- Save/load story progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveStory:\n",
    "    \"\"\"\n",
    "    Interactive story generator with branching narratives.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Story initialization with setting/genre\n",
    "    2. Generate story segments\n",
    "    3. Present choices to the user\n",
    "    4. Branch based on user decisions\n",
    "    5. Track story state and history\n",
    "    6. Save/load functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genre: str, setting: str):\n",
    "        self.genre = genre\n",
    "        self.setting = setting\n",
    "        self.story_segments = []\n",
    "        self.choices_made = []\n",
    "    \n",
    "    def start_story(self):\n",
    "        \"\"\"Generate opening segment.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_choices(self, current_segment: str) -> List[str]:\n",
    "        \"\"\"Generate 2-3 choices for the user.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def continue_story(self, choice: str):\n",
    "        \"\"\"Generate next segment based on choice.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_full_story(self) -> str:\n",
    "        \"\"\"Compile all segments into full story.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "# story = InteractiveStory(genre=\"sci-fi\", setting=\"space station\")\n",
    "# story.start_story()\n",
    "# while not story.is_complete():\n",
    "#     choices = story.generate_choices(story.current_segment)\n",
    "#     user_choice = input(f\"Choose {choices}: \")\n",
    "#     story.continue_story(user_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fe628",
   "metadata": {},
   "source": [
    "### Challenge 2: Multi-Language Translation Service\n",
    "\n",
    "Create a translation service that:\n",
    "- Translates text to multiple languages\n",
    "- Maintains context and tone\n",
    "- Handles idioms and cultural nuances\n",
    "- Provides back-translation verification\n",
    "- Estimates costs for batch translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationService:\n",
    "    \"\"\"\n",
    "    Multi-language translation service.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Translate text to specified language\n",
    "    2. Batch translation\n",
    "    3. Context preservation\n",
    "    4. Back-translation for verification\n",
    "    5. Cost estimation\n",
    "    6. Quality scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.supported_languages = [\n",
    "            'Spanish', 'French', 'German', 'Italian', 'Portuguese',\n",
    "            'Chinese', 'Japanese', 'Korean', 'Arabic', 'Russian'\n",
    "        ]\n",
    "    \n",
    "    # TODO: Implement translation methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# translator = TranslationService()\n",
    "# result = translator.translate(\"Hello, how are you?\", target_language=\"Spanish\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1861a12",
   "metadata": {},
   "source": [
    "### Challenge 3: Content Repurposing Tool\n",
    "\n",
    "Build a tool that repurposes content across different formats:\n",
    "- Blog post â†’ Social media posts\n",
    "- Article â†’ Email newsletter\n",
    "- Long-form â†’ Multiple short-form pieces\n",
    "- Technical docs â†’ User-friendly guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b23e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentRepurposer:\n",
    "    \"\"\"\n",
    "    Repurpose content across different formats and platforms.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Analyze input content\n",
    "    2. Convert to different formats\n",
    "    3. Optimize for each platform\n",
    "    4. Maintain key messages\n",
    "    5. Generate multiple variations\n",
    "    6. Track conversion quality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.formats = {\n",
    "            'twitter': {'max_length': 280, 'style': 'concise'},\n",
    "            'linkedin': {'max_length': 1300, 'style': 'professional'},\n",
    "            'email': {'max_length': 500, 'style': 'conversational'},\n",
    "            'instagram': {'max_length': 2200, 'style': 'visual'}\n",
    "        }\n",
    "    \n",
    "    # TODO: Implement repurposing methods\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Usage example:\n",
    "# repurposer = ContentRepurposer()\n",
    "# original = \"Long blog post content...\"\n",
    "# social_posts = repurposer.convert_to_social(original, platforms=['twitter', 'linkedin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2327bc3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned:\n",
    "\n",
    "1. âœ… Differences between completion and chat models\n",
    "2. âœ… Advanced parameter tuning (temperature, top_p, penalties)\n",
    "3. âœ… Building chatbots with conversation memory\n",
    "4. âœ… Token counting and cost estimation\n",
    "5. âœ… Implementing streaming responses\n",
    "6. âœ… Various text generation use cases\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Chat models** are the modern standard for most applications\n",
    "- **Parameters matter**: Different tasks require different settings\n",
    "- **Memory management**: Critical for maintaining context in conversations\n",
    "- **Cost awareness**: Always track token usage and costs\n",
    "- **Streaming**: Improves UX for long-form generation\n",
    "- **Use case specific**: Tailor prompts and parameters to your application\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with low temperature** for consistent results, increase for creativity\n",
    "2. **Use presence_penalty** to encourage topic diversity\n",
    "3. **Implement token limits** to prevent runaway costs\n",
    "4. **Stream responses** for better user experience\n",
    "5. **Track costs** in production applications\n",
    "6. **Test parameters** systematically for each use case\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Complete the challenge projects\n",
    "- Build your own text generation application\n",
    "- Experiment with different model combinations\n",
    "- Move on to Lab 3: Building a Simple AI Application\n",
    "\n",
    "**Provided by:** ADC ENGINEERING & CONSULTING LTD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
