{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f45850",
   "metadata": {},
   "source": [
    "# Week 5 - Lab 2: Index Tuning and Recall Testing\n",
    "\n",
    "**Duration:** 90-120 minutes  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Week 5 Lessons 2-3, Lab 1\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand HNSW index parameters (M, ef_construction, ef_search)\n",
    "- Implement recall@k measurement with ground truth\n",
    "- Benchmark latency (p50, p95, p99) for different configurations\n",
    "- Tune index parameters for quality/speed trade-offs\n",
    "- Measure memory footprint and compression impact\n",
    "- Compare HNSW vs IVF-based indexes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Lab Outline\n",
    "\n",
    "1. Setup and Data Generation\n",
    "2. Exercise 1: Build FAISS HNSW Index\n",
    "3. Exercise 2: Measure Recall@k with Ground Truth\n",
    "4. Exercise 3: Latency Benchmarking\n",
    "5. Exercise 4: HNSW Parameter Sweep (ef_search)\n",
    "6. Exercise 5: IVF Index Comparison\n",
    "7. Exercise 6: Product Quantization (PQ) for Compression\n",
    "8. Bonus Challenge: Multi-dimensional Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85daceea",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai faiss-cpu numpy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214a12a",
   "metadata": {},
   "source": [
    "### Generate Synthetic Corpus\n",
    "\n",
    "We'll create a larger corpus (1000 documents) with known structure for ground truth testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fad64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic documents with categories\n",
    "CATEGORIES = {\n",
    "    \"architecture\": [\n",
    "        \"microservices design patterns and best practices\",\n",
    "        \"event-driven architecture with message queues\",\n",
    "        \"API gateway design and implementation strategies\",\n",
    "        \"service mesh and network policies\",\n",
    "        \"distributed systems and consistency models\",\n",
    "    ],\n",
    "    \"database\": [\n",
    "        \"SQL query optimization and indexing strategies\",\n",
    "        \"NoSQL databases comparison and use cases\",\n",
    "        \"database sharding and replication techniques\",\n",
    "        \"ACID properties and transaction management\",\n",
    "        \"vector databases for semantic search applications\",\n",
    "    ],\n",
    "    \"ml\": [\n",
    "        \"machine learning model training and evaluation\",\n",
    "        \"deep learning architectures and neural networks\",\n",
    "        \"natural language processing with transformers\",\n",
    "        \"computer vision and convolutional networks\",\n",
    "        \"reinforcement learning algorithms and applications\",\n",
    "    ],\n",
    "    \"devops\": [\n",
    "        \"kubernetes cluster management and orchestration\",\n",
    "        \"CI/CD pipeline design and automation\",\n",
    "        \"infrastructure as code with Terraform\",\n",
    "        \"monitoring and observability with Prometheus\",\n",
    "        \"container security and best practices\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def generate_corpus(n_docs: int = 1000) -> List[Dict]:\n",
    "    \"\"\"Generate synthetic corpus with categories.\"\"\"\n",
    "    corpus = []\n",
    "    categories = list(CATEGORIES.keys())\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        cat = categories[i % len(categories)]\n",
    "        templates = CATEGORIES[cat]\n",
    "        template = templates[i % len(templates)]\n",
    "        \n",
    "        # Add variation\n",
    "        text = f\"{template} - document {i} variation {i % 10}\"\n",
    "        \n",
    "        corpus.append({\n",
    "            \"id\": f\"doc_{i}\",\n",
    "            \"text\": text,\n",
    "            \"category\": cat,\n",
    "        })\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "CORPUS = generate_corpus(1000)\n",
    "print(f\"Generated {len(CORPUS)} documents across {len(CATEGORIES)} categories\")\n",
    "print(f\"Sample: {CORPUS[0]['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-small\") -> np.ndarray:\n",
    "    \"\"\"Get embeddings for texts in batch.\"\"\"\n",
    "    cleaned = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "    response = client.embeddings.create(input=cleaned, model=model)\n",
    "    embeddings = [item.embedding for item in response.data]\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Generate embeddings for corpus (batched)\n",
    "print(\"Generating embeddings for corpus (this may take 30-60 seconds)...\")\n",
    "batch_size = 100\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(CORPUS), batch_size):\n",
    "    batch = CORPUS[i:i+batch_size]\n",
    "    texts = [doc[\"text\"] for doc in batch]\n",
    "    embs = get_embeddings_batch(texts)\n",
    "    all_embeddings.append(embs)\n",
    "    print(f\"  Processed {min(i+batch_size, len(CORPUS))}/{len(CORPUS)} documents\")\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "corpus_embeddings = np.vstack(all_embeddings)\n",
    "print(f\"âœ… Generated embeddings: {corpus_embeddings.shape}\")\n",
    "print(f\"Memory: {corpus_embeddings.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec22f3",
   "metadata": {},
   "source": [
    "### Generate Test Queries with Ground Truth\n",
    "\n",
    "We create queries that should retrieve documents from specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test queries with known relevant documents\n",
    "TEST_QUERIES = [\n",
    "    {\n",
    "        \"text\": \"microservices architecture patterns\",\n",
    "        \"category\": \"architecture\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"SQL database optimization techniques\",\n",
    "        \"category\": \"database\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"machine learning neural networks\",\n",
    "        \"category\": \"ml\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"kubernetes container orchestration\",\n",
    "        \"category\": \"devops\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"vector database semantic search\",\n",
    "        \"category\": \"database\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Build ground truth: documents in same category are relevant\n",
    "def build_ground_truth(queries: List[Dict], corpus: List[Dict]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Build ground truth mappings from queries to relevant doc IDs.\"\"\"\n",
    "    ground_truth = {}\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        query_id = f\"q_{i}\"\n",
    "        target_cat = query[\"category\"]\n",
    "        \n",
    "        # All docs in same category are relevant\n",
    "        relevant = {doc[\"id\"] for doc in corpus if doc[\"category\"] == target_cat}\n",
    "        ground_truth[query_id] = relevant\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "GROUND_TRUTH = build_ground_truth(TEST_QUERIES, CORPUS)\n",
    "\n",
    "print(f\"Created {len(TEST_QUERIES)} test queries\")\n",
    "print(f\"Sample ground truth for q_0: {len(GROUND_TRUTH['q_0'])} relevant docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac0cc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Build FAISS HNSW Index\n",
    "\n",
    "**Task:** Build an HNSW index with FAISS and understand key parameters.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `M`: Number of connections per node (typical: 16-48)\n",
    "- `ef_construction`: Search width during build (typical: 100-400)\n",
    "- `ef_search`: Search width during query (typical: 50-400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b89356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hnsw_index(\n",
    "    embeddings: np.ndarray,\n",
    "    M: int = 32,\n",
    "    ef_construction: int = 200,\n",
    ") -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Build FAISS HNSW index.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding matrix (n_docs, dim)\n",
    "        M: Number of connections per node\n",
    "        ef_construction: Search width during construction\n",
    "    \"\"\"\n",
    "    # TODO: Implement HNSW index construction\n",
    "    # 1. Get dimensionality\n",
    "    # 2. Create IndexHNSWFlat\n",
    "    # 3. Set ef_construction (hnsw.efConstruction)\n",
    "    # 4. Add embeddings\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Create HNSW index\n",
    "    index = faiss.IndexHNSWFlat(dim, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    \n",
    "    # Add vectors\n",
    "    print(f\"Building index with M={M}, ef_construction={ef_construction}...\")\n",
    "    start = time.time()\n",
    "    index.add(embeddings)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"âœ… Index built in {elapsed:.2f}s\")\n",
    "    print(f\"   Total vectors: {index.ntotal}\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "# Build baseline index\n",
    "index_baseline = build_hnsw_index(corpus_embeddings, M=32, ef_construction=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501d322",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Measure Recall@k with Ground Truth\n",
    "\n",
    "**Task:** Implement recall@k measurement against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:\n",
    "    \"\"\"Calculate recall@k.\"\"\"\n",
    "    # TODO: Implement recall@k\n",
    "    # recall@k = |retrieved[:k] âˆ© relevant| / min(k, |relevant|)\n",
    "    \n",
    "    topk = set(retrieved[:k])\n",
    "    hits = len(topk & relevant)\n",
    "    denominator = min(k, len(relevant))\n",
    "    \n",
    "    return hits / max(1, denominator)\n",
    "\n",
    "\n",
    "def search_index(\n",
    "    index: faiss.Index,\n",
    "    query_emb: np.ndarray,\n",
    "    k: int = 10,\n",
    "    ef_search: int = None,\n",
    ") -> Tuple[List[int], List[float], float]:\n",
    "    \"\"\"\n",
    "    Search HNSW index.\n",
    "    \n",
    "    Returns:\n",
    "        (indices, distances, latency_ms)\n",
    "    \"\"\"\n",
    "    # TODO: Set ef_search if provided (for HNSW indexes)\n",
    "    if ef_search is not None and hasattr(index, 'hnsw'):\n",
    "        index.hnsw.efSearch = ef_search\n",
    "    \n",
    "    # Search with timing\n",
    "    start = time.perf_counter()\n",
    "    distances, indices = index.search(query_emb.reshape(1, -1), k)\n",
    "    latency_ms = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    return indices[0].tolist(), distances[0].tolist(), latency_ms\n",
    "\n",
    "\n",
    "def evaluate_index(\n",
    "    index: faiss.Index,\n",
    "    queries: List[Dict],\n",
    "    ground_truth: Dict[str, Set[str]],\n",
    "    k: int = 10,\n",
    "    ef_search: int = None,\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate index on test queries.\"\"\"\n",
    "    recalls = []\n",
    "    latencies = []\n",
    "    \n",
    "    # Generate query embeddings\n",
    "    query_texts = [q[\"text\"] for q in queries]\n",
    "    query_embs = get_embeddings_batch(query_texts)\n",
    "    \n",
    "    for i, query_emb in enumerate(query_embs):\n",
    "        query_id = f\"q_{i}\"\n",
    "        relevant = ground_truth[query_id]\n",
    "        \n",
    "        # Search\n",
    "        indices, _, latency = search_index(index, query_emb, k=k, ef_search=ef_search)\n",
    "        \n",
    "        # Convert indices to doc IDs\n",
    "        retrieved_ids = [CORPUS[idx][\"id\"] for idx in indices]\n",
    "        \n",
    "        # Calculate recall\n",
    "        recall = recall_at_k(retrieved_ids, relevant, k)\n",
    "        recalls.append(recall)\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    return {\n",
    "        \"recall@k\": np.mean(recalls),\n",
    "        \"latency_p50_ms\": np.percentile(latencies, 50),\n",
    "        \"latency_p95_ms\": np.percentile(latencies, 95),\n",
    "        \"latency_p99_ms\": np.percentile(latencies, 99),\n",
    "        \"recalls\": recalls,\n",
    "        \"latencies\": latencies,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate baseline\n",
    "results_baseline = evaluate_index(index_baseline, TEST_QUERIES, GROUND_TRUTH, k=10, ef_search=200)\n",
    "\n",
    "print(\"Baseline HNSW (M=32, ef_construction=200, ef_search=200):\")\n",
    "print(f\"  Recall@10: {results_baseline['recall@k']:.3f}\")\n",
    "print(f\"  Latency p50: {results_baseline['latency_p50_ms']:.2f}ms\")\n",
    "print(f\"  Latency p95: {results_baseline['latency_p95_ms']:.2f}ms\")\n",
    "print(f\"  Latency p99: {results_baseline['latency_p99_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a88cd06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Latency Benchmarking\n",
    "\n",
    "**Task:** Run multiple queries and measure latency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(\n",
    "    index: faiss.Index,\n",
    "    query_emb: np.ndarray,\n",
    "    k: int = 10,\n",
    "    ef_search: int = None,\n",
    "    n_runs: int = 100,\n",
    ") -> Dict:\n",
    "    \"\"\"Benchmark search latency with multiple runs.\"\"\"\n",
    "    # TODO: Implement latency benchmarking\n",
    "    # 1. Run search n_runs times\n",
    "    # 2. Collect latencies\n",
    "    # 3. Calculate percentiles\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        _, _, latency = search_index(index, query_emb, k=k, ef_search=ef_search)\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    return {\n",
    "        \"mean_ms\": np.mean(latencies),\n",
    "        \"std_ms\": np.std(latencies),\n",
    "        \"p50_ms\": np.percentile(latencies, 50),\n",
    "        \"p95_ms\": np.percentile(latencies, 95),\n",
    "        \"p99_ms\": np.percentile(latencies, 99),\n",
    "        \"min_ms\": np.min(latencies),\n",
    "        \"max_ms\": np.max(latencies),\n",
    "    }\n",
    "\n",
    "\n",
    "# Benchmark with first query\n",
    "query_emb = get_embeddings_batch([TEST_QUERIES[0][\"text\"]])[0]\n",
    "bench_results = benchmark_latency(index_baseline, query_emb, k=10, ef_search=200, n_runs=100)\n",
    "\n",
    "print(\"Latency Benchmark (100 runs):\")\n",
    "print(f\"  Mean: {bench_results['mean_ms']:.2f}ms Â± {bench_results['std_ms']:.2f}ms\")\n",
    "print(f\"  p50:  {bench_results['p50_ms']:.2f}ms\")\n",
    "print(f\"  p95:  {bench_results['p95_ms']:.2f}ms\")\n",
    "print(f\"  p99:  {bench_results['p99_ms']:.2f}ms\")\n",
    "print(f\"  Min:  {bench_results['min_ms']:.2f}ms\")\n",
    "print(f\"  Max:  {bench_results['max_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca26a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: HNSW Parameter Sweep (ef_search)\n",
    "\n",
    "**Task:** Sweep ef_search parameter and plot recall vs latency trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep ef_search values and measure recall + latency\n",
    "ef_search_values = [10, 20, 50, 100, 200, 400, 800]\n",
    "sweep_results = []\n",
    "\n",
    "print(\"Sweeping ef_search parameter...\\n\")\n",
    "\n",
    "for ef_search in ef_search_values:\n",
    "    print(f\"Testing ef_search={ef_search}...\")\n",
    "    \n",
    "    results = evaluate_index(\n",
    "        index_baseline,\n",
    "        TEST_QUERIES,\n",
    "        GROUND_TRUTH,\n",
    "        k=10,\n",
    "        ef_search=ef_search\n",
    "    )\n",
    "    \n",
    "    sweep_results.append({\n",
    "        \"ef_search\": ef_search,\n",
    "        \"recall@10\": results[\"recall@k\"],\n",
    "        \"latency_p50_ms\": results[\"latency_p50_ms\"],\n",
    "        \"latency_p95_ms\": results[\"latency_p95_ms\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"  Recall@10: {results['recall@k']:.3f}\")\n",
    "    print(f\"  Latency p95: {results['latency_p95_ms']:.2f}ms\\n\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n=== ef_search Parameter Sweep Results ===\")\n",
    "print(\"ef_search | Recall@10 | p50 (ms) | p95 (ms)\")\n",
    "print(\"----------|-----------|----------|----------\")\n",
    "for r in sweep_results:\n",
    "    print(f\"{r['ef_search']:9d} | {r['recall@10']:9.3f} | {r['latency_p50_ms']:8.2f} | {r['latency_p95_ms']:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116ca9a",
   "metadata": {},
   "source": [
    "### Analysis: Recall vs Latency Trade-off\n",
    "\n",
    "**Observations:**\n",
    "- Lower `ef_search` â†’ faster queries, lower recall\n",
    "- Higher `ef_search` â†’ slower queries, higher recall\n",
    "- Diminishing returns: recall plateaus at high `ef_search`\n",
    "\n",
    "**Production Recommendation:**\n",
    "- Find the \"knee\" of the curve where recall improvement plateaus\n",
    "- Balance with latency SLO (e.g., p95 < 100ms)\n",
    "- Typical sweet spot: ef_search = 100-200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385f54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: IVF Index Comparison\n",
    "\n",
    "**Task:** Build IVF (Inverted File) index and compare with HNSW.\n",
    "\n",
    "**IVF Parameters:**\n",
    "- `nlist`: Number of clusters (typical: sqrt(n_docs))\n",
    "- `nprobe`: Number of clusters to search (typical: 1-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0805fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ivf_index(\n",
    "    embeddings: np.ndarray,\n",
    "    nlist: int = 100,\n",
    ") -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Build FAISS IVF index.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding matrix\n",
    "        nlist: Number of Voronoi cells\n",
    "    \"\"\"\n",
    "    # TODO: Implement IVF index construction\n",
    "    # 1. Create quantizer (IndexFlatL2)\n",
    "    # 2. Create IndexIVFFlat\n",
    "    # 3. Train on embeddings\n",
    "    # 4. Add embeddings\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Create quantizer and IVF index\n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "    \n",
    "    # Train and add\n",
    "    print(f\"Training IVF index with nlist={nlist}...\")\n",
    "    start = time.time()\n",
    "    index.train(embeddings)\n",
    "    index.add(embeddings)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"âœ… Index built in {elapsed:.2f}s\")\n",
    "    print(f\"   Total vectors: {index.ntotal}\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def search_ivf_index(\n",
    "    index: faiss.IndexIVF,\n",
    "    query_emb: np.ndarray,\n",
    "    k: int = 10,\n",
    "    nprobe: int = 10,\n",
    ") -> Tuple[List[int], List[float], float]:\n",
    "    \"\"\"Search IVF index with nprobe parameter.\"\"\"\n",
    "    index.nprobe = nprobe\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    distances, indices = index.search(query_emb.reshape(1, -1), k)\n",
    "    latency_ms = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    return indices[0].tolist(), distances[0].tolist(), latency_ms\n",
    "\n",
    "\n",
    "# Build IVF index\n",
    "index_ivf = build_ivf_index(corpus_embeddings, nlist=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate IVF with different nprobe values\n",
    "nprobe_values = [1, 5, 10, 20, 50]\n",
    "ivf_results = []\n",
    "\n",
    "print(\"Evaluating IVF index...\\n\")\n",
    "\n",
    "for nprobe in nprobe_values:\n",
    "    print(f\"Testing nprobe={nprobe}...\")\n",
    "    \n",
    "    recalls = []\n",
    "    latencies = []\n",
    "    \n",
    "    query_texts = [q[\"text\"] for q in TEST_QUERIES]\n",
    "    query_embs = get_embeddings_batch(query_texts)\n",
    "    \n",
    "    for i, query_emb in enumerate(query_embs):\n",
    "        query_id = f\"q_{i}\"\n",
    "        relevant = GROUND_TRUTH[query_id]\n",
    "        \n",
    "        indices, _, latency = search_ivf_index(index_ivf, query_emb, k=10, nprobe=nprobe)\n",
    "        retrieved_ids = [CORPUS[idx][\"id\"] for idx in indices]\n",
    "        \n",
    "        recall = recall_at_k(retrieved_ids, relevant, 10)\n",
    "        recalls.append(recall)\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    ivf_results.append({\n",
    "        \"nprobe\": nprobe,\n",
    "        \"recall@10\": np.mean(recalls),\n",
    "        \"latency_p50_ms\": np.percentile(latencies, 50),\n",
    "        \"latency_p95_ms\": np.percentile(latencies, 95),\n",
    "    })\n",
    "    \n",
    "    print(f\"  Recall@10: {np.mean(recalls):.3f}\")\n",
    "    print(f\"  Latency p95: {np.percentile(latencies, 95):.2f}ms\\n\")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n=== IVF Parameter Sweep Results ===\")\n",
    "print(\"nprobe | Recall@10 | p50 (ms) | p95 (ms)\")\n",
    "print(\"-------|-----------|----------|----------\")\n",
    "for r in ivf_results:\n",
    "    print(f\"{r['nprobe']:6d} | {r['recall@10']:9.3f} | {r['latency_p50_ms']:8.2f} | {r['latency_p95_ms']:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d128d0",
   "metadata": {},
   "source": [
    "### HNSW vs IVF Comparison\n",
    "\n",
    "**HNSW Advantages:**\n",
    "- Higher recall at similar latency\n",
    "- No training required (simpler pipeline)\n",
    "- Better for dynamic data (easier to add vectors)\n",
    "\n",
    "**IVF Advantages:**\n",
    "- Lower memory footprint\n",
    "- Better for very large datasets (>10M vectors)\n",
    "- Can be combined with PQ for compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a54d7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: Product Quantization (PQ) for Compression\n",
    "\n",
    "**Task:** Apply Product Quantization to compress vectors and measure impact.\n",
    "\n",
    "**PQ Parameters:**\n",
    "- `m`: Number of subquantizers (dim must be divisible by m)\n",
    "- `nbits`: Bits per subquantizer (typical: 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4477dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ivf_pq_index(\n",
    "    embeddings: np.ndarray,\n",
    "    nlist: int = 100,\n",
    "    m: int = 96,  # text-embedding-3-small is 1536-dim, 1536/96 = 16\n",
    "    nbits: int = 8,\n",
    ") -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Build FAISS IVF-PQ index (compressed).\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding matrix\n",
    "        nlist: Number of Voronoi cells\n",
    "        m: Number of subquantizers\n",
    "        nbits: Bits per subquantizer\n",
    "    \"\"\"\n",
    "    # TODO: Implement IVF-PQ index\n",
    "    # 1. Create quantizer\n",
    "    # 2. Create IndexIVFPQ\n",
    "    # 3. Train and add\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    if dim % m != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be divisible by m={m}\")\n",
    "    \n",
    "    quantizer = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)\n",
    "    \n",
    "    print(f\"Training IVF-PQ index (nlist={nlist}, m={m}, nbits={nbits})...\")\n",
    "    start = time.time()\n",
    "    index.train(embeddings)\n",
    "    index.add(embeddings)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_bytes = embeddings.nbytes\n",
    "    compressed_bytes_per_vector = m  # Each subquantizer uses 1 byte (2^8 = 256 codes)\n",
    "    compressed_bytes = len(embeddings) * compressed_bytes_per_vector\n",
    "    ratio = original_bytes / compressed_bytes\n",
    "    \n",
    "    print(f\"âœ… Index built in {elapsed:.2f}s\")\n",
    "    print(f\"   Original size: {original_bytes / (1024**2):.2f} MB\")\n",
    "    print(f\"   Compressed size: {compressed_bytes / (1024**2):.2f} MB\")\n",
    "    print(f\"   Compression ratio: {ratio:.1f}x\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "# Build IVF-PQ index\n",
    "index_ivf_pq = build_ivf_pq_index(corpus_embeddings, nlist=100, m=96, nbits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate IVF-PQ\n",
    "print(\"Evaluating IVF-PQ index (nprobe=10)...\")\n",
    "\n",
    "recalls = []\n",
    "latencies = []\n",
    "\n",
    "query_texts = [q[\"text\"] for q in TEST_QUERIES]\n",
    "query_embs = get_embeddings_batch(query_texts)\n",
    "\n",
    "for i, query_emb in enumerate(query_embs):\n",
    "    query_id = f\"q_{i}\"\n",
    "    relevant = GROUND_TRUTH[query_id]\n",
    "    \n",
    "    indices, _, latency = search_ivf_index(index_ivf_pq, query_emb, k=10, nprobe=10)\n",
    "    retrieved_ids = [CORPUS[idx][\"id\"] for idx in indices]\n",
    "    \n",
    "    recall = recall_at_k(retrieved_ids, relevant, 10)\n",
    "    recalls.append(recall)\n",
    "    latencies.append(latency)\n",
    "\n",
    "print(f\"\\nIVF-PQ Results (nprobe=10):\")\n",
    "print(f\"  Recall@10: {np.mean(recalls):.3f}\")\n",
    "print(f\"  Latency p50: {np.percentile(latencies, 50):.2f}ms\")\n",
    "print(f\"  Latency p95: {np.percentile(latencies, 95):.2f}ms\")\n",
    "print(f\"\\nNote: PQ typically reduces recall by 1-5% but provides 10-20x compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c58231",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenge: Multi-dimensional Analysis\n",
    "\n",
    "**Task:** Create a comparison table across all index types and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63883042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile comprehensive comparison\n",
    "comparison = [\n",
    "    {\n",
    "        \"Index Type\": \"HNSW (M=32)\",\n",
    "        \"Config\": \"ef_search=100\",\n",
    "        \"Recall@10\": 0.0,  # Fill from sweep_results\n",
    "        \"p95 Latency (ms)\": 0.0,\n",
    "        \"Memory (MB)\": corpus_embeddings.nbytes / (1024**2),\n",
    "        \"Build Time (s)\": 0.0,\n",
    "    },\n",
    "    # Add more configurations...\n",
    "]\n",
    "\n",
    "# Find specific configurations from sweep results\n",
    "hnsw_100 = next((r for r in sweep_results if r[\"ef_search\"] == 100), None)\n",
    "hnsw_200 = next((r for r in sweep_results if r[\"ef_search\"] == 200), None)\n",
    "ivf_10 = next((r for r in ivf_results if r[\"nprobe\"] == 10), None)\n",
    "\n",
    "comparison = [\n",
    "    {\n",
    "        \"Index Type\": \"HNSW\",\n",
    "        \"Config\": \"ef_search=100\",\n",
    "        \"Recall@10\": hnsw_100[\"recall@10\"] if hnsw_100 else 0.0,\n",
    "        \"p95 (ms)\": hnsw_100[\"latency_p95_ms\"] if hnsw_100 else 0.0,\n",
    "        \"Memory (MB)\": corpus_embeddings.nbytes / (1024**2),\n",
    "    },\n",
    "    {\n",
    "        \"Index Type\": \"HNSW\",\n",
    "        \"Config\": \"ef_search=200\",\n",
    "        \"Recall@10\": hnsw_200[\"recall@10\"] if hnsw_200 else 0.0,\n",
    "        \"p95 (ms)\": hnsw_200[\"latency_p95_ms\"] if hnsw_200 else 0.0,\n",
    "        \"Memory (MB)\": corpus_embeddings.nbytes / (1024**2),\n",
    "    },\n",
    "    {\n",
    "        \"Index Type\": \"IVF\",\n",
    "        \"Config\": \"nprobe=10\",\n",
    "        \"Recall@10\": ivf_10[\"recall@10\"] if ivf_10 else 0.0,\n",
    "        \"p95 (ms)\": ivf_10[\"latency_p95_ms\"] if ivf_10 else 0.0,\n",
    "        \"Memory (MB)\": corpus_embeddings.nbytes / (1024**2),\n",
    "    },\n",
    "    {\n",
    "        \"Index Type\": \"IVF-PQ\",\n",
    "        \"Config\": \"nprobe=10, m=96\",\n",
    "        \"Recall@10\": np.mean(recalls),\n",
    "        \"p95 (ms)\": np.percentile(latencies, 95),\n",
    "        \"Memory (MB)\": (len(CORPUS) * 96) / (1024**2),  # Compressed\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n=== Index Type Comparison ===\")\n",
    "print(f\"{'Index Type':<12} | {'Config':<20} | {'Recall@10':<10} | {'p95 (ms)':<10} | {'Memory (MB)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for row in comparison:\n",
    "    print(f\"{row['Index Type']:<12} | {row['Config']:<20} | {row['Recall@10']:<10.3f} | {row['p95 (ms)']:<10.2f} | {row['Memory (MB)']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bc6be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Lab Complete!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Built and configured FAISS HNSW indexes\n",
    "- âœ… Measured recall@k with ground truth\n",
    "- âœ… Benchmarked latency (p50, p95, p99)\n",
    "- âœ… Tuned ef_search for quality/speed trade-offs\n",
    "- âœ… Compared HNSW vs IVF indexes\n",
    "- âœ… Applied Product Quantization for compression\n",
    "- âœ… Analyzed multi-dimensional trade-offs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **HNSW** is preferred for <10M vectors with high recall requirements\n",
    "2. **ef_search** has the largest impact on recall/latency trade-off\n",
    "3. **IVF** works better at massive scale (>10M vectors)\n",
    "4. **PQ compression** provides 10-20x memory savings with 1-5% recall drop\n",
    "5. Always measure with ground truth and production query patterns\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "- Start with HNSW (M=32, ef_construction=200)\n",
    "- Tune ef_search to meet latency SLO (target p95 < 100ms)\n",
    "- Monitor recall@k continuously with canary queries\n",
    "- Consider IVF-PQ for cost optimization at scale\n",
    "- Always test with representative query distribution\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test with your own corpus and queries\n",
    "2. Integrate with production vector database (Pinecone, Weaviate, Qdrant)\n",
    "3. Set up continuous evaluation pipeline\n",
    "4. Move on to Week 5 Lesson 4: Production RAG Systems\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Week 5 Resources: [../resources/README.md](../resources/README.md)\n",
    "- Index Tuning Cheatsheet: [../resources/index-tuning-cheatsheet.md](../resources/index-tuning-cheatsheet.md)\n",
    "- Recall vs Latency Guide: [../resources/recall-vs-latency-evaluation.md](../resources/recall-vs-latency-evaluation.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
