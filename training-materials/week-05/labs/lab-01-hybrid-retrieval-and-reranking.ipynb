{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef07ae7",
   "metadata": {},
   "source": [
    "# Week 5 - Lab 1: Hybrid Retrieval & Re-ranking\n",
    "\n",
    "**Duration:** 90-120 minutes  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** Week 4 RAG fundamentals, Week 5 Lessons 1-2\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "- Implement dense (semantic) and lexical (BM25) retrieval\n",
    "- Build fusion strategies (weighted, RRF) to combine retrievers\n",
    "- Apply MMR for diversity and deduplication\n",
    "- Implement LLM-based re-ranking with JSON outputs\n",
    "- Measure recall@k, precision@k, and latency\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Lab Outline\n",
    "\n",
    "1. Setup and Data Preparation\n",
    "2. Exercise 1: Dense Retrieval with Embeddings\n",
    "3. Exercise 2: Lexical Retrieval with BM25\n",
    "4. Exercise 3: Hybrid Fusion (Weighted + RRF)\n",
    "5. Exercise 4: MMR for Diversity\n",
    "6. Exercise 5: LLM Re-ranking\n",
    "7. Exercise 6: End-to-End Pipeline Evaluation\n",
    "8. Bonus Challenge: Parameter Sweep\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dc114",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai numpy rank-bm25 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432fd0a",
   "metadata": {},
   "source": [
    "### Sample Document Corpus\n",
    "\n",
    "We'll use a small corpus of documents about RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "CORPUS = [\n",
    "    {\"id\": \"doc1\", \"text\": \"RAG combines retrieval and generation for accurate, grounded answers. It retrieves relevant documents then generates responses based on that context.\"},\n",
    "    {\"id\": \"doc2\", \"text\": \"Vector databases like Chroma, Pinecone, and Weaviate enable semantic search through embedding similarity. HNSW indexes provide fast approximate nearest neighbor search.\"},\n",
    "    {\"id\": \"doc3\", \"text\": \"Hybrid retrieval combines dense vectors with BM25 lexical search. This improves recall on rare terms, IDs, and exact matches that pure semantic search might miss.\"},\n",
    "    {\"id\": \"doc4\", \"text\": \"Query rewriting with HyDE generates hypothetical answers to improve retrieval. Multi-query expansion creates paraphrases for better coverage.\"},\n",
    "    {\"id\": \"doc5\", \"text\": \"Re-ranking with cross-encoders or LLMs refines initial retrieval results. This two-stage approach balances speed and precision.\"},\n",
    "    {\"id\": \"doc6\", \"text\": \"MMR (Maximal Marginal Relevance) promotes diversity in retrieved results. It balances relevance to the query with dissimilarity to already selected documents.\"},\n",
    "    {\"id\": \"doc7\", \"text\": \"Production RAG systems need monitoring for recall, latency, and cost. SLOs typically target 99.9% availability and p95 latency under 2 seconds.\"},\n",
    "    {\"id\": \"doc8\", \"text\": \"Chunking strategies affect retrieval quality. Options include fixed-token windows, paragraph-aware splitting, and semantic chunking with overlap.\"},\n",
    "    {\"id\": \"doc9\", \"text\": \"Embeddings transform text into dense vectors capturing semantic meaning. OpenAI's text-embedding-3-small produces 1536-dimensional vectors efficiently.\"},\n",
    "    {\"id\": \"doc10\", \"text\": \"Index tuning involves parameters like HNSW's ef_search and M. Higher values improve recall at the cost of increased latency and memory usage.\"},\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(CORPUS)} documents\")\n",
    "print(f\"Sample doc: {CORPUS[0]['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb85507",
   "metadata": {},
   "source": [
    "### Ground Truth for Evaluation\n",
    "\n",
    "We define relevant documents for test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with ground truth\n",
    "TEST_QUERIES = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"text\": \"How does hybrid retrieval work?\",\n",
    "        \"relevant\": {\"doc3\", \"doc2\"},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"text\": \"What is MMR and why use it?\",\n",
    "        \"relevant\": {\"doc6\"},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"text\": \"Explain query rewriting techniques\",\n",
    "        \"relevant\": {\"doc4\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Test queries: {len(TEST_QUERIES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010cc4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Dense Retrieval with Embeddings\n",
    "\n",
    "**Task:** Implement semantic search using OpenAI embeddings and cosine similarity.\n",
    "\n",
    "**Steps:**\n",
    "1. Generate embeddings for all corpus documents\n",
    "2. Implement cosine similarity function\n",
    "3. Create a dense retriever that returns top-k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c972aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"Get embedding vector for text.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    \"\"\"Get embeddings for multiple texts in batch.\"\"\"\n",
    "    cleaned = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "    response = client.embeddings.create(input=cleaned, model=model)\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "# TODO: Generate embeddings for corpus\n",
    "print(\"Generating embeddings for corpus...\")\n",
    "corpus_texts = [doc[\"text\"] for doc in CORPUS]\n",
    "corpus_embeddings = get_embeddings_batch(corpus_texts)\n",
    "corpus_embeddings = np.array(corpus_embeddings)\n",
    "\n",
    "print(f\"âœ… Generated {len(corpus_embeddings)} embeddings, shape: {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5baf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    # TODO: Implement cosine similarity\n",
    "    # Hint: dot product divided by product of norms\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def dense_retrieve(query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Retrieve top-k documents using dense (semantic) search.\"\"\"\n",
    "    # TODO: Implement dense retrieval\n",
    "    # 1. Get query embedding\n",
    "    # 2. Calculate similarity with all corpus embeddings\n",
    "    # 3. Return top-k (doc_id, score) pairs\n",
    "    \n",
    "    query_emb = np.array(get_embedding(query))\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(corpus_embeddings):\n",
    "        sim = cosine_similarity(query_emb, doc_emb)\n",
    "        similarities.append((CORPUS[i][\"id\"], sim))\n",
    "    \n",
    "    # Sort by similarity descending and return top-k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "\n",
    "# Test dense retrieval\n",
    "query = \"What is hybrid search?\"\n",
    "results = dense_retrieve(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Dense retrieval results:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"  {doc_id}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288eaf3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Lexical Retrieval with BM25\n",
    "\n",
    "**Task:** Implement keyword-based retrieval using BM25.\n",
    "\n",
    "**Steps:**\n",
    "1. Tokenize corpus documents\n",
    "2. Build BM25 index\n",
    "3. Create a lexical retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization (lowercase + split).\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "# TODO: Build BM25 index\n",
    "tokenized_corpus = [simple_tokenize(doc[\"text\"]) for doc in CORPUS]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "print(f\"âœ… BM25 index built with {len(tokenized_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c688793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_retrieve(query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Retrieve top-k documents using BM25 lexical search.\"\"\"\n",
    "    # TODO: Implement BM25 retrieval\n",
    "    # 1. Tokenize query\n",
    "    # 2. Get BM25 scores\n",
    "    # 3. Return top-k (doc_id, score) pairs\n",
    "    \n",
    "    query_tokens = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Create (doc_id, score) pairs and sort\n",
    "    results = [(CORPUS[i][\"id\"], scores[i]) for i in range(len(scores))]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:k]\n",
    "\n",
    "\n",
    "# Test BM25 retrieval\n",
    "query = \"What is hybrid search?\"\n",
    "results = bm25_retrieve(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"BM25 retrieval results:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"  {doc_id}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dce06b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Hybrid Fusion (Weighted + RRF)\n",
    "\n",
    "**Task:** Combine dense and BM25 results using two fusion strategies.\n",
    "\n",
    "**Steps:**\n",
    "1. Implement score normalization\n",
    "2. Implement weighted score fusion\n",
    "3. Implement Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(scores: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"Min-max normalize scores to [0, 1].\"\"\"\n",
    "    if not scores:\n",
    "        return {}\n",
    "    \n",
    "    # TODO: Implement min-max normalization\n",
    "    vals = list(scores.values())\n",
    "    min_val, max_val = min(vals), max(vals)\n",
    "    \n",
    "    if max_val - min_val < 1e-9:\n",
    "        return {k: 0.0 for k in scores}\n",
    "    \n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in scores.items()}\n",
    "\n",
    "\n",
    "def weighted_fusion(\n",
    "    dense_results: List[Tuple[str, float]],\n",
    "    bm25_results: List[Tuple[str, float]],\n",
    "    alpha: float = 0.6,\n",
    "    k: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Fuse results using weighted score combination.\"\"\"\n",
    "    # TODO: Implement weighted fusion\n",
    "    # 1. Convert to dicts\n",
    "    # 2. Normalize separately\n",
    "    # 3. Combine: alpha * dense + (1-alpha) * bm25\n",
    "    # 4. Sort and return top-k\n",
    "    \n",
    "    dense_dict = dict(dense_results)\n",
    "    bm25_dict = dict(bm25_results)\n",
    "    \n",
    "    dense_norm = normalize_scores(dense_dict)\n",
    "    bm25_norm = normalize_scores(bm25_dict)\n",
    "    \n",
    "    all_ids = set(dense_norm.keys()) | set(bm25_norm.keys())\n",
    "    \n",
    "    fused = {}\n",
    "    for doc_id in all_ids:\n",
    "        d_score = dense_norm.get(doc_id, 0.0)\n",
    "        b_score = bm25_norm.get(doc_id, 0.0)\n",
    "        fused[doc_id] = alpha * d_score + (1 - alpha) * b_score\n",
    "    \n",
    "    results = sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
    "    return results[:k]\n",
    "\n",
    "\n",
    "# Test weighted fusion\n",
    "query = \"What is hybrid search?\"\n",
    "dense_res = dense_retrieve(query, k=10)\n",
    "bm25_res = bm25_retrieve(query, k=10)\n",
    "fused_res = weighted_fusion(dense_res, bm25_res, alpha=0.6, k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Weighted fusion results (alpha=0.6):\")\n",
    "for doc_id, score in fused_res:\n",
    "    print(f\"  {doc_id}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3000db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fusion(\n",
    "    dense_results: List[Tuple[str, float]],\n",
    "    bm25_results: List[Tuple[str, float]],\n",
    "    k_param: int = 60,\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Fuse results using Reciprocal Rank Fusion.\"\"\"\n",
    "    # TODO: Implement RRF\n",
    "    # Formula: score = sum(1 / (k + rank)) for each retriever\n",
    "    # rank is 1-based position in results list\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    \n",
    "    # Process dense results\n",
    "    for rank, (doc_id, _) in enumerate(dense_results, start=1):\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + 1.0 / (k_param + rank)\n",
    "    \n",
    "    # Process BM25 results\n",
    "    for rank, (doc_id, _) in enumerate(bm25_results, start=1):\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + 1.0 / (k_param + rank)\n",
    "    \n",
    "    results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "# Test RRF\n",
    "rrf_res = rrf_fusion(dense_res, bm25_res, k_param=60, top_k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"RRF fusion results (k=60):\")\n",
    "for doc_id, score in rrf_res:\n",
    "    print(f\"  {doc_id}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72b18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: MMR for Diversity\n",
    "\n",
    "**Task:** Implement Maximal Marginal Relevance to promote diversity.\n",
    "\n",
    "**Steps:**\n",
    "1. Given initial retrieval results, apply MMR\n",
    "2. Balance relevance vs diversity with lambda parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_select(\n",
    "    query_emb: np.ndarray,\n",
    "    doc_ids: List[str],\n",
    "    k: int = 5,\n",
    "    lambda_param: float = 0.5\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Select k documents using Maximal Marginal Relevance.\n",
    "    \n",
    "    Args:\n",
    "        query_emb: Query embedding vector\n",
    "        doc_ids: Candidate document IDs\n",
    "        k: Number of documents to select\n",
    "        lambda_param: Balance between relevance (1.0) and diversity (0.0)\n",
    "    \"\"\"\n",
    "    # TODO: Implement MMR\n",
    "    # Algorithm:\n",
    "    # 1. Start with empty selected set\n",
    "    # 2. While len(selected) < k:\n",
    "    #    a. For each candidate, calculate:\n",
    "    #       MMR = lambda * sim(q, doc) - (1-lambda) * max(sim(doc, selected_doc))\n",
    "    #    b. Select candidate with highest MMR score\n",
    "    #    c. Remove from candidates, add to selected\n",
    "    \n",
    "    # Get embeddings for candidates\n",
    "    doc_indices = [i for i, doc in enumerate(CORPUS) if doc[\"id\"] in doc_ids]\n",
    "    candidate_embs = corpus_embeddings[doc_indices]\n",
    "    candidate_ids = [CORPUS[i][\"id\"] for i in doc_indices]\n",
    "    \n",
    "    selected = []\n",
    "    candidates = list(range(len(candidate_ids)))\n",
    "    \n",
    "    # Calculate query similarities once\n",
    "    query_sims = np.dot(candidate_embs, query_emb) / (\n",
    "        np.linalg.norm(candidate_embs, axis=1) * np.linalg.norm(query_emb)\n",
    "    )\n",
    "    \n",
    "    while candidates and len(selected) < k:\n",
    "        if not selected:\n",
    "            # First selection: most relevant\n",
    "            best_idx = candidates[np.argmax(query_sims[candidates])]\n",
    "            selected.append(best_idx)\n",
    "            candidates.remove(best_idx)\n",
    "        else:\n",
    "            # Calculate MMR for remaining candidates\n",
    "            mmr_scores = []\n",
    "            selected_embs = candidate_embs[selected]\n",
    "            \n",
    "            for c in candidates:\n",
    "                # Relevance to query\n",
    "                relevance = query_sims[c]\n",
    "                \n",
    "                # Max similarity to already selected\n",
    "                c_emb = candidate_embs[c]\n",
    "                sims_to_selected = np.dot(selected_embs, c_emb) / (\n",
    "                    np.linalg.norm(selected_embs, axis=1) * np.linalg.norm(c_emb)\n",
    "                )\n",
    "                redundancy = np.max(sims_to_selected)\n",
    "                \n",
    "                # MMR score\n",
    "                mmr = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
    "                mmr_scores.append(mmr)\n",
    "            \n",
    "            # Select best MMR\n",
    "            best_idx = candidates[np.argmax(mmr_scores)]\n",
    "            selected.append(best_idx)\n",
    "            candidates.remove(best_idx)\n",
    "    \n",
    "    return [candidate_ids[i] for i in selected]\n",
    "\n",
    "\n",
    "# Test MMR\n",
    "query = \"What is hybrid search?\"\n",
    "query_emb = np.array(get_embedding(query))\n",
    "initial_results = [doc_id for doc_id, _ in fused_res[:10]]\n",
    "\n",
    "mmr_results = mmr_select(query_emb, initial_results, k=5, lambda_param=0.5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"MMR selected documents (lambda=0.5):\")\n",
    "for i, doc_id in enumerate(mmr_results, 1):\n",
    "    print(f\"  {i}. {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4025d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: LLM Re-ranking\n",
    "\n",
    "**Task:** Use an LLM to re-rank retrieved documents.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a re-ranking prompt\n",
    "2. Parse JSON output with scores\n",
    "3. Return re-ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c29951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_rerank(\n",
    "    query: str,\n",
    "    doc_ids: List[str],\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using LLM.\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc_id, score) tuples sorted by relevance\n",
    "    \"\"\"\n",
    "    # Get document texts\n",
    "    candidates = []\n",
    "    for i, doc in enumerate(CORPUS):\n",
    "        if doc[\"id\"] in doc_ids:\n",
    "            candidates.append((i, doc[\"id\"], doc[\"text\"]))\n",
    "    \n",
    "    # Build prompt\n",
    "    system_prompt = (\n",
    "        \"You are a relevance ranking expert. Rank passages by relevance to the query. \"\n",
    "        \"Return JSON list with format: [{\\\"index\\\": <int>, \\\"score\\\": <0-1 float>, \\\"reason\\\": \\\"...\\\"}] \"\n",
    "        \"sorted by score descending.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = f\"Query: {query}\\n\\nCandidates:\\n\"\n",
    "    for idx, doc_id, text in candidates:\n",
    "        user_prompt += f\"{idx}) {text[:200]}...\\n\\n\"\n",
    "    \n",
    "    # Call LLM\n",
    "    # TODO: Implement LLM call and parse JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    text = response.choices[0].message.content\n",
    "    \n",
    "    # Extract JSON (handle markdown code blocks)\n",
    "    if \"```json\" in text:\n",
    "        text = text.split(\"```json\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in text:\n",
    "        text = text.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    rankings = json.loads(text.strip())\n",
    "    \n",
    "    # Map indices back to doc_ids\n",
    "    results = []\n",
    "    for item in rankings[:top_k]:\n",
    "        idx = item[\"index\"]\n",
    "        score = item[\"score\"]\n",
    "        doc_id = candidates[idx][1]\n",
    "        results.append((doc_id, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test LLM re-ranking\n",
    "query = \"What is hybrid search?\"\n",
    "candidate_ids = [doc_id for doc_id, _ in fused_res[:8]]\n",
    "\n",
    "reranked = llm_rerank(query, candidate_ids, top_k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"LLM re-ranked results:\")\n",
    "for doc_id, score in reranked:\n",
    "    print(f\"  {doc_id}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49b2ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: End-to-End Pipeline Evaluation\n",
    "\n",
    "**Task:** Build complete pipeline and evaluate with recall@k, precision@k.\n",
    "\n",
    "**Steps:**\n",
    "1. Create pipeline function\n",
    "2. Implement evaluation metrics\n",
    "3. Run on test queries and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f78aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(\n",
    "    retrieved: List[str],\n",
    "    relevant: Set[str],\n",
    "    k: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision and recall at k.\"\"\"\n",
    "    # TODO: Implement metrics\n",
    "    topk = retrieved[:k]\n",
    "    hits = sum(1 for doc_id in topk if doc_id in relevant)\n",
    "    \n",
    "    precision = hits / max(1, k)\n",
    "    recall = hits / max(1, len(relevant))\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def hybrid_rag_pipeline(\n",
    "    query: str,\n",
    "    method: str = \"rrf\",\n",
    "    k: int = 5,\n",
    "    use_mmr: bool = False,\n",
    "    use_rerank: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Complete hybrid RAG retrieval pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        method: Fusion method ('weighted' or 'rrf')\n",
    "        k: Number of results\n",
    "        use_mmr: Apply MMR for diversity\n",
    "        use_rerank: Apply LLM re-ranking\n",
    "    \"\"\"\n",
    "    # Step 1: Dense + BM25 retrieval\n",
    "    dense_res = dense_retrieve(query, k=20)\n",
    "    bm25_res = bm25_retrieve(query, k=20)\n",
    "    \n",
    "    # Step 2: Fusion\n",
    "    if method == \"weighted\":\n",
    "        fused = weighted_fusion(dense_res, bm25_res, alpha=0.6, k=k*2)\n",
    "    else:\n",
    "        fused = rrf_fusion(dense_res, bm25_res, k_param=60, top_k=k*2)\n",
    "    \n",
    "    doc_ids = [doc_id for doc_id, _ in fused]\n",
    "    \n",
    "    # Step 3: Optional MMR\n",
    "    if use_mmr:\n",
    "        query_emb = np.array(get_embedding(query))\n",
    "        doc_ids = mmr_select(query_emb, doc_ids, k=k*2, lambda_param=0.5)\n",
    "    \n",
    "    # Step 4: Optional LLM re-ranking\n",
    "    if use_rerank:\n",
    "        reranked = llm_rerank(query, doc_ids[:k*2], top_k=k)\n",
    "        doc_ids = [doc_id for doc_id, _ in reranked]\n",
    "    \n",
    "    return doc_ids[:k]\n",
    "\n",
    "\n",
    "# Evaluate pipeline\n",
    "def evaluate_pipeline(pipeline_fn, queries, k=5):\n",
    "    \"\"\"Evaluate pipeline on test queries.\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for query_info in queries:\n",
    "        query = query_info[\"text\"]\n",
    "        relevant = query_info[\"relevant\"]\n",
    "        \n",
    "        results = pipeline_fn(query, k=k)\n",
    "        p, r = precision_recall_at_k(results, relevant, k)\n",
    "        \n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "    \n",
    "    return {\n",
    "        \"precision@k\": sum(precisions) / len(precisions),\n",
    "        \"recall@k\": sum(recalls) / len(recalls),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    {\"name\": \"Weighted fusion\", \"method\": \"weighted\", \"use_mmr\": False, \"use_rerank\": False},\n",
    "    {\"name\": \"RRF fusion\", \"method\": \"rrf\", \"use_mmr\": False, \"use_rerank\": False},\n",
    "    {\"name\": \"RRF + MMR\", \"method\": \"rrf\", \"use_mmr\": True, \"use_rerank\": False},\n",
    "]\n",
    "\n",
    "print(\"Pipeline Evaluation Results:\\n\")\n",
    "for config in configs:\n",
    "    pipeline = lambda q, k=5: hybrid_rag_pipeline(\n",
    "        q, method=config[\"method\"], k=k,\n",
    "        use_mmr=config[\"use_mmr\"], use_rerank=config[\"use_rerank\"]\n",
    "    )\n",
    "    \n",
    "    results = evaluate_pipeline(pipeline, TEST_QUERIES, k=5)\n",
    "    \n",
    "    print(f\"{config['name']}:\")\n",
    "    print(f\"  Precision@5: {results['precision@k']:.3f}\")\n",
    "    print(f\"  Recall@5: {results['recall@k']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a8e8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenge: Parameter Sweep\n",
    "\n",
    "**Task:** Sweep fusion weights (alpha) and measure impact on recall.\n",
    "\n",
    "Try different alpha values (0.3, 0.5, 0.7, 0.9) and plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parameter sweep\n",
    "# 1. Loop over alpha values\n",
    "# 2. For each, run evaluation\n",
    "# 3. Collect and display results\n",
    "\n",
    "alphas = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "sweep_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Override weighted_fusion to use this alpha\n",
    "    pipeline = lambda q, k=5: hybrid_rag_pipeline(q, method=\"weighted\", k=k)\n",
    "    \n",
    "    # Temporarily modify fusion function\n",
    "    original_fn = weighted_fusion\n",
    "    \n",
    "    def custom_fusion(d, b, alpha=alpha, k=5):\n",
    "        return original_fn(d, b, alpha=alpha, k=k)\n",
    "    \n",
    "    # Run evaluation\n",
    "    metrics = evaluate_pipeline(pipeline, TEST_QUERIES, k=5)\n",
    "    sweep_results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"precision\": metrics[\"precision@k\"],\n",
    "        \"recall\": metrics[\"recall@k\"]\n",
    "    })\n",
    "\n",
    "print(\"Alpha Sweep Results:\\n\")\n",
    "print(\"Alpha | Precision@5 | Recall@5\")\n",
    "print(\"------|-------------|----------\")\n",
    "for r in sweep_results:\n",
    "    print(f\"{r['alpha']:.1f}   | {r['precision']:.3f}       | {r['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9de549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Lab Complete!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Dense retrieval with embeddings and cosine similarity\n",
    "- âœ… Lexical retrieval with BM25\n",
    "- âœ… Hybrid fusion strategies (weighted and RRF)\n",
    "- âœ… MMR for diversity and deduplication\n",
    "- âœ… LLM-based re-ranking with JSON outputs\n",
    "- âœ… End-to-end pipeline evaluation\n",
    "- âœ… Parameter tuning and experimentation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try with your own document corpus\n",
    "2. Experiment with different fusion weights\n",
    "3. Add more sophisticated query rewriting\n",
    "4. Integrate with a real vector database\n",
    "5. Move on to Lab 2: Index Tuning and Recall Testing\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Week 5 Resources: [../resources/README.md](../resources/README.md)\n",
    "- Hybrid Retrieval Guide: [../resources/hybrid-retrieval-tuning.md](../resources/hybrid-retrieval-tuning.md)\n",
    "- Evaluation Harness: [../resources/examples/](../resources/examples/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
